

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>tokenize Package &mdash; NLTK 2.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="NLTK 2.0 documentation" href="../index.html" /> 
  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
        <div class="headertitle"><a
          href="../index.html">NLTK 2.0 documentation</a></div>
        <div class="rel">
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="tokenize-package">
<h1>tokenize Package<a class="headerlink" href="#tokenize-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2><tt class="xref py py-mod docutils literal"><span class="pre">tokenize</span></tt> Package<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-nltk.tokenize"></span><p>Functions for X{tokenizing}, i.e., dividing text strings into
substrings.</p>
<dl class="class">
<dt id="nltk.tokenize.WhitespaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">WhitespaceTokenizer</tt><a class="headerlink" href="#nltk.tokenize.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
sequence of whitespace characters as a separator.  Whitespace
characters are space (C{&#8216; &#8216;}), tab (C{&#8216;t&#8217;}), and newline
(C{&#8216;n&#8217;}).  If you are performing the tokenization yourself
(rather than building a tokenizer to pass to some other piece of
code), consider using the string C{split()} method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.SpaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">SpaceTokenizer</tt><a class="headerlink" href="#nltk.tokenize.SpaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single space character as a separator.  If you are performing the
tokenization yourself (rather than building a tokenizer to pass to
some other piece of code), consider using the string C{split()}
method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TabTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TabTokenizer</tt><a class="headerlink" href="#nltk.tokenize.TabTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single tab character as a separator.  If you are performing the
tokenization yourself (rather than building a tokenizer to pass to
some other piece of code), consider using the string C{split()}
method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.LineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">LineTokenizer</tt><big>(</big><em>blanklines='discard'</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single newline character as a separator.  Handling of blank lines
may be controlled using a constructor parameter.</p>
<dl class="method">
<dt id="nltk.tokenize.LineTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.LineTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.RegexpTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">RegexpTokenizer</tt><big>(</big><em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that splits a string into substrings using a regular
expression.  The regular expression can be specified to match
either tokens or separators between tokens.</p>
<p>Unlike C{re.findall()} and C{re.split()}, C{RegexpTokenizer} does
not treat regular expressions that contain grouping parenthases
specially.</p>
<dl class="method">
<dt id="nltk.tokenize.RegexpTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.RegexpTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.BlanklineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">BlanklineTokenizer</tt><a class="headerlink" href="#nltk.tokenize.BlanklineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
sequence of blank lines as a separator.  Blank lines are defined
as lines containing no characters, or containing only space
(C{&#8216; &#8216;}) or tab (C{&#8216;        &#8216;}) characters.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.WordPunctTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">WordPunctTokenizer</tt><a class="headerlink" href="#nltk.tokenize.WordPunctTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a text into sequences of alphabetic and
non-alphabetic characters.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">WordPunctTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&quot;She said &#39;hello&#39;.&quot;</span><span class="p">)</span>
<span class="go">[&#39;She&#39;, &#39;said&#39;, &quot;&#39;&quot;, &#39;hello&#39;, &quot;&#39;.&quot;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.regexp_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">regexp_tokenize</tt><big>(</big><em>text</em>, <em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="headerlink" href="#nltk.tokenize.regexp_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Split the given text string, based on the given regular expression
pattern.  See the documentation for L{RegexpTokenizer.tokenize()}
for descriptions of the arguments.</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.word_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">word_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#word_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.word_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Use NLTK&#8217;s currently recommended word tokenizer to tokenize words
in the given sentence.  Currently, this uses
L{TreebankWordTokenizer}.  This tokenizer should be fed a single
sentence at a time.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.SExprTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">SExprTokenizer</tt><big>(</big><em>parens='()'</em>, <em>strict=True</em><big>)</big><a class="headerlink" href="#nltk.tokenize.SExprTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides strings into X{s-expressions}.  An
s-expresion can be either:</p>
<blockquote>
<div><ul class="simple">
<li>A parenthasized expression, including any nested parenthasized
expressions.</li>
<li>A sequence of non-whitespace non-parenthasis characters.</li>
</ul>
</div></blockquote>
<p>For example, the string C{&#8216;(a (b c)) d e (f)&#8217;} consists of four
s-expressions: C{&#8216;(a (b c))&#8217;}, C{&#8216;d&#8217;}, C{&#8216;e&#8217;}, and C{&#8216;(f)&#8217;}.</p>
<dl class="method">
<dt id="nltk.tokenize.SExprTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.SExprTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize the text into s-expressions.  For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parenthases are assumed to mark sexprs.  In particular, no
special processing is done to exclude parenthases that occur
inside strings, or following backslash characters.</p>
<p>If the given expression contains non-matching parenthases,
then the behavior of the tokenizer depends on the C{strict}
parameter to the constructor.  If C{strict} is C{True}, then
raise a C{ValueError}.  If C{strict} is C{False}, then any
unmatched close parenthases will be listed as their own
s-expression; and the last partial sexpr with unmatched open
parenthases will be listed as its own sexpr:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<p>&#64;param text: the string to be tokenized
&#64;type text: C{string} or C{iter(string)}
&#64;return: An iterator over tokens (each of which is an s-expression)</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.line_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">line_tokenize</tt><big>(</big><em>text</em>, <em>blanklines='discard'</em><big>)</big><a class="headerlink" href="#nltk.tokenize.line_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="nltk.tokenize.PunktWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">PunktWordTokenizer</tt><big>(</big><em>lang_vars=&lt;nltk.tokenize.punkt.PunktLanguageVars object at 0x105cb8d90&gt;</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<dl class="method">
<dt id="nltk.tokenize.PunktWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.PunktSentenceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">PunktSentenceTokenizer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt.PunktLanguageVars object at 0x105cc7090&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt.PunktToken'&gt;</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt>, <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A sentence tokenizer which uses an unsupervised algorithm to build
a model for abbreviation words, collocations, and words that start
sentences; and then uses that model to find sentence boundaries.
This approach has been shown to work well for many European
languages.</p>
<dl class="attribute">
<dt id="nltk.tokenize.PunktSentenceTokenizer.PUNCTUATION">
<tt class="descname">PUNCTUATION</tt><em class="property"> = (';', ':', ',', '.', '!', '?')</em><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.dump">
<tt class="descname">dump</tt><big>(</big><em>tokens</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.dump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_text">
<tt class="descname">sentences_from_text</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text by only
testing candidate sentence breaks. If realign_boundaries is
True, includes in the sentence closing punctuation that
follows the period.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_text_legacy">
<tt class="descname">sentences_from_text_legacy</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_text_legacy" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as L{sentences_from_text}.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_tokens">
<tt class="descname">sentences_from_tokens</tt><big>(</big><em>tokens</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a sequence of tokens, generates lists of tokens, each list
corresponding to a sentence.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the (start, end) spans of sentences
in the text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.text_contains_sentbreak">
<tt class="descname">text_contains_sentbreak</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.text_contains_sentbreak" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the given text includes a sentence break.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the sentences in that text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.train">
<tt class="descname">train</tt><big>(</big><em>train_text</em>, <em>verbose=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TreebankWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TreebankWordTokenizer</tt><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A word tokenizer that tokenizes sentences using the conventions
used by the Penn Treebank.  Contractions, such as &#8220;can&#8217;t&#8221;, are
split in to two tokens.  E.g.:</p>
<blockquote>
<div><ul class="simple">
<li>can&#8217;t S{-&gt;} ca n&#8217;t</li>
<li>he&#8217;ll S{-&gt;} he &#8216;ll</li>
<li>weren&#8217;t S{-} were n&#8217;t</li>
</ul>
</div></blockquote>
<p>This tokenizer assumes that the text has already been segmented into
sentences.  Any periods &#8211; apart from those at the end of a string &#8211;
are assumed to be part of the word they are attached to (e.g. for
abbreviations, etc), and are not separately tokenized.</p>
<dl class="attribute">
<dt id="nltk.tokenize.TreebankWordTokenizer.CONTRACTIONS2">
<tt class="descname">CONTRACTIONS2</tt><em class="property"> = [&lt;_sre.SRE_Pattern object at 0x105c7a6c0&gt;, &lt;_sre.SRE_Pattern object at 0x105cb53c0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca16f0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1780&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1810&gt;, &lt;_sre.SRE_Pattern object at 0x105ca18a0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1930&gt;, &lt;_sre.SRE_Pattern object at 0x105ca19c0&gt;, &lt;_sre.SRE_Pattern object at 0x105c958b0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1a50&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1ae0&gt;]</em><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.TreebankWordTokenizer.CONTRACTIONS3">
<tt class="descname">CONTRACTIONS3</tt><em class="property"> = [&lt;_sre.SRE_Pattern object at 0x105c7dea0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca6ad0&gt;]</em><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.TreebankWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.sent_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">sent_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#sent_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sent_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Use NLTK&#8217;s currently recommended sentence tokenizer to tokenize
sentences in the given text.  Currently, this uses
L{PunktSentenceTokenizer}.</p>
</dd></dl>

<dl class="function">
<dt>
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">word_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#word_tokenize"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Use NLTK&#8217;s currently recommended word tokenizer to tokenize words
in the given sentence.  Currently, this uses
L{TreebankWordTokenizer}.  This tokenizer should be fed a single
sentence at a time.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TextTilingTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TextTilingTokenizer</tt><big>(</big><em>w=20, k=10, similarity_method=0, stopwords=None, smoothing_method=[0], smoothing_width=2, smoothing_rounds=1, cutoff_policy=1, demo_mode=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TextTilingTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A section tokenizer based on the TextTiling algorithm. The
algorithm detects subtopic shifts based on the analysis of lexical
co-occurence patterns.</p>
<p>The process starts by tokenizing the text into pseudosentences of
a fixed size w. Then, depending on the method used, similarity
scores are assigned at sentence gaps. The algorithm proceeds by
detecting the peak differences between these scores and marking
them as boundaries. The boundaries are normalized to the closest
paragraph break and the segmented text is returned.</p>
<p>&#64;type w: number
&#64;param w: Pseudosentence size
&#64;type k: number
&#64;param k: Size(in sentences) of the block used in the block comparison</p>
<blockquote>
<div>method</div></blockquote>
<p>&#64;type similarity_method: constant
&#64;param similarity_method: The method used for determining similarity scores
&#64;type stopwords: list
&#64;param stopwords: A list of stopwords that are filtered out
&#64;type smoothing_method: constant
&#64;param smoothing_method: The method used for smoothing the score plot
&#64;type smoothing_width: number
&#64;param smoothing_width: The width of the window used by the smoothing</p>
<blockquote>
<div>method</div></blockquote>
<p>&#64;type smoothing_rounds: number
&#64;param smoothing_rounds: The number of smoothing passes
&#64;type cutoff_policy: constant
&#64;param cutoff_policy: The policy used to determine the number of boundaries</p>
<dl class="method">
<dt id="nltk.tokenize.TextTilingTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TextTilingTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>The main function. Follows a pipeline structure.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.api">
<span id="api-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">api</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.api" title="Permalink to this headline">¶</a></h2>
<p>Tokenizer Interface</p>
<dl class="class">
<dt id="nltk.tokenize.api.StringTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.api.</tt><tt class="descname">StringTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by splitting
on the specified string (defined in subclasses).</p>
<dl class="method">
<dt id="nltk.tokenize.api.StringTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.StringTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.api.TokenizerI">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.api.</tt><tt class="descname">TokenizerI</tt><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A processing interface for I{tokenizing} a string, or dividing it
into a list of substrings.</p>
<dl class="docutils">
<dt>Subclasses must define:</dt>
<dd><ul class="first last simple">
<li>either L{tokenize()} or L{batch_tokenize()} (or both)</li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.batch_span_tokenize">
<tt class="descname">batch_span_tokenize</tt><big>(</big><em>strings</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.batch_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.batch_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.span_tokenize()} to each element of C{strings}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">strings</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{iter} of C{list} of C{tuple} of C{int}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.batch_tokenize">
<tt class="descname">batch_tokenize</tt><big>(</big><em>strings</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.batch_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.batch_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.tokenize()} to each element of C{strings}.  I.e.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">strings</span><span class="p">]</span>
</pre></div>
</div>
<p>&#64;rtype: C{list} of C{list} of C{str}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets (start_i, end_i),
where s[start_i:end_i] is the corresponding token.</p>
<p>&#64;return: C{iter} of C{tuple} of C{int}</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Divide the given string into a list of substrings.</p>
<p>&#64;return: C{list} of C{str}</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.punkt">
<span id="punkt-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">punkt</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.punkt" title="Permalink to this headline">¶</a></h2>
<p>The Punkt sentence tokenizer.  The algorithm for this tokenizer is
described in Kiss &amp; Strunk (2006):</p>
<div class="highlight-python"><pre>Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence
  Boundary Detection.  Computational Linguistics 32: 485-525.</pre>
</div>
<dl class="class">
<dt id="nltk.tokenize.punkt.PunktLanguageVars">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktLanguageVars</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stores variables, mostly regular expressions, which may be
language-dependent for correct application of the algorithm.
An extension of this class may modify its properties to suit
a language other than English; an instance can then be passed
as an argument to PunktSentenceTokenizer and PunktTrainer
constructors.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.internal_punctuation">
<tt class="descname">internal_punctuation</tt><em class="property"> = ',:;'</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.internal_punctuation" title="Permalink to this definition">¶</a></dt>
<dd><p>sentence internal punctuation, which indicates an abbreviation if
preceded by a period-final token.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.period_context_re">
<tt class="descname">period_context_re</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars.period_context_re"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.period_context_re" title="Permalink to this definition">¶</a></dt>
<dd><p>Compiles and returns a regular expression to find contexts
including possible sentence boundaries.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.re_boundary_realignment">
<tt class="descname">re_boundary_realignment</tt><em class="property"> = &lt;_sre.SRE_Pattern object at 0x105c52db0&gt;</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.re_boundary_realignment" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to realign punctuation that should be included in a sentence
although it follows the period (or ?, !).</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.sent_end_chars">
<tt class="descname">sent_end_chars</tt><em class="property"> = ('.', '?', '!')</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.sent_end_chars" title="Permalink to this definition">¶</a></dt>
<dd><p>Characters which are candidates for sentence boundaries</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktLanguageVars.word_tokenize">
<tt class="descname">word_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a string to split of punctuation other than periods</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktParameters">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktParameters</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stores data used to perform sentence boundary detection with punkt.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.abbrev_types">
<tt class="descname">abbrev_types</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for known abbreviations.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.add_ortho_context">
<tt class="descname">add_ortho_context</tt><big>(</big><em>typ</em>, <em>flag</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.add_ortho_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.add_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_abbrevs">
<tt class="descname">clear_abbrevs</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_abbrevs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_abbrevs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_collocations">
<tt class="descname">clear_collocations</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_collocations"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_collocations" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_ortho_context">
<tt class="descname">clear_ortho_context</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_ortho_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_sent_starters">
<tt class="descname">clear_sent_starters</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_sent_starters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_sent_starters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.collocations">
<tt class="descname">collocations</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.collocations" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word type tuples for known common collocations
where the first word ends in a period.  E.g., (&#8216;S.&#8217;, &#8216;Bach&#8217;)
is a common collocation in a text that discusses &#8216;Johann
S. Bach&#8217;.  These count as negative evidence for sentence
boundaries.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.ortho_context">
<tt class="descname">ortho_context</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.ortho_context" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary mapping word types to the set of orthographic
contexts that word type appears in.  Contexts are represented
by adding orthographic context flags: ...</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.sent_starters">
<tt class="descname">sent_starters</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.sent_starters" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for words that often appear at the
beginning of sentences.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktSentenceTokenizer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt.PunktLanguageVars object at 0x105cc7090&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt.PunktToken'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt>, <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A sentence tokenizer which uses an unsupervised algorithm to build
a model for abbreviation words, collocations, and words that start
sentences; and then uses that model to find sentence boundaries.
This approach has been shown to work well for many European
languages.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION">
<tt class="descname">PUNCTUATION</tt><em class="property"> = (';', ':', ',', '.', '!', '?')</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.dump">
<tt class="descname">dump</tt><big>(</big><em>tokens</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.dump"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.dump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text">
<tt class="descname">sentences_from_text</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text by only
testing candidate sentence breaks. If realign_boundaries is
True, includes in the sentence closing punctuation that
follows the period.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy">
<tt class="descname">sentences_from_text_legacy</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text_legacy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as L{sentences_from_text}.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens">
<tt class="descname">sentences_from_tokens</tt><big>(</big><em>tokens</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a sequence of tokens, generates lists of tokens, each list
corresponding to a sentence.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the (start, end) spans of sentences
in the text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak">
<tt class="descname">text_contains_sentbreak</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.text_contains_sentbreak"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the given text includes a sentence break.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the sentences in that text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.train">
<tt class="descname">train</tt><big>(</big><em>train_text</em>, <em>verbose=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktToken">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktToken</tt><big>(</big><em>tok</em>, <em>**params</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stores a token of text with annotations produced during
sentence boundary detection.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.abbr">
<tt class="descname">abbr</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.abbr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.ellipsis">
<tt class="descname">ellipsis</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.ellipsis" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.first_case">
<tt class="descname">first_case</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.first_case"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_case" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.first_lower">
<tt class="descname">first_lower</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.first_lower"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_lower" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token&#8217;s first character is lowercase.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.first_upper">
<tt class="descname">first_upper</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.first_upper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.first_upper" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token&#8217;s first character is uppercase.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.is_alpha">
<tt class="descname">is_alpha</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.is_alpha"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is all alphabetic.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.is_ellipsis">
<tt class="descname">is_ellipsis</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.is_ellipsis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_ellipsis" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of an ellipsis.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.is_initial">
<tt class="descname">is_initial</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.is_initial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of an initial.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.is_non_punct">
<tt class="descname">is_non_punct</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.is_non_punct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_non_punct" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token is either a number or is alphabetic.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.is_number">
<tt class="descname">is_number</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.is_number"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.is_number" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the token text is that of a number.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.linestart">
<tt class="descname">linestart</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.linestart" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.parastart">
<tt class="descname">parastart</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.parastart" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.period_final">
<tt class="descname">period_final</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.period_final" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.sentbreak">
<tt class="descname">sentbreak</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.sentbreak" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.tok">
<tt class="descname">tok</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.tok" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.type">
<tt class="descname">type</tt><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.type_no_period">
<tt class="descname">type_no_period</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.type_no_period"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type_no_period" title="Permalink to this definition">¶</a></dt>
<dd><p>The type with its final period removed if it has one.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktToken.type_no_sentperiod">
<tt class="descname">type_no_sentperiod</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktToken.type_no_sentperiod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktToken.type_no_sentperiod" title="Permalink to this definition">¶</a></dt>
<dd><p>The type with its final period removed if it is marked as a
sentence break.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktTrainer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktTrainer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt.PunktLanguageVars object at 0x105cb8f90&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt.PunktToken'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt></p>
<p>Learns parameters used in Punkt sentence boundary detection.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV">
<tt class="descname">ABBREV</tt><em class="property"> = 0.29999999999999999</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV" title="Permalink to this definition">¶</a></dt>
<dd><p>cut-off value whether a &#8216;token&#8217; is an abbreviation</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF">
<tt class="descname">ABBREV_BACKOFF</tt><em class="property"> = 5</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF" title="Permalink to this definition">¶</a></dt>
<dd><p>upper cut-off for Mikheev&#8217;s(2002) abbreviation detection algorithm</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.COLLOCATION">
<tt class="descname">COLLOCATION</tt><em class="property"> = 7.8799999999999999</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.COLLOCATION" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that two tokens need to be considered
as a collocation</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY">
<tt class="descname">IGNORE_ABBREV_PENALTY</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY" title="Permalink to this definition">¶</a></dt>
<dd><p>allows the disabling of the abbreviation penalty heuristic, which
exponentially disadvantages words that are found at times without a
final period.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS">
<tt class="descname">INCLUDE_ABBREV_COLLOCS</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word is an abbreviation. Such collocations override the orthographic
heuristic, but not the sentence starter heuristic. This is overridden by
INCLUDE_ALL_COLLOCS, and if both are false, only collocations with initials
and ordinals are considered.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS">
<tt class="descname">INCLUDE_ALL_COLLOCS</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word ends in a period. It may be useful in corpora where there is a lot
of variation that makes abbreviations like Mr difficult to identify.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ">
<tt class="descname">MIN_COLLOC_FREQ</tt><em class="property"> = 1</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ" title="Permalink to this definition">¶</a></dt>
<dd><p>this sets a minimum bound on the number of times a bigram needs to
appear before it can be considered a collocation, in addition to log
likelihood statistics. This is useful when INCLUDE_ALL_COLLOCS is True.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.SENT_STARTER">
<tt class="descname">SENT_STARTER</tt><em class="property"> = 30</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.SENT_STARTER" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that a token requires to be considered
as a frequent sentence starter</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.finalize_training">
<tt class="descname">finalize_training</tt><big>(</big><em>verbose=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.finalize_training"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.finalize_training" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses data that has been gathered in training to determine likely
collocations and sentence starters.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.find_abbrev_types">
<tt class="descname">find_abbrev_types</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.find_abbrev_types"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.find_abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>Recalculates abbreviations given type frequencies, despite no prior
determination of abbreviations.
This fails to include abbreviations otherwise found as &#8220;rare&#8221;.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.freq_threshold">
<tt class="descname">freq_threshold</tt><big>(</big><em>ortho_thresh=2</em>, <em>type_thresh=2</em>, <em>colloc_thres=2</em>, <em>sentstart_thresh=2</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.freq_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.freq_threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows memory use to be reduced after much training by removing data
about rare tokens that are unlikely to have a statistical effect with
further training. Entries occurring above the given thresholds will be
retained.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.get_params">
<tt class="descname">get_params</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.get_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates and returns parameters for sentence boundary detection as
derived from training.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train">
<tt class="descname">train</tt><big>(</big><em>text</em>, <em>verbose=False</em>, <em>finalize=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given text. If finalize is True, it
will determine all the parameters for sentence boundary detection. If
not, this will be delayed until get_params() or finalize_training() is
called. If verbose is True, abbreviations found will be listed.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train_tokens">
<tt class="descname">train_tokens</tt><big>(</big><em>tokens</em>, <em>verbose=False</em>, <em>finalize=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given list of tokens.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktWordTokenizer</tt><big>(</big><em>lang_vars=&lt;nltk.tokenize.punkt.PunktLanguageVars object at 0x105cb8d90&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktWordTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<dl class="method">
<dt id="nltk.tokenize.punkt.PunktWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktWordTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.punkt.main">
<tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">main</tt><big>(</big><em>text</em>, <em>tok_cls=&lt;class 'nltk.tokenize.punkt.PunktSentenceTokenizer'&gt;</em>, <em>train_cls=&lt;class 'nltk.tokenize.punkt.PunktTrainer'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.main" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a punkt model and applies it to the same text</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.regexp">
<span id="regexp-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">regexp</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.regexp" title="Permalink to this headline">¶</a></h2>
<p>Tokenizers that divide strings into substrings using regular
expressions that can match either tokens or separators between tokens.</p>
<dl class="class">
<dt id="nltk.tokenize.regexp.BlanklineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">BlanklineTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#BlanklineTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.BlanklineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
sequence of blank lines as a separator.  Blank lines are defined
as lines containing no characters, or containing only space
(C{&#8216; &#8216;}) or tab (C{&#8216;        &#8216;}) characters.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.RegexpTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">RegexpTokenizer</tt><big>(</big><em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that splits a string into substrings using a regular
expression.  The regular expression can be specified to match
either tokens or separators between tokens.</p>
<p>Unlike C{re.findall()} and C{re.split()}, C{RegexpTokenizer} does
not treat regular expressions that contain grouping parenthases
specially.</p>
<dl class="method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.WhitespaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">WhitespaceTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WhitespaceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
sequence of whitespace characters as a separator.  Whitespace
characters are space (C{&#8216; &#8216;}), tab (C{&#8216;t&#8217;}), and newline
(C{&#8216;n&#8217;}).  If you are performing the tokenization yourself
(rather than building a tokenizer to pass to some other piece of
code), consider using the string C{split()} method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.WordPunctTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">WordPunctTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WordPunctTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.WordPunctTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>A tokenizer that divides a text into sequences of alphabetic and
non-alphabetic characters.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">WordPunctTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&quot;She said &#39;hello&#39;.&quot;</span><span class="p">)</span>
<span class="go">[&#39;She&#39;, &#39;said&#39;, &quot;&#39;&quot;, &#39;hello&#39;, &quot;&#39;.&quot;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.regexp.regexp_tokenize">
<tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">regexp_tokenize</tt><big>(</big><em>text</em>, <em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#regexp_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.regexp_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Split the given text string, based on the given regular expression
pattern.  See the documentation for L{RegexpTokenizer.tokenize()}
for descriptions of the arguments.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.sexpr">
<span id="sexpr-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">sexpr</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.sexpr" title="Permalink to this headline">¶</a></h2>
<p>A tokenizer that divides strings into s-expressions.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sexpr_tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<dl class="class">
<dt id="nltk.tokenize.sexpr.SExprTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.sexpr.</tt><tt class="descname">SExprTokenizer</tt><big>(</big><em>parens='()'</em>, <em>strict=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides strings into X{s-expressions}.  An
s-expresion can be either:</p>
<blockquote>
<div><ul class="simple">
<li>A parenthasized expression, including any nested parenthasized
expressions.</li>
<li>A sequence of non-whitespace non-parenthasis characters.</li>
</ul>
</div></blockquote>
<p>For example, the string C{&#8216;(a (b c)) d e (f)&#8217;} consists of four
s-expressions: C{&#8216;(a (b c))&#8217;}, C{&#8216;d&#8217;}, C{&#8216;e&#8217;}, and C{&#8216;(f)&#8217;}.</p>
<dl class="method">
<dt id="nltk.tokenize.sexpr.SExprTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize the text into s-expressions.  For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parenthases are assumed to mark sexprs.  In particular, no
special processing is done to exclude parenthases that occur
inside strings, or following backslash characters.</p>
<p>If the given expression contains non-matching parenthases,
then the behavior of the tokenizer depends on the C{strict}
parameter to the constructor.  If C{strict} is C{True}, then
raise a C{ValueError}.  If C{strict} is C{False}, then any
unmatched close parenthases will be listed as their own
s-expression; and the last partial sexpr with unmatched open
parenthases will be listed as its own sexpr:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<p>&#64;param text: the string to be tokenized
&#64;type text: C{string} or C{iter(string)}
&#64;return: An iterator over tokens (each of which is an s-expression)</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.sexpr.demo">
<tt class="descclassname">nltk.tokenize.sexpr.</tt><tt class="descname">demo</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#demo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.simple">
<span id="simple-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">simple</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.simple" title="Permalink to this headline">¶</a></h2>
<p>Tokenizers that divide strings into substrings using the string
C{split()} method.</p>
<p>These tokenizers follow the standard L{TokenizerI} interface, and so
can be used with any code that expects a tokenizer.  For example,
these tokenizers can be used to specify the tokenization conventions
when building a L{CorpusReader&lt;nltk.corpus.reader.api.CorpusReader&gt;}.
But if you are tokenizing a string yourself, consider using string
C{split()} method directly instead.</p>
<dl class="class">
<dt id="nltk.tokenize.simple.CharTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">CharTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>A tokenizer that produces individual characters.  If you are performing
the tokenization yourself (rather than building a tokenizer to pass to
some other piece of code), consider iterating over the characters of
the string directly instead: for char in string</p>
<dl class="method">
<dt id="nltk.tokenize.simple.CharTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.simple.CharTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.LineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">LineTokenizer</tt><big>(</big><em>blanklines='discard'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single newline character as a separator.  Handling of blank lines
may be controlled using a constructor parameter.</p>
<dl class="method">
<dt id="nltk.tokenize.simple.LineTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.simple.LineTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.SpaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">SpaceTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#SpaceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.SpaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single space character as a separator.  If you are performing the
tokenization yourself (rather than building a tokenizer to pass to
some other piece of code), consider using the string C{split()}
method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.TabTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">TabTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#TabTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.TabTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by treating any
single tab character as a separator.  If you are performing the
tokenization yourself (rather than building a tokenizer to pass to
some other piece of code), consider using the string C{split()}
method instead:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.simple.line_tokenize">
<tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">line_tokenize</tt><big>(</big><em>text</em>, <em>blanklines='discard'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#line_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.line_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.texttiling">
<span id="texttiling-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">texttiling</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.texttiling" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TextTilingTokenizer</tt><big>(</big><em>w=20, k=10, similarity_method=0, stopwords=None, smoothing_method=[0], smoothing_width=2, smoothing_rounds=1, cutoff_policy=1, demo_mode=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A section tokenizer based on the TextTiling algorithm. The
algorithm detects subtopic shifts based on the analysis of lexical
co-occurence patterns.</p>
<p>The process starts by tokenizing the text into pseudosentences of
a fixed size w. Then, depending on the method used, similarity
scores are assigned at sentence gaps. The algorithm proceeds by
detecting the peak differences between these scores and marking
them as boundaries. The boundaries are normalized to the closest
paragraph break and the segmented text is returned.</p>
<p>&#64;type w: number
&#64;param w: Pseudosentence size
&#64;type k: number
&#64;param k: Size(in sentences) of the block used in the block comparison</p>
<blockquote>
<div>method</div></blockquote>
<p>&#64;type similarity_method: constant
&#64;param similarity_method: The method used for determining similarity scores
&#64;type stopwords: list
&#64;param stopwords: A list of stopwords that are filtered out
&#64;type smoothing_method: constant
&#64;param smoothing_method: The method used for smoothing the score plot
&#64;type smoothing_width: number
&#64;param smoothing_width: The width of the window used by the smoothing</p>
<blockquote>
<div>method</div></blockquote>
<p>&#64;type smoothing_rounds: number
&#64;param smoothing_rounds: The number of smoothing passes
&#64;type cutoff_policy: constant
&#64;param cutoff_policy: The policy used to determine the number of boundaries</p>
<dl class="method">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>The main function. Follows a pipeline structure.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.texttiling.TokenSequence">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TokenSequence</tt><big>(</big><em>index</em>, <em>wrdindex_list</em>, <em>original_length=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A token list with its original length and its index</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.texttiling.TokenTableField">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TokenTableField</tt><big>(</big><em>first_pos</em>, <em>ts_occurences</em>, <em>total_count=1</em>, <em>par_count=1</em>, <em>last_par=0</em>, <em>last_tok_seq=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenTableField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenTableField" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A field in the token table holding parameters for each token,
used later in the process</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.texttiling.demo">
<tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">demo</tt><big>(</big><em>text=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#demo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="nltk.tokenize.texttiling.smooth">
<tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">smooth</tt><big>(</big><em>x</em>, <em>window_len=11</em>, <em>window='flat'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#smooth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.smooth" title="Permalink to this definition">¶</a></dt>
<dd><p>smooth the data using a window with requested size.</p>
<p>This method is based on the convolution of a scaled window with the signal.
The signal is prepared by introducing reflected copies of the signal 
(with the window size) in both ends so that transient parts are minimized
in the begining and end part of the output signal.</p>
<dl class="docutils">
<dt>input:</dt>
<dd><p class="first">x: the input signal 
window_len: the dimension of the smoothing window; should be an odd integer
window: the type of window from &#8216;flat&#8217;, &#8216;hanning&#8217;, &#8216;hamming&#8217;, &#8216;bartlett&#8217;, &#8216;blackman&#8217;</p>
<blockquote class="last">
<div>flat window will produce a moving average smoothing.</div></blockquote>
</dd>
<dt>output:</dt>
<dd>the smoothed signal</dd>
</dl>
<p>example:</p>
<p>t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)</p>
<p>see also:</p>
<p>numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
scipy.signal.lfilter</p>
<p>TODO: the window parameter could be the window itself if an array instead of a string</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.treebank">
<span id="treebank-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">treebank</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.treebank" title="Permalink to this headline">¶</a></h2>
<p>A regular-expression based word tokenizer that tokenizes sentences
using the conventions used by the Penn Treebank.</p>
<dl class="class">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.treebank.</tt><tt class="descname">TreebankWordTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A word tokenizer that tokenizes sentences using the conventions
used by the Penn Treebank.  Contractions, such as &#8220;can&#8217;t&#8221;, are
split in to two tokens.  E.g.:</p>
<blockquote>
<div><ul class="simple">
<li>can&#8217;t S{-&gt;} ca n&#8217;t</li>
<li>he&#8217;ll S{-&gt;} he &#8216;ll</li>
<li>weren&#8217;t S{-} were n&#8217;t</li>
</ul>
</div></blockquote>
<p>This tokenizer assumes that the text has already been segmented into
sentences.  Any periods &#8211; apart from those at the end of a string &#8211;
are assumed to be part of the word they are attached to (e.g. for
abbreviations, etc), and are not separately tokenized.</p>
<dl class="attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS2">
<tt class="descname">CONTRACTIONS2</tt><em class="property"> = [&lt;_sre.SRE_Pattern object at 0x105c7a6c0&gt;, &lt;_sre.SRE_Pattern object at 0x105cb53c0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca16f0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1780&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1810&gt;, &lt;_sre.SRE_Pattern object at 0x105ca18a0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1930&gt;, &lt;_sre.SRE_Pattern object at 0x105ca19c0&gt;, &lt;_sre.SRE_Pattern object at 0x105c958b0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1a50&gt;, &lt;_sre.SRE_Pattern object at 0x105ca1ae0&gt;]</em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS3">
<tt class="descname">CONTRACTIONS3</tt><em class="property"> = [&lt;_sre.SRE_Pattern object at 0x105c7dea0&gt;, &lt;_sre.SRE_Pattern object at 0x105ca6ad0&gt;]</em><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.CONTRACTIONS3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.util">
<span id="util-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">util</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.util" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nltk.tokenize.util.regexp_span_tokenize">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">regexp_span_tokenize</tt><big>(</big><em>s</em>, <em>regexp</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#regexp_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.regexp_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens in the string, as defined by the token
delimiter regexp, and generate (start, end) offsets.</p>
<p>&#64;param s: the string to be tokenized
&#64;type s: C{str}
&#64;param regexp: the token separator regexp
&#64;type regexp: C{str}
&#64;rtype: C{iter} of C{tuple} of C{int}</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.util.spans_to_relative">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">spans_to_relative</tt><big>(</big><em>spans</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#spans_to_relative"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.spans_to_relative" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert absolute token spans to relative spans.</p>
<p>&#64;param spans: the (start, end) offsets of the tokens
&#64;type s: C{iter} of C{tuple} of C{int}
&#64;rtype: C{iter} of C{tuple} of C{int}</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.util.string_span_tokenize">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">string_span_tokenize</tt><big>(</big><em>s</em>, <em>sep</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#string_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.string_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens in the string, as defined by the token
delimiter, and generate (start, end) offsets.</p>
<p>&#64;param s: the string to be tokenized
&#64;type s: C{str}
&#64;param sep: the token separator
&#64;type sep: C{str}
&#64;rtype: C{iter} of C{tuple} of C{int}</p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table Of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">NLTK News</a></li>
</ul>

          <h3 style="margin-top: 1.5em;">Search</h3>
          <form class="search" action="../search.html" method="get">
            <input type="text" name="q" />
            <input type="submit" value="Go" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
          </form>
          <p class="searchtip" style="font-size: 90%">
            Enter search terms or a module, class or function name.
          </p>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             >index</a>
            <br/>
            <a href="../_sources/api/nltk.tokenize.txt"
               rel="nofollow">Show Source</a>
        </div>

        <div class="right">
          
    <div class="footer">
        &copy; Copyright 2011, Steven Bird.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>