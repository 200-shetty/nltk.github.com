

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>tokenize Package &mdash; NLTK 2.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="NLTK 2.0 documentation" href="../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../index.html">NLTK 2.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="tokenize-package">
<h1>tokenize Package<a class="headerlink" href="#tokenize-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2><tt class="xref py py-mod docutils literal"><span class="pre">tokenize</span></tt> Package<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<span class="target" id="module-nltk.tokenize"></span><p>This package contains several <em>tokenizers</em>, which break continuous text
into a sequence of units, such as words and punctuation.  Tokenizers operate on a string,
and return a sequence of strings, one per token.  The decision about which
tokenizer to use often depends on the particular application.</p>
<p>The most frequently used tokenizer is <tt class="docutils literal"><span class="pre">word_tokenize()</span></tt>, e.g.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="s">&quot;Good muffins cost $3.88 in New York.&quot;</span><span class="p">)</span>
<span class="go">[&#39;Good&#39;, &#39;muffins&#39;, &#39;cost&#39;, &#39;$&#39;, &#39;3.88&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>For more information about tokenization, please see the tokenizer HOWTO,
or chapter 3 of the NLTK book.</p>
<dl class="class">
<dt id="nltk.tokenize.WhitespaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">WhitespaceTokenizer</tt><a class="headerlink" href="#nltk.tokenize.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a string on whitespace (space, tab, newline).
In general, users should use <tt class="docutils literal"><span class="pre">str.split()</span></tt> instead, e.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="s">&quot;lorem ipsum&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.SpaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">SpaceTokenizer</tt><a class="headerlink" href="#nltk.tokenize.SpaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>Tokenize a string using the space character as a delimiter.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TabTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TabTokenizer</tt><a class="headerlink" href="#nltk.tokenize.TabTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>Tokenize a string use the tab character as a delimiter.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.LineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">LineTokenizer</tt><big>(</big><em>blanklines='discard'</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>Tokenize a string into its lines, optionally discarding blank lines.</p>
<dl class="method">
<dt id="nltk.tokenize.LineTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.LineTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="headerlink" href="#nltk.tokenize.LineTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.RegexpTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">RegexpTokenizer</tt><big>(</big><em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that splits a string using a regular expression, which
matches either the tokens or the separators between tokens.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s">&#39;\w+|\$[\d\.]+|\S+&#39;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pattern</strong> (<em>str</em>) &#8211; The pattern used to build this tokenizer.
(This pattern may safely contain grouping parentheses.)</li>
<li><strong>gaps</strong> (<em>bool</em>) &#8211; True if this tokenizer&#8217;s pattern should be used
to find separators between tokens; False if this
tokenizer&#8217;s pattern should be used to find the tokens
themselves.</li>
<li><strong>discard_empty</strong> (<em>bool</em>) &#8211; True if any empty tokens <cite>&#8216;&#8217;</cite>
generated by the tokenizer should be discarded.  Empty
tokens can only be generated if <cite>_gaps == True</cite>.</li>
<li><strong>flags</strong> (<em>int</em>) &#8211; The regexp flags used to compile this
tokenizer&#8217;s pattern.  By default, the following flags are
used: <cite>re.UNICODE | re.MULTILINE | re.DOTALL</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nltk.tokenize.RegexpTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.RegexpTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.RegexpTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.BlanklineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">BlanklineTokenizer</tt><a class="headerlink" href="#nltk.tokenize.BlanklineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a string, treating any sequence of blank lines as a delimiter.
Blank lines are defined as lines containing no characters, except for
space or tab characters.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.WordPunctTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">WordPunctTokenizer</tt><a class="headerlink" href="#nltk.tokenize.WordPunctTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a text into a sequence of alphabetic and
non-alphabetic characters.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.regexp</span> <span class="kn">import</span> <span class="n">WordPunctTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WordPunctTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&quot;She said &#39;hello&#39;.&quot;</span><span class="p">)</span>
<span class="go">[&#39;She&#39;, &#39;said&#39;, &quot;&#39;&quot;, &#39;hello&#39;, &quot;&#39;.&quot;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.regexp_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">regexp_tokenize</tt><big>(</big><em>text</em>, <em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="headerlink" href="#nltk.tokenize.regexp_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>.  See <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.RegexpTokenizer" title="nltk.tokenize.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">RegexpTokenizer</span></tt></a>
for descriptions of the arguments.</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.word_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">word_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#word_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.word_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>,
using NLTK&#8217;s recommended word tokenizer
(currently <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.TreebankWordTokenizer" title="nltk.tokenize.TreebankWordTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">TreebankWordTokenizer</span></tt></a>).
This tokenizer is designed to work on a sentence at a time.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.SExprTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">SExprTokenizer</tt><big>(</big><em>parens='()'</em>, <em>strict=True</em><big>)</big><a class="headerlink" href="#nltk.tokenize.SExprTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides strings into s-expressions.
An s-expresion can be either:</p>
<blockquote>
<div><ul class="simple">
<li>a parenthesized expression, including any nested parenthesized
expressions, or</li>
<li>a sequence of non-whitespace non-parenthesis characters.</li>
</ul>
</div></blockquote>
<p>For example, the string <tt class="docutils literal"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span> <span class="pre">d</span> <span class="pre">e</span> <span class="pre">(f)</span></tt> consists of four
s-expressions: <tt class="docutils literal"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span></tt>, <tt class="docutils literal"><span class="pre">d</span></tt>, <tt class="docutils literal"><span class="pre">e</span></tt>, and <tt class="docutils literal"><span class="pre">(f)</span></tt>.</p>
<dl class="method">
<dt id="nltk.tokenize.SExprTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.SExprTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of s-expressions extracted from <em>text</em>.
For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parentheses are assumed to mark s-expressions.
(No special processing is done to exclude parentheses that occur
inside strings, or following backslash characters.)</p>
<p>If the given expression contains non-matching parentheses,
then the behavior of the tokenizer depends on the C{strict}
parameter to the constructor.  If C{strict} is C{True}, then
raise a C{ValueError}.  If C{strict} is C{False}, then any
unmatched close parentheses will be listed as their own
s-expression; and the last partial s-expression with unmatched open
parentheses will be listed as its own s-expression:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>text</strong> (<em>str or iter(str)</em>) &#8211; the string to be tokenized</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">iter(str)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.line_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">line_tokenize</tt><big>(</big><em>text</em>, <em>blanklines='discard'</em><big>)</big><a class="headerlink" href="#nltk.tokenize.line_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="nltk.tokenize.PunktWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">PunktWordTokenizer</tt><big>(</big><em>lang_vars=&lt;nltk.tokenize.punkt._PunktLanguageVars object at 0x3f605d0&gt;</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<dl class="method">
<dt id="nltk.tokenize.PunktWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.PunktSentenceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">PunktSentenceTokenizer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt._PunktLanguageVars object at 0x3f60890&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt._PunktToken'&gt;</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt>, <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A sentence tokenizer which uses an unsupervised algorithm to build
a model for abbreviation words, collocations, and words that start
sentences; and then uses that model to find sentence boundaries.
This approach has been shown to work well for many European
languages.</p>
<dl class="attribute">
<dt id="nltk.tokenize.PunktSentenceTokenizer.PUNCTUATION">
<tt class="descname">PUNCTUATION</tt><em class="property"> = (';', ':', ',', '.', '!', '?')</em><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.dump">
<tt class="descname">dump</tt><big>(</big><em>tokens</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.dump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_text">
<tt class="descname">sentences_from_text</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text by only
testing candidate sentence breaks. If realign_boundaries is
True, includes in the sentence closing punctuation that
follows the period.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_text_legacy">
<tt class="descname">sentences_from_text_legacy</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_text_legacy" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as L{sentences_from_text}.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.sentences_from_tokens">
<tt class="descname">sentences_from_tokens</tt><big>(</big><em>tokens</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.sentences_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a sequence of tokens, generates lists of tokens, each list
corresponding to a sentence.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the (start, end) spans of sentences
in the text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.text_contains_sentbreak">
<tt class="descname">text_contains_sentbreak</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.text_contains_sentbreak" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the given text includes a sentence break.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the sentences in that text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.PunktSentenceTokenizer.train">
<tt class="descname">train</tt><big>(</big><em>train_text</em>, <em>verbose=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.PunktSentenceTokenizer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TreebankWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TreebankWordTokenizer</tt><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A word tokenizer that tokenizes sentences using the conventions
used by the Penn Treebank.  Contractions are split in to two tokens, e.g.:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">can't</span> <span class="pre">-&gt;</span> <span class="pre">ca</span> <span class="pre">n't</span></tt></li>
<li><tt class="docutils literal"><span class="pre">he'll</span> <span class="pre">-&gt;</span> <span class="pre">he</span> <span class="pre">'ll</span></tt></li>
<li><tt class="docutils literal"><span class="pre">weren't</span> <span class="pre">-&gt;</span> <span class="pre">were</span> <span class="pre">n't</span></tt></li>
</ul>
</div></blockquote>
<p>NB. this tokenizer assumes that the text is presented as one sentence per line,
where each line is delimited with a newline character.
The only periods to be treated as separate tokens are those appearing
at the end of a line.</p>
<dl class="method">
<dt id="nltk.tokenize.TreebankWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TreebankWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>, using the tokenization
conventions of the Penn Treebank.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.sent_tokenize">
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">sent_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#sent_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sent_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a sentence-tokenized copy of <em>text</em>,
using NLTK&#8217;s recommended sentence tokenizer
(currently <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.PunktSentenceTokenizer" title="nltk.tokenize.PunktSentenceTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">PunktSentenceTokenizer</span></tt></a>).</p>
</dd></dl>

<dl class="function">
<dt>
<tt class="descclassname">nltk.tokenize.</tt><tt class="descname">word_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize.html#word_tokenize"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Return a tokenized copy of <em>text</em>,
using NLTK&#8217;s recommended word tokenizer
(currently <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.TreebankWordTokenizer" title="nltk.tokenize.TreebankWordTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">TreebankWordTokenizer</span></tt></a>).
This tokenizer is designed to work on a sentence at a time.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.TextTilingTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.</tt><tt class="descname">TextTilingTokenizer</tt><big>(</big><em>w=20, k=10, similarity_method=0, stopwords=None, smoothing_method=[0], smoothing_width=2, smoothing_rounds=1, cutoff_policy=1, demo_mode=False</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TextTilingTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>Tokenize a document into topical sections using the TextTiling algorithm.
This algorithm detects subtopic shifts based on the analysis of lexical
co-occurrence patterns.</p>
<p>The process starts by tokenizing the text into pseudosentences of
a fixed size w. Then, depending on the method used, similarity
scores are assigned at sentence gaps. The algorithm proceeds by
detecting the peak differences between these scores and marking
them as boundaries. The boundaries are normalized to the closest
paragraph break and the segmented text is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> (<em>int</em>) &#8211; Pseudosentence size</li>
<li><strong>k</strong> (<em>int</em>) &#8211; Size (in sentences) of the block used in the block comparison method</li>
<li><strong>similarity_method</strong> (<em>constant</em>) &#8211; The method used for determining similarity scores:
<cite>BLOCK_COMPARISON</cite> (default) or <cite>VOCABULARY_INTRODUCTION</cite>.</li>
<li><strong>stopwords</strong> (<em>list(str)</em>) &#8211; A list of stopwords that are filtered out (defaults to NLTK&#8217;s stopwords corpus)</li>
<li><strong>smoothing_method</strong> (<em>constant</em>) &#8211; The method used for smoothing the score plot:
<cite>DEFAULT_SMOOTHING</cite> (default)</li>
<li><strong>smoothing_width</strong> (<em>int</em>) &#8211; The width of the window used by the smoothing method</li>
<li><strong>smoothing_rounds</strong> (<em>int</em>) &#8211; The number of smoothing passes</li>
<li><strong>cutoff_policy</strong> (<em>constant</em>) &#8211; The policy used to determine the number of boundaries:
<cite>HC</cite> (default) or <cite>LC</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nltk.tokenize.TextTilingTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="headerlink" href="#nltk.tokenize.TextTilingTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>, where each &#8220;token&#8221; represents
a separate topic.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.api">
<span id="api-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">api</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.api" title="Permalink to this headline">¶</a></h2>
<p>Tokenizer Interface</p>
<dl class="class">
<dt id="nltk.tokenize.api.StringTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.api.</tt><tt class="descname">StringTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides a string into substrings by splitting
on the specified string (defined in subclasses).</p>
<dl class="method">
<dt id="nltk.tokenize.api.StringTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.StringTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#StringTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.StringTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.api.TokenizerI">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.api.</tt><tt class="descname">TokenizerI</tt><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A processing interface for tokenizing a string.
Subclasses must define <tt class="docutils literal"><span class="pre">tokenize()</span></tt> or <tt class="docutils literal"><span class="pre">batch_tokenize()</span></tt> (or both).</p>
<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.batch_span_tokenize">
<tt class="descname">batch_span_tokenize</tt><big>(</big><em>strings</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.batch_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.batch_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.span_tokenize()} to each element of C{strings}.  I.e.:</p>
<blockquote>
<div>return [self.span_tokenize(s) for s in strings]</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">iter(list(tuple(int, int)))</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.batch_tokenize">
<tt class="descname">batch_tokenize</tt><big>(</big><em>strings</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.batch_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.batch_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply L{self.tokenize()} to each element of C{strings}.  I.e.:</p>
<blockquote>
<div>return [self.tokenize(s) for s in strings]</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list(list(str))</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Identify the tokens using integer offsets (start_i, end_i),
where s[start_i:end_i] is the corresponding token.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">iter(tuple(int, int))</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.api.TokenizerI.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/api.html#TokenizerI.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.api.TokenizerI.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>s</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of str</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.punkt">
<span id="punkt-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">punkt</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.punkt" title="Permalink to this headline">¶</a></h2>
<p>The Punkt sentence tokenizer.  The algorithm for this tokenizer is
described in Kiss &amp; Strunk (2006):</p>
<div class="highlight-python"><pre>Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence
  Boundary Detection.  Computational Linguistics 32: 485-525.</pre>
</div>
<dl class="class">
<dt id="nltk.tokenize.punkt.PunktParameters">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktParameters</tt><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Stores data used to perform sentence boundary detection with Punkt.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.abbrev_types">
<tt class="descname">abbrev_types</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for known abbreviations.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.add_ortho_context">
<tt class="descname">add_ortho_context</tt><big>(</big><em>typ</em>, <em>flag</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.add_ortho_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.add_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_abbrevs">
<tt class="descname">clear_abbrevs</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_abbrevs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_abbrevs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_collocations">
<tt class="descname">clear_collocations</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_collocations"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_collocations" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_ortho_context">
<tt class="descname">clear_ortho_context</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_ortho_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_ortho_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktParameters.clear_sent_starters">
<tt class="descname">clear_sent_starters</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktParameters.clear_sent_starters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.clear_sent_starters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.collocations">
<tt class="descname">collocations</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.collocations" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word type tuples for known common collocations
where the first word ends in a period.  E.g., (&#8216;S.&#8217;, &#8216;Bach&#8217;)
is a common collocation in a text that discusses &#8216;Johann
S. Bach&#8217;.  These count as negative evidence for sentence
boundaries.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.ortho_context">
<tt class="descname">ortho_context</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.ortho_context" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary mapping word types to the set of orthographic
contexts that word type appears in.  Contexts are represented
by adding orthographic context flags: ...</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktParameters.sent_starters">
<tt class="descname">sent_starters</tt><em class="property"> = None</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktParameters.sent_starters" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of word types for words that often appear at the
beginning of sentences.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktSentenceTokenizer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt._PunktLanguageVars object at 0x3f60890&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt._PunktToken'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt>, <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A sentence tokenizer which uses an unsupervised algorithm to build
a model for abbreviation words, collocations, and words that start
sentences; and then uses that model to find sentence boundaries.
This approach has been shown to work well for many European
languages.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION">
<tt class="descname">PUNCTUATION</tt><em class="property"> = (';', ':', ',', '.', '!', '?')</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.PUNCTUATION" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.dump">
<tt class="descname">dump</tt><big>(</big><em>tokens</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.dump"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.dump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text">
<tt class="descname">sentences_from_text</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text by only
testing candidate sentence breaks. If realign_boundaries is
True, includes in the sentence closing punctuation that
follows the period.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy">
<tt class="descname">sentences_from_text_legacy</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_text_legacy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_text_legacy" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, generates the sentences in that text. Annotates all
tokens, rather than just those with possible sentence breaks. Should
produce the same results as L{sentences_from_text}.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens">
<tt class="descname">sentences_from_tokens</tt><big>(</big><em>tokens</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.sentences_from_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.sentences_from_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a sequence of tokens, generates lists of tokens, each list
corresponding to a sentence.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the (start, end) spans of sentences
in the text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak">
<tt class="descname">text_contains_sentbreak</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.text_contains_sentbreak"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.text_contains_sentbreak" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the given text includes a sentence break.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em>, <em>realign_boundaries=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text, returns a list of the sentences in that text.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktSentenceTokenizer.train">
<tt class="descname">train</tt><big>(</big><em>train_text</em>, <em>verbose=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktSentenceTokenizer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktSentenceTokenizer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Derives parameters from a given training text, or uses the parameters
given. Repeated calls to this method destroy previous parameters. For
incremental training, instantiate a separate PunktTrainer instance.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktTrainer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktTrainer</tt><big>(</big><em>train_text=None</em>, <em>verbose=False</em>, <em>lang_vars=&lt;nltk.tokenize.punkt._PunktLanguageVars object at 0x3f607d0&gt;</em>, <em>token_cls=&lt;class 'nltk.tokenize.punkt._PunktToken'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.punkt._PunktBaseClass</span></tt></p>
<p>Learns parameters used in Punkt sentence boundary detection.</p>
<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV">
<tt class="descname">ABBREV</tt><em class="property"> = 0.29999999999999999</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV" title="Permalink to this definition">¶</a></dt>
<dd><p>cut-off value whether a &#8216;token&#8217; is an abbreviation</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF">
<tt class="descname">ABBREV_BACKOFF</tt><em class="property"> = 5</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.ABBREV_BACKOFF" title="Permalink to this definition">¶</a></dt>
<dd><p>upper cut-off for Mikheev&#8217;s(2002) abbreviation detection algorithm</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.COLLOCATION">
<tt class="descname">COLLOCATION</tt><em class="property"> = 7.8799999999999999</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.COLLOCATION" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that two tokens need to be considered
as a collocation</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY">
<tt class="descname">IGNORE_ABBREV_PENALTY</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.IGNORE_ABBREV_PENALTY" title="Permalink to this definition">¶</a></dt>
<dd><p>allows the disabling of the abbreviation penalty heuristic, which
exponentially disadvantages words that are found at times without a
final period.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS">
<tt class="descname">INCLUDE_ABBREV_COLLOCS</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ABBREV_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word is an abbreviation. Such collocations override the orthographic
heuristic, but not the sentence starter heuristic. This is overridden by
INCLUDE_ALL_COLLOCS, and if both are false, only collocations with initials
and ordinals are considered.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS">
<tt class="descname">INCLUDE_ALL_COLLOCS</tt><em class="property"> = False</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.INCLUDE_ALL_COLLOCS" title="Permalink to this definition">¶</a></dt>
<dd><p>this includes as potential collocations all word pairs where the first
word ends in a period. It may be useful in corpora where there is a lot
of variation that makes abbreviations like Mr difficult to identify.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ">
<tt class="descname">MIN_COLLOC_FREQ</tt><em class="property"> = 1</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.MIN_COLLOC_FREQ" title="Permalink to this definition">¶</a></dt>
<dd><p>this sets a minimum bound on the number of times a bigram needs to
appear before it can be considered a collocation, in addition to log
likelihood statistics. This is useful when INCLUDE_ALL_COLLOCS is True.</p>
</dd></dl>

<dl class="attribute">
<dt id="nltk.tokenize.punkt.PunktTrainer.SENT_STARTER">
<tt class="descname">SENT_STARTER</tt><em class="property"> = 30</em><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.SENT_STARTER" title="Permalink to this definition">¶</a></dt>
<dd><p>minimal log-likelihood value that a token requires to be considered
as a frequent sentence starter</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.finalize_training">
<tt class="descname">finalize_training</tt><big>(</big><em>verbose=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.finalize_training"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.finalize_training" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses data that has been gathered in training to determine likely
collocations and sentence starters.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.find_abbrev_types">
<tt class="descname">find_abbrev_types</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.find_abbrev_types"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.find_abbrev_types" title="Permalink to this definition">¶</a></dt>
<dd><p>Recalculates abbreviations given type frequencies, despite no prior
determination of abbreviations.
This fails to include abbreviations otherwise found as &#8220;rare&#8221;.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.freq_threshold">
<tt class="descname">freq_threshold</tt><big>(</big><em>ortho_thresh=2</em>, <em>type_thresh=2</em>, <em>colloc_thres=2</em>, <em>sentstart_thresh=2</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.freq_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.freq_threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows memory use to be reduced after much training by removing data
about rare tokens that are unlikely to have a statistical effect with
further training. Entries occurring above the given thresholds will be
retained.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.get_params">
<tt class="descname">get_params</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.get_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates and returns parameters for sentence boundary detection as
derived from training.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train">
<tt class="descname">train</tt><big>(</big><em>text</em>, <em>verbose=False</em>, <em>finalize=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given text. If finalize is True, it
will determine all the parameters for sentence boundary detection. If
not, this will be delayed until get_params() or finalize_training() is
called. If verbose is True, abbreviations found will be listed.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.tokenize.punkt.PunktTrainer.train_tokens">
<tt class="descname">train_tokens</tt><big>(</big><em>tokens</em>, <em>verbose=False</em>, <em>finalize=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktTrainer.train_tokens"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktTrainer.train_tokens" title="Permalink to this definition">¶</a></dt>
<dd><p>Collects training data from a given list of tokens.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.punkt.PunktWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">PunktWordTokenizer</tt><big>(</big><em>lang_vars=&lt;nltk.tokenize.punkt._PunktLanguageVars object at 0x3f605d0&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktWordTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<dl class="method">
<dt id="nltk.tokenize.punkt.PunktWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#PunktWordTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.PunktWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.punkt.main">
<tt class="descclassname">nltk.tokenize.punkt.</tt><tt class="descname">main</tt><big>(</big><em>text</em>, <em>tok_cls=&lt;class 'nltk.tokenize.punkt.PunktSentenceTokenizer'&gt;</em>, <em>train_cls=&lt;class 'nltk.tokenize.punkt.PunktTrainer'&gt;</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/punkt.html#main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.punkt.main" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a punkt model and applies it to the same text</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.regexp">
<span id="regexp-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">regexp</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.regexp" title="Permalink to this headline">¶</a></h2>
<p>Tokenizers that divide strings into substrings using regular
expressions that can match either tokens or separators between tokens.</p>
<dl class="class">
<dt id="nltk.tokenize.regexp.BlanklineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">BlanklineTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#BlanklineTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.BlanklineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a string, treating any sequence of blank lines as a delimiter.
Blank lines are defined as lines containing no characters, except for
space or tab characters.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.RegexpTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">RegexpTokenizer</tt><big>(</big><em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that splits a string using a regular expression, which
matches either the tokens or the separators between tokens.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s">&#39;\w+|\$[\d\.]+|\S+&#39;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>pattern</strong> (<em>str</em>) &#8211; The pattern used to build this tokenizer.
(This pattern may safely contain grouping parentheses.)</li>
<li><strong>gaps</strong> (<em>bool</em>) &#8211; True if this tokenizer&#8217;s pattern should be used
to find separators between tokens; False if this
tokenizer&#8217;s pattern should be used to find the tokens
themselves.</li>
<li><strong>discard_empty</strong> (<em>bool</em>) &#8211; True if any empty tokens <cite>&#8216;&#8217;</cite>
generated by the tokenizer should be discarded.  Empty
tokens can only be generated if <cite>_gaps == True</cite>.</li>
<li><strong>flags</strong> (<em>int</em>) &#8211; The regexp flags used to compile this
tokenizer&#8217;s pattern.  By default, the following flags are
used: <cite>re.UNICODE | re.MULTILINE | re.DOTALL</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.regexp.RegexpTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#RegexpTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.RegexpTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.WhitespaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">WhitespaceTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WhitespaceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a string on whitespace (space, tab, newline).
In general, users should use <tt class="docutils literal"><span class="pre">str.split()</span></tt> instead, e.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="s">&quot;lorem ipsum&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.regexp.WordPunctTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">WordPunctTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#WordPunctTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.WordPunctTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.regexp.RegexpTokenizer</span></tt></a></p>
<p>Tokenize a text into a sequence of alphabetic and
non-alphabetic characters.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.regexp</span> <span class="kn">import</span> <span class="n">WordPunctTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WordPunctTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&quot;She said &#39;hello&#39;.&quot;</span><span class="p">)</span>
<span class="go">[&#39;She&#39;, &#39;said&#39;, &quot;&#39;&quot;, &#39;hello&#39;, &quot;&#39;.&quot;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.regexp.regexp_tokenize">
<tt class="descclassname">nltk.tokenize.regexp.</tt><tt class="descname">regexp_tokenize</tt><big>(</big><em>text</em>, <em>pattern</em>, <em>gaps=False</em>, <em>discard_empty=True</em>, <em>flags=56</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/regexp.html#regexp_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.regexp.regexp_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>.  See <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer" title="nltk.tokenize.regexp.RegexpTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">RegexpTokenizer</span></tt></a>
for descriptions of the arguments.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.sexpr">
<span id="sexpr-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">sexpr</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.sexpr" title="Permalink to this headline">¶</a></h2>
<p>A tokenizer that divides strings into s-expressions.  E.g.:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sexpr_tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<dl class="class">
<dt id="nltk.tokenize.sexpr.SExprTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.sexpr.</tt><tt class="descname">SExprTokenizer</tt><big>(</big><em>parens='()'</em>, <em>strict=True</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A tokenizer that divides strings into s-expressions.
An s-expresion can be either:</p>
<blockquote>
<div><ul class="simple">
<li>a parenthesized expression, including any nested parenthesized
expressions, or</li>
<li>a sequence of non-whitespace non-parenthesis characters.</li>
</ul>
</div></blockquote>
<p>For example, the string <tt class="docutils literal"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span> <span class="pre">d</span> <span class="pre">e</span> <span class="pre">(f)</span></tt> consists of four
s-expressions: <tt class="docutils literal"><span class="pre">(a</span> <span class="pre">(b</span> <span class="pre">c))</span></tt>, <tt class="docutils literal"><span class="pre">d</span></tt>, <tt class="docutils literal"><span class="pre">e</span></tt>, and <tt class="docutils literal"><span class="pre">(f)</span></tt>.</p>
<dl class="method">
<dt id="nltk.tokenize.sexpr.SExprTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#SExprTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.SExprTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of s-expressions extracted from <em>text</em>.
For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;(a b (c d)) e f (g)&#39;</span><span class="p">)</span>
<span class="go">[&#39;(a b (c d))&#39;, &#39;e&#39;, &#39;f&#39;, &#39;(g)&#39;]</span>
</pre></div>
</div>
<p>All parentheses are assumed to mark s-expressions.
(No special processing is done to exclude parentheses that occur
inside strings, or following backslash characters.)</p>
<p>If the given expression contains non-matching parentheses,
then the behavior of the tokenizer depends on the C{strict}
parameter to the constructor.  If C{strict} is C{True}, then
raise a C{ValueError}.  If C{strict} is C{False}, then any
unmatched close parentheses will be listed as their own
s-expression; and the last partial s-expression with unmatched open
parentheses will be listed as its own s-expression:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">SExprTokenizer</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">&#39;c) d) e (f (g&#39;</span><span class="p">)</span>
<span class="go">[&#39;c&#39;, &#39;)&#39;, &#39;d&#39;, &#39;)&#39;, &#39;e&#39;, &#39;(f (g&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>text</strong> (<em>str or iter(str)</em>) &#8211; the string to be tokenized</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">iter(str)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.sexpr.demo">
<tt class="descclassname">nltk.tokenize.sexpr.</tt><tt class="descname">demo</tt><big>(</big><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/sexpr.html#demo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.sexpr.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.simple">
<span id="simple-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">simple</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.simple" title="Permalink to this headline">¶</a></h2>
<p>Tokenizers that divide strings into substrings using the string
<tt class="docutils literal"><span class="pre">split()</span></tt> method.</p>
<p>These tokenizers implement the <tt class="docutils literal"><span class="pre">TokenizerI</span></tt> interface, and so
can be used with any code that expects a tokenizer, e.g.
<a class="reference internal" href="../web/api/nltk.corpus.reader.html#nltk.corpus.reader.CorpusReader" title="nltk.corpus.reader.CorpusReader"><tt class="xref py py-class docutils literal"><span class="pre">CorpusReader</span></tt></a>.</p>
<p>When tokenizing using a particular delimiter string, consider using
the string <tt class="docutils literal"><span class="pre">split()</span></tt> method directly, as this is more efficient.</p>
<dl class="class">
<dt id="nltk.tokenize.simple.CharTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">CharTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>Tokenize a string into individual characters.  If this functionality
is ever required directly, use <tt class="docutils literal"><span class="pre">for</span> <span class="pre">char</span> <span class="pre">in</span> <span class="pre">string</span></tt>.</p>
<dl class="method">
<dt id="nltk.tokenize.simple.CharTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.simple.CharTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#CharTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.CharTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.LineTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">LineTokenizer</tt><big>(</big><em>blanklines='discard'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>Tokenize a string into its lines, optionally discarding blank lines.</p>
<dl class="method">
<dt id="nltk.tokenize.simple.LineTokenizer.span_tokenize">
<tt class="descname">span_tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.tokenize.simple.LineTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>s</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#LineTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.LineTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.SpaceTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">SpaceTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#SpaceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.SpaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>Tokenize a string using the space character as a delimiter.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.simple.TabTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">TabTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#TabTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.TabTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.StringTokenizer" title="nltk.tokenize.api.StringTokenizer"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.StringTokenizer</span></tt></a></p>
<p>Tokenize a string use the tab character as a delimiter.</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.simple.line_tokenize">
<tt class="descclassname">nltk.tokenize.simple.</tt><tt class="descname">line_tokenize</tt><big>(</big><em>text</em>, <em>blanklines='discard'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/simple.html#line_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.simple.line_tokenize" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.texttiling">
<span id="texttiling-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">texttiling</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.texttiling" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TextTilingTokenizer</tt><big>(</big><em>w=20, k=10, similarity_method=0, stopwords=None, smoothing_method=[0], smoothing_width=2, smoothing_rounds=1, cutoff_policy=1, demo_mode=False</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>Tokenize a document into topical sections using the TextTiling algorithm.
This algorithm detects subtopic shifts based on the analysis of lexical
co-occurrence patterns.</p>
<p>The process starts by tokenizing the text into pseudosentences of
a fixed size w. Then, depending on the method used, similarity
scores are assigned at sentence gaps. The algorithm proceeds by
detecting the peak differences between these scores and marking
them as boundaries. The boundaries are normalized to the closest
paragraph break and the segmented text is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> (<em>int</em>) &#8211; Pseudosentence size</li>
<li><strong>k</strong> (<em>int</em>) &#8211; Size (in sentences) of the block used in the block comparison method</li>
<li><strong>similarity_method</strong> (<em>constant</em>) &#8211; The method used for determining similarity scores:
<cite>BLOCK_COMPARISON</cite> (default) or <cite>VOCABULARY_INTRODUCTION</cite>.</li>
<li><strong>stopwords</strong> (<em>list(str)</em>) &#8211; A list of stopwords that are filtered out (defaults to NLTK&#8217;s stopwords corpus)</li>
<li><strong>smoothing_method</strong> (<em>constant</em>) &#8211; The method used for smoothing the score plot:
<cite>DEFAULT_SMOOTHING</cite> (default)</li>
<li><strong>smoothing_width</strong> (<em>int</em>) &#8211; The width of the window used by the smoothing method</li>
<li><strong>smoothing_rounds</strong> (<em>int</em>) &#8211; The number of smoothing passes</li>
<li><strong>cutoff_policy</strong> (<em>constant</em>) &#8211; The policy used to determine the number of boundaries:
<cite>HC</cite> (default) or <cite>LC</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="nltk.tokenize.texttiling.TextTilingTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TextTilingTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TextTilingTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>, where each &#8220;token&#8221; represents
a separate topic.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.texttiling.TokenSequence">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TokenSequence</tt><big>(</big><em>index</em>, <em>wrdindex_list</em>, <em>original_length=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenSequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A token list with its original length and its index</p>
</dd></dl>

<dl class="class">
<dt id="nltk.tokenize.texttiling.TokenTableField">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">TokenTableField</tt><big>(</big><em>first_pos</em>, <em>ts_occurences</em>, <em>total_count=1</em>, <em>par_count=1</em>, <em>last_par=0</em>, <em>last_tok_seq=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#TokenTableField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.TokenTableField" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A field in the token table holding parameters for each token,
used later in the process</p>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.texttiling.demo">
<tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">demo</tt><big>(</big><em>text=None</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#demo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="nltk.tokenize.texttiling.smooth">
<tt class="descclassname">nltk.tokenize.texttiling.</tt><tt class="descname">smooth</tt><big>(</big><em>x</em>, <em>window_len=11</em>, <em>window='flat'</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/texttiling.html#smooth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.texttiling.smooth" title="Permalink to this definition">¶</a></dt>
<dd><p>smooth the data using a window with requested size.</p>
<p>This method is based on the convolution of a scaled window with the signal.
The signal is prepared by introducing reflected copies of the signal 
(with the window size) in both ends so that transient parts are minimized
in the beginning and end part of the output signal.</p>
<dl class="docutils">
<dt>input:</dt>
<dd><p class="first">x: the input signal 
window_len: the dimension of the smoothing window; should be an odd integer
window: the type of window from &#8216;flat&#8217;, &#8216;hanning&#8217;, &#8216;hamming&#8217;, &#8216;bartlett&#8217;, &#8216;blackman&#8217;</p>
<blockquote class="last">
<div>flat window will produce a moving average smoothing.</div></blockquote>
</dd>
<dt>output:</dt>
<dd>the smoothed signal</dd>
</dl>
<p>example:</p>
<p>t=linspace(-2,2,0.1)
x=sin(t)+randn(len(t))*0.1
y=smooth(x)</p>
<p>see also:</p>
<p>numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
scipy.signal.lfilter</p>
<p>TODO: the window parameter could be the window itself if an array instead of a string</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.treebank">
<span id="treebank-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">treebank</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.treebank" title="Permalink to this headline">¶</a></h2>
<dl class="docutils">
<dt>A tokenizer that uses the Penn Treebank conventions:</dt>
<dd><ul class="first last simple">
<li>split standard contractions, e.g. <tt class="docutils literal"><span class="pre">don't</span> <span class="pre">-&gt;</span> <span class="pre">``do</span> <span class="pre">n't</span></tt></li>
<li>treat most punctuation characters as separate tokens</li>
<li>split off commas and single quotes, when followed by whitespace</li>
<li>separate periods that appear at the end of line</li>
</ul>
</dd>
</dl>
<dl class="class">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer">
<em class="property">class </em><tt class="descclassname">nltk.tokenize.treebank.</tt><tt class="descname">TreebankWordTokenizer</tt><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../web/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI" title="nltk.tokenize.api.TokenizerI"><tt class="xref py py-class docutils literal"><span class="pre">nltk.tokenize.api.TokenizerI</span></tt></a></p>
<p>A word tokenizer that tokenizes sentences using the conventions
used by the Penn Treebank.  Contractions are split in to two tokens, e.g.:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">can't</span> <span class="pre">-&gt;</span> <span class="pre">ca</span> <span class="pre">n't</span></tt></li>
<li><tt class="docutils literal"><span class="pre">he'll</span> <span class="pre">-&gt;</span> <span class="pre">he</span> <span class="pre">'ll</span></tt></li>
<li><tt class="docutils literal"><span class="pre">weren't</span> <span class="pre">-&gt;</span> <span class="pre">were</span> <span class="pre">n't</span></tt></li>
</ul>
</div></blockquote>
<p>NB. this tokenizer assumes that the text is presented as one sentence per line,
where each line is delimited with a newline character.
The only periods to be treated as separate tokens are those appearing
at the end of a line.</p>
<dl class="method">
<dt id="nltk.tokenize.treebank.TreebankWordTokenizer.tokenize">
<tt class="descname">tokenize</tt><big>(</big><em>text</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer.tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.treebank.TreebankWordTokenizer.tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tokenized copy of <em>text</em>, using the tokenization
conventions of the Penn Treebank.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.tokenize.util">
<span id="util-module"></span><h2><tt class="xref py py-mod docutils literal"><span class="pre">util</span></tt> Module<a class="headerlink" href="#module-nltk.tokenize.util" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nltk.tokenize.util.regexp_span_tokenize">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">regexp_span_tokenize</tt><big>(</big><em>s</em>, <em>regexp</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#regexp_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.regexp_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="docutils literal"><span class="pre">(start,</span> <span class="pre">end)</span></tt> tuples,
by splitting the string at each successive match of <em>regexp</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>s</strong> (<em>str</em>) &#8211; the string to be tokenized</li>
<li><strong>regexp</strong> (<em>str</em>) &#8211; regular expression that matches token separators</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">iter(tuple(int, int))</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.util.spans_to_relative">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">spans_to_relative</tt><big>(</big><em>spans</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#spans_to_relative"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.spans_to_relative" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a sequence of relative spans, given a sequence of spans.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spans</strong> (<em>iter(tuple(int, int))</em>) &#8211; a sequence of (start, end) offsets of the tokens</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">iter(tuple(int, int))</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="nltk.tokenize.util.string_span_tokenize">
<tt class="descclassname">nltk.tokenize.util.</tt><tt class="descname">string_span_tokenize</tt><big>(</big><em>s</em>, <em>sep</em><big>)</big><a class="reference internal" href="../_modules/nltk/tokenize/util.html#string_span_tokenize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.tokenize.util.string_span_tokenize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the offsets of the tokens in <em>s</em>, as a sequence of <tt class="docutils literal"><span class="pre">(start,</span> <span class="pre">end)</span></tt> tuples,
by splitting the string at each occurrence of <em>sep</em>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>s</strong> (<em>str</em>) &#8211; the string to be tokenized</li>
<li><strong>sep</strong> (<em>str</em>) &#8211; the token separator</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">iter(tuple(int, int))</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">tokenize Package</a><ul>
<li><a class="reference internal" href="#id1"><tt class="docutils literal"><span class="pre">tokenize</span></tt> Package</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.api"><tt class="docutils literal"><span class="pre">api</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.punkt"><tt class="docutils literal"><span class="pre">punkt</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.regexp"><tt class="docutils literal"><span class="pre">regexp</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.sexpr"><tt class="docutils literal"><span class="pre">sexpr</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.simple"><tt class="docutils literal"><span class="pre">simple</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.texttiling"><tt class="docutils literal"><span class="pre">texttiling</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.treebank"><tt class="docutils literal"><span class="pre">treebank</span></tt> Module</a></li>
<li><a class="reference internal" href="#module-nltk.tokenize.util"><tt class="docutils literal"><span class="pre">util</span></tt> Module</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/api/nltk.tokenize.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../index.html">NLTK 2.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Steven Bird.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2.
    </div>
  </body>
</html>