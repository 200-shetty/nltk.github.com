<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>Demonstrate word embedding using Gensim</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="demonstrate-word-embedding-using-gensim">
<h1 class="title">Demonstrate word embedding using Gensim</h1>

<!-- Copyright (C) 2001-2015 NLTK Project -->
<!-- For license information, see LICENSE.TXT -->
<p>We demonstrate three functions:
- Train the word embeddings using brown corpus;
- Load the pre-trained model and perform simple tasks; and
- Pruning the pre-trained binary model.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; import gensim
</pre>
</blockquote>
<div class="section" id="train-the-model">
<h1>Train the model</h1>
<p>Here we train a word embedding using the Brown Corpus:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; model = gensim.models.Word2Vec(brown.sents())
</pre>
</blockquote>
<p>It might take some time to train the model. So, after it is trained, it can be saved as follows:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; model.save('brown.embedding')
&gt;&gt;&gt; new_model = gensim.models.Word2Vec.load('brown.embedding')
</pre>
</blockquote>
<dl class="docutils">
<dt>The model will be the list of words with their embedding. We can easily get the vector representation of a word.</dt>
<dd><pre class="first last doctest-block">
&gt;&gt;&gt; len(new_model['university'])
100
</pre>
</dd>
</dl>
<p>There are some supporting functions already implemented in Gensim to manipulate with word embeddings.
For example, to compute the cosine similarity between 2 words:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; new_model.similarity('university','school') &gt; 0.3
True
</pre>
</blockquote>
</div>
<div class="section" id="using-the-pre-trained-model">
<h1>Using the pre-trained model</h1>
<p>NLTK includes a pre-trained model which is part of a model that is trained on 100 billion words from the Google News Dataset.
The full model is from <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a> (about 3 GB).</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.data import find
&gt;&gt;&gt; word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))
&gt;&gt;&gt; model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)
</pre>
</blockquote>
<p>We pruned the model to only include the most common words (~44k words).</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; len(model.vocab)
43981
</pre>
</blockquote>
<p>Each word is represented in the space of 300 dimensions:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; len(model['university'])
300
</pre>
</blockquote>
<p>Finding the top n words that are similar to a target word is simple. The result is the list of n words with the score.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; model.most_similar(positive=['university'], topn = 3)
[(u'universities', 0.7003918886184692), (u'faculty', 0.6780908703804016), (u'undergraduate', 0.6587098240852356)]
</pre>
</blockquote>
<p>Finding a word that is not in a list is also supported, although, implementing this by yourself is simple.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; model.doesnt_match('breakfast cereal dinner lunch'.split())
'cereal'
</pre>
</blockquote>
<p>Mikolov et al. (2013) figured out that word embedding captures much of syntactic and semantic regularities. For example,
the vector 'King - Man + Woman' is close to 'Queen' and 'Germany - Berlin + Paris' is close to 'France'.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)
[(u'queen', 0.7118192911148071)]
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)
[(u'France', 0.7884092926979065)]
</pre>
</blockquote>
<p>We can visualize the word embeddings using t-SNE (<a class="reference external" href="http://lvdmaaten.github.io/tsne/">http://lvdmaaten.github.io/tsne/</a>). For this demonstration, we visualize the first 1000 words.</p>
<div class="line-block">
<div class="line">import numpy as np</div>
<div class="line">labels = []</div>
<div class="line">count = 0</div>
<div class="line">max_count = 1000</div>
<div class="line">X = np.zeros(shape=(max_count,len(model['university'])))</div>
<div class="line"><br /></div>
<div class="line">for term in model.vocab:</div>
<div class="line-block">
<div class="line">X[count] = model[term]</div>
<div class="line">labels.append(term)</div>
<div class="line">count+= 1</div>
<div class="line">if count &gt;= max_count: break</div>
<div class="line"><br /></div>
</div>
<div class="line"># It is recommended to use PCA first to reduce to ~50 dimensions</div>
<div class="line">from sklearn.decomposition import PCA</div>
<div class="line">pca = PCA(n_components=50)</div>
<div class="line">X_50 = pca.fit_transform(X)</div>
<div class="line"><br /></div>
<div class="line"># Using TSNE to further reduce to 2 dimensions</div>
<div class="line">from sklearn.manifold import TSNE</div>
<div class="line">model_tsne = TSNE(n_components=2, random_state=0)</div>
<div class="line">Y = model_tsne.fit_transform(X_50)</div>
<div class="line"><br /></div>
<div class="line"># Show the scatter plot</div>
<div class="line">import matplotlib.pyplot as plt</div>
<div class="line">plt.scatter(Y[:,0], Y[:,1], 20)</div>
<div class="line"><br /></div>
<div class="line"># Add labels</div>
<div class="line">for label, x, y in zip(labels, Y[:, 0], Y[:, 1]):</div>
<div class="line-block">
<div class="line">plt.annotate(label, xy = (x,y), xytext = (0, 0), textcoords = 'offset points', size = 10)</div>
<div class="line"><br /></div>
</div>
<div class="line">plt.show()</div>
</div>
</div>
<div class="section" id="prune-the-trained-binary-model">
<h1>Prune the trained binary model</h1>
<p>Here is the supporting code to extract part of the binary model (GoogleNews-vectors-negative300.bin.gz) from <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a>
We use this code to get the <cite>word2vec_sample</cite> model.</p>
<div class="line-block">
<div class="line">import gensim</div>
<div class="line">from gensim.models.word2vec import Word2Vec</div>
<div class="line"># Load the binary model</div>
<div class="line">model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True);</div>
<div class="line"><br /></div>
<div class="line"># Only output word that appear in the Brown corpus</div>
<div class="line">from nltk.corpus import brown</div>
<div class="line">words = set(brown.words())</div>
<div class="line">print (len(words))</div>
<div class="line"><br /></div>
<div class="line"># Output presented word to a temporary file</div>
<div class="line">out_file = 'pruned.word2vec.txt'</div>
<div class="line">f = open(out_file,'wb')</div>
<div class="line"><br /></div>
<div class="line">word_presented = words.intersection(model.vocab.keys())</div>
<div class="line">f.write('{} {}n'.format(len(word_presented),len(model['word'])))</div>
<div class="line"><br /></div>
<div class="line">for word in word_presented:</div>
<div class="line-block">
<div class="line">f.write('{} {}n'.format(word, ' '.join(str(value) for value in model[word])))</div>
<div class="line"><br /></div>
</div>
<div class="line">f.close()</div>
</div>
</div>
</div>
</body>
</html>
