<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>Loading Resources From the Data Package</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="loading-resources-from-the-data-package">
<h1 class="title">Loading Resources From the Data Package</h1>

<!-- Copyright (C) 2001-2015 NLTK Project -->
<!-- For license information, see LICENSE.TXT -->
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; import nltk.data
</pre>
</blockquote>
<div class="section" id="overview">
<h1>Overview</h1>
<p>The <cite>nltk.data</cite> module contains functions that can be used to load
NLTK resource files, such as corpora, grammars, and saved processing
objects.</p>
</div>
<div class="section" id="loading-data-files">
<h1>Loading Data Files</h1>
<p>Resources are loaded using the function <cite>nltk.data.load()</cite>, which
takes as its first argument a URL specifying what file should be
loaded.  The <tt class="docutils literal">nltk:</tt> protocol loads files from the NLTK data
distribution:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from __future__ import print_function
&gt;&gt;&gt; tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')
&gt;&gt;&gt; tokenizer.tokenize('Hello.  This is a test.  It works!')
['Hello.', 'This is a test.', 'It works!']
</pre>
</blockquote>
<p>It is important to note that there should be no space following the
colon (':') in the URL; 'nltk: tokenizers/punkt/english.pickle' will
not work!</p>
<p>The <tt class="docutils literal">nltk:</tt> protocol is used by default if no protocol is specified:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.load('tokenizers/punkt/english.pickle') # doctest: +ELLIPSIS
&lt;nltk.tokenize.punkt.PunktSentenceTokenizer object at ...&gt;
</pre>
</blockquote>
<p>But it is also possible to load resources from <tt class="docutils literal">http:</tt>, <tt class="docutils literal">ftp:</tt>,
and <tt class="docutils literal">file:</tt> URLs, e.g. <tt class="docutils literal">cfg = <span class="pre">nltk.data.load('http://example.com/path/to/toy.cfg')</span></tt></p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; # Load a grammar using an absolute path.
&gt;&gt;&gt; url = 'file:%s' % nltk.data.find('grammars/sample_grammars/toy.cfg')
&gt;&gt;&gt; url.replace('\\', '/') # doctest: +ELLIPSIS
'file:...toy.cfg'
&gt;&gt;&gt; print(nltk.data.load(url)) # doctest: +ELLIPSIS
Grammar with 14 productions (start state = S)
    S -&gt; NP VP
    PP -&gt; P NP
    ...
    P -&gt; 'on'
    P -&gt; 'in'
</pre>
</blockquote>
<p>The second argument to the <cite>nltk.data.load()</cite> function specifies the
file format, which determines how the file's contents are processed
before they are returned by <tt class="docutils literal">load()</tt>.  The formats that are
currently supported by the data module are described by the dictionary
<cite>nltk.data.FORMATS</cite>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; for format, descr in sorted(nltk.data.FORMATS.items()):
...     print('{0:&lt;7} {1:}'.format(format, descr)) # doctest: +NORMALIZE_WHITESPACE
cfg     A context free grammar.
fcfg    A feature CFG.
fol     A list of first order logic expressions, parsed with
nltk.sem.logic.Expression.fromstring.
json    A serialized python object, stored using the json module.
logic   A list of first order logic expressions, parsed with
nltk.sem.logic.LogicParser.  Requires an additional logic_parser
parameter
pcfg    A probabilistic CFG.
pickle  A serialized python object, stored using the pickle
module.
raw     The raw (byte string) contents of a file.
text    The raw (unicode string) contents of a file.
val     A semantic valuation, parsed by
nltk.sem.Valuation.fromstring.
yaml    A serialized python object, stored using the yaml module.
</pre>
</blockquote>
<p><cite>nltk.data.load()</cite> will raise a ValueError if a bad format name is
specified:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.load('grammars/sample_grammars/toy.cfg', 'bar')
Traceback (most recent call last):
  . . .
ValueError: Unknown format type!
</pre>
</blockquote>
<p>By default, the <tt class="docutils literal">&quot;auto&quot;</tt> format is used, which chooses a format
based on the filename's extension.  The mapping from file extensions
to format names is specified by <cite>nltk.data.AUTO_FORMATS</cite>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; for ext, format in sorted(nltk.data.AUTO_FORMATS.items()):
...     print('.%-7s -&gt; %s' % (ext, format))
.cfg     -&gt; cfg
.fcfg    -&gt; fcfg
.fol     -&gt; fol
.json    -&gt; json
.logic   -&gt; logic
.pcfg    -&gt; pcfg
.pickle  -&gt; pickle
.text    -&gt; text
.txt     -&gt; text
.val     -&gt; val
.yaml    -&gt; yaml
</pre>
</blockquote>
<p>If <cite>nltk.data.load()</cite> is unable to determine the format based on the
filename's extension, it will raise a ValueError:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.load('foo.bar')
Traceback (most recent call last):
  . . .
ValueError: Could not determine format for foo.bar based on its file
extension; use the &quot;format&quot; argument to specify the format explicitly.
</pre>
</blockquote>
<p>Note that by explicitly specifying the <tt class="docutils literal">format</tt> argument, you can
override the load method's default processing behavior.  For example,
to get the raw contents of any file, simply use <tt class="docutils literal"><span class="pre">format=&quot;raw&quot;</span></tt>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; s = nltk.data.load('grammars/sample_grammars/toy.cfg', 'text')
&gt;&gt;&gt; print(s) # doctest: +ELLIPSIS
S -&gt; NP VP
PP -&gt; P NP
NP -&gt; Det N | NP PP
VP -&gt; V NP | VP PP
...
</pre>
</blockquote>
</div>
<div class="section" id="making-local-copies">
<h1>Making Local Copies</h1>
<!-- This will not be visible in the html output: create a tempdir to
play in.
>>> import tempfile, os
>>> tempdir = tempfile.mkdtemp()
>>> old_dir = os.path.abspath('.')
>>> os.chdir(tempdir) -->
<p>The function <cite>nltk.data.retrieve()</cite> copies a given resource to a local
file.  This can be useful, for example, if you want to edit one of the
sample grammars.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.retrieve('grammars/sample_grammars/toy.cfg')
Retrieving 'nltk:grammars/sample_grammars/toy.cfg', saving to 'toy.cfg'
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # Simulate editing the grammar.
&gt;&gt;&gt; with open('toy.cfg') as inp:
...     s = inp.read().replace('NP', 'DP')
&gt;&gt;&gt; with open('toy.cfg', 'w') as out:
...     _bytes_written = out.write(s)
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # Load the edited grammar, &amp; display it.
&gt;&gt;&gt; cfg = nltk.data.load('file:///' + os.path.abspath('toy.cfg'))
&gt;&gt;&gt; print(cfg) # doctest: +ELLIPSIS
Grammar with 14 productions (start state = S)
    S -&gt; DP VP
    PP -&gt; P DP
    ...
    P -&gt; 'on'
    P -&gt; 'in'
</pre>
</blockquote>
<p>The second argument to <cite>nltk.data.retrieve()</cite> specifies the filename
for the new copy of the file.  By default, the source file's filename
is used.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.retrieve('grammars/sample_grammars/toy.cfg', 'mytoy.cfg')
Retrieving 'nltk:grammars/sample_grammars/toy.cfg', saving to 'mytoy.cfg'
&gt;&gt;&gt; os.path.isfile('./mytoy.cfg')
True
&gt;&gt;&gt; nltk.data.retrieve('grammars/sample_grammars/np.fcfg')
Retrieving 'nltk:grammars/sample_grammars/np.fcfg', saving to 'np.fcfg'
&gt;&gt;&gt; os.path.isfile('./np.fcfg')
True
</pre>
</blockquote>
<p>If a file with the specified (or default) filename already exists in
the current directory, then <cite>nltk.data.retrieve()</cite> will raise a
ValueError exception.  It will <em>not</em> overwrite the file:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; os.path.isfile('./toy.cfg')
True
&gt;&gt;&gt; nltk.data.retrieve('grammars/sample_grammars/toy.cfg') # doctest: +ELLIPSIS
Traceback (most recent call last):
  . . .
ValueError: File '...toy.cfg' already exists!
</pre>
</blockquote>
<!-- This will not be visible in the html output: clean up the tempdir.
>>> os.chdir(old_dir)
>>> for f in os.listdir(tempdir):
...     os.remove(os.path.join(tempdir, f))
>>> os.rmdir(tempdir) -->
</div>
<div class="section" id="finding-files-in-the-nltk-data-package">
<h1>Finding Files in the NLTK Data Package</h1>
<p>The <cite>nltk.data.find()</cite> function searches the NLTK data package for a
given file, and returns a pointer to that file.  This pointer can
either be a <cite>FileSystemPathPointer</cite> (whose <cite>path</cite> attribute gives the
absolute path of the file); or a <cite>ZipFilePathPointer</cite>, specifying a
zipfile and the name of an entry within that zipfile.  Both pointer
types define the <cite>open()</cite> method, which can be used to read the string
contents of the file.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; path = nltk.data.find('corpora/abc/rural.txt')
&gt;&gt;&gt; str(path) # doctest: +ELLIPSIS
'...rural.txt'
&gt;&gt;&gt; print(path.open().read(60).decode())
PM denies knowledge of AWB kickbacks
The Prime Minister has
</pre>
</blockquote>
<p>Alternatively, the <cite>nltk.data.load()</cite> function can be used with the
keyword argument <tt class="docutils literal"><span class="pre">format=&quot;raw&quot;</span></tt>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; s = nltk.data.load('corpora/abc/rural.txt', format='raw')[:60]
&gt;&gt;&gt; print(s.decode())
PM denies knowledge of AWB kickbacks
The Prime Minister has
</pre>
</blockquote>
<p>Alternatively, you can use the keyword argument <tt class="docutils literal"><span class="pre">format=&quot;text&quot;</span></tt>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; s = nltk.data.load('corpora/abc/rural.txt', format='text')[:60]
&gt;&gt;&gt; print(s)
PM denies knowledge of AWB kickbacks
The Prime Minister has
</pre>
</blockquote>
</div>
<div class="section" id="resource-caching">
<h1>Resource Caching</h1>
<p>NLTK uses a weakref dictionary to maintain a cache of resources that
have been loaded.  If you load a resource that is already stored in
the cache, then the cached copy will be returned.  This behavior can
be seen by the trace output generated when verbose=True:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; feat0 = nltk.data.load('grammars/book_grammars/feat0.fcfg', verbose=True)
&lt;&lt;Loading nltk:grammars/book_grammars/feat0.fcfg&gt;&gt;
&gt;&gt;&gt; feat0 = nltk.data.load('grammars/book_grammars/feat0.fcfg', verbose=True)
&lt;&lt;Using cached copy of nltk:grammars/book_grammars/feat0.fcfg&gt;&gt;
</pre>
</blockquote>
<p>If you wish to load a resource from its source, bypassing the cache,
use the <tt class="docutils literal">cache=False</tt> argument to <cite>nltk.data.load()</cite>.  This can be
useful, for example, if the resource is loaded from a local file, and
you are actively editing that file:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; feat0 = nltk.data.load('grammars/book_grammars/feat0.fcfg',cache=False,verbose=True)
&lt;&lt;Loading nltk:grammars/book_grammars/feat0.fcfg&gt;&gt;
</pre>
</blockquote>
<p>The cache <em>no longer</em> uses weak references.  A resource will not be
automatically expunged from the cache when no more objects are using
it.  In the following example, when we clear the variable <tt class="docutils literal">feat0</tt>,
the reference count for the feature grammar object drops to zero.
However, the object remains cached:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; del feat0
&gt;&gt;&gt; feat0 = nltk.data.load('grammars/book_grammars/feat0.fcfg',
...                        verbose=True)
&lt;&lt;Using cached copy of nltk:grammars/book_grammars/feat0.fcfg&gt;&gt;
</pre>
</blockquote>
<p>You can clear the entire contents of the cache, using
<cite>nltk.data.clear_cache()</cite>:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; nltk.data.clear_cache()
</pre>
</blockquote>
</div>
<div class="section" id="retrieving-other-data-sources">
<h1>Retrieving other Data Sources</h1>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; formulas = nltk.data.load('grammars/book_grammars/background.fol')
&gt;&gt;&gt; for f in formulas: print(str(f))
all x.(boxerdog(x) -&gt; dog(x))
all x.(boxer(x) -&gt; person(x))
all x.-(dog(x) &amp; person(x))
all x.(married(x) &lt;-&gt; exists y.marry(x,y))
all x.(bark(x) -&gt; dog(x))
all x y.(marry(x,y) -&gt; (person(x) &amp; person(y)))
-(Vincent = Mia)
-(Vincent = Fido)
-(Mia = Fido)
</pre>
</blockquote>
</div>
<div class="section" id="regression-tests">
<h1>Regression Tests</h1>
<p>Create a temp dir for tests that write files:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; import tempfile, os
&gt;&gt;&gt; tempdir = tempfile.mkdtemp()
&gt;&gt;&gt; old_dir = os.path.abspath('.')
&gt;&gt;&gt; os.chdir(tempdir)
</pre>
</blockquote>
<p>The <cite>retrieve()</cite> function accepts all url types:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; urls = ['https://raw.githubusercontent.com/nltk/nltk/develop/nltk/test/toy.cfg',
...         'file:%s' % nltk.data.find('grammars/sample_grammars/toy.cfg'),
...         'nltk:grammars/sample_grammars/toy.cfg',
...         'grammars/sample_grammars/toy.cfg']
&gt;&gt;&gt; for i, url in enumerate(urls):
...     nltk.data.retrieve(url, 'toy-%d.cfg' % i) # doctest: +ELLIPSIS
Retrieving 'https://raw.githubusercontent.com/nltk/nltk/develop/nltk/test/toy.cfg', saving to 'toy-0.cfg'
Retrieving 'file:...toy.cfg', saving to 'toy-1.cfg'
Retrieving 'nltk:grammars/sample_grammars/toy.cfg', saving to 'toy-2.cfg'
Retrieving 'nltk:grammars/sample_grammars/toy.cfg', saving to 'toy-3.cfg'
</pre>
</blockquote>
<p>Clean up the temp dir:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; os.chdir(old_dir)
&gt;&gt;&gt; for f in os.listdir(tempdir):
...     os.remove(os.path.join(tempdir, f))
&gt;&gt;&gt; os.rmdir(tempdir)
</pre>
</blockquote>
<div class="section" id="lazy-loader">
<h2>Lazy Loader</h2>
<p>A lazy loader is a wrapper object that defers loading a resource until
it is accessed or used in any way.  This is mainly intended for
internal use by NLTK's corpus readers.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; # Create a lazy loader for toy.cfg.
&gt;&gt;&gt; ll = nltk.data.LazyLoader('grammars/sample_grammars/toy.cfg')
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # Show that it's not loaded yet:
&gt;&gt;&gt; object.__repr__(ll) # doctest: +ELLIPSIS
'&lt;nltk.data.LazyLoader object at ...&gt;'
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # printing it is enough to cause it to be loaded:
&gt;&gt;&gt; print(ll)
&lt;Grammar with 14 productions&gt;
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # Show that it's now been loaded:
&gt;&gt;&gt; object.__repr__(ll) # doctest: +ELLIPSIS
'&lt;nltk.grammar.CFG object at ...&gt;'
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; # Test that accessing an attribute also loads it:
&gt;&gt;&gt; ll = nltk.data.LazyLoader('grammars/sample_grammars/toy.cfg')
&gt;&gt;&gt; ll.start()
S
&gt;&gt;&gt; object.__repr__(ll) # doctest: +ELLIPSIS
'&lt;nltk.grammar.CFG object at ...&gt;'
</pre>
</blockquote>
</div>
<div class="section" id="buffered-gzip-reading-and-writing">
<h2>Buffered Gzip Reading and Writing</h2>
<p>Write performance to gzip-compressed is extremely poor when the files become large.
File creation can become a bottleneck in those cases.</p>
<p>Read performance from large gzipped pickle files was improved in data.py by
buffering the reads. A similar fix can be applied to writes by buffering
the writes to a StringIO object first.</p>
<p>This is mainly intended for internal use. The test simply tests that reading
and writing work as intended and does not test how much improvement buffering
provides.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.compat import StringIO
&gt;&gt;&gt; test = nltk.data.BufferedGzipFile('testbuf.gz', 'wb', size=2**10)
&gt;&gt;&gt; ans = []
&gt;&gt;&gt; for i in range(10000):
...     ans.append(str(i).encode('ascii'))
...     test.write(str(i).encode('ascii'))
&gt;&gt;&gt; test.close()
&gt;&gt;&gt; test = nltk.data.BufferedGzipFile('testbuf.gz', 'rb')
&gt;&gt;&gt; test.read() == b''.join(ans)
True
&gt;&gt;&gt; test.close()
&gt;&gt;&gt; import os
&gt;&gt;&gt; os.unlink('testbuf.gz')
</pre>
</blockquote>
</div>
<div class="section" id="json-encoding-and-decoding">
<h2>JSON Encoding and Decoding</h2>
<p>JSON serialization is used instead of pickle for some classes.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk import jsontags
&gt;&gt;&gt; from nltk.jsontags import JSONTaggedEncoder, JSONTaggedDecoder, register_tag
&gt;&gt;&gt; &#64;jsontags.register_tag
... class JSONSerializable:
...     json_tag = 'JSONSerializable'
...
...     def __init__(self, n):
...         self.n = n
...
...     def encode_json_obj(self):
...         return self.n
...
...     &#64;classmethod
...     def decode_json_obj(cls, obj):
...         n = obj
...         return cls(n)
...
&gt;&gt;&gt; JSONTaggedEncoder().encode(JSONSerializable(1))
'{&quot;!JSONSerializable&quot;: 1}'
&gt;&gt;&gt; JSONTaggedDecoder().decode('{&quot;!JSONSerializable&quot;: 1}').n
1
</pre>
</blockquote>
</div>
</div>
</div>
</body>
</html>
