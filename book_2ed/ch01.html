<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>1. Language Processing and Python</title>
<style type="text/css">

/*
:Author: Edward Loper, James Curran
:Copyright: This stylesheet has been placed in the public domain.

Stylesheet for use with Docutils.

This stylesheet defines new css classes used by NLTK.

It uses a Python syntax highlighting scheme that matches
the colour scheme used by IDLE, which makes it easier for
beginners to check they are typing things in correctly.
*/

/* Include the standard docutils stylesheet. */
@import url(default.css);

/* Custom inline roles */
span.placeholder    { font-style: italic; font-family: monospace; }
span.example        { font-style: italic; }
span.emphasis       { font-style: italic; }
span.termdef        { font-weight: bold; }
/*span.term           { font-style: italic; }*/
span.category       { font-variant: small-caps; }
span.feature        { font-variant: small-caps; }
span.fval           { font-style: italic; }
span.math           { font-style: italic; }
span.mathit         { font-style: italic; }
span.lex            { font-variant: small-caps; }
span.guide-linecount{ text-align: right; display: block;}

/* Python souce code listings */
span.pysrc-prompt   { color: #9b0000; }
span.pysrc-more     { color: #9b00ff; }
span.pysrc-keyword  { color: #e06000; }
span.pysrc-builtin  { color: #940094; }
span.pysrc-string   { color: #00aa00; }
span.pysrc-comment  { color: #ff0000; }
span.pysrc-output   { color: #0000ff; }
span.pysrc-except   { color: #ff0000; }
span.pysrc-defname  { color: #008080; }


/* Doctest blocks */
pre.doctest         { margin: 0; padding: 0; font-weight: bold; }
div.doctest         { margin: 0 1em 1em 1em; padding: 0; }
table.doctest       { margin: 0; padding: 0;
                      border-top: 1px solid gray;
                      border-bottom: 1px solid gray; }
pre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;
                      background-color: #ffffff; }

/* Python source listings */
div.pylisting       { margin: 0 1em 1em 1em; padding: 0; }
table.pylisting     { margin: 0; padding: 0;
                      border-top: 1px solid gray; }
td.caption { border-top: 1px solid black; margin: 0; padding: 0; }
.caption-label { font-weight: bold;  }
td.caption p { margin: 0; padding: 0; font-style: normal;}

table tr td.codeblock { 
  padding: 0.2em ! important; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeffee;
}

table tr td.doctest  { 
  padding: 0.2em; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeeeff;
}

td.codeblock table tr td.copybar {
    background: #40a060; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }
td.doctest table tr td.copybar {
    background: #4060a0; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }

td.pysrc { padding-left: 0.5em; }

img.callout { border-width: 0px; }

table.docutils {
    border-style: solid;
    border-width: 1px;
    margin-top: 6px;
    border-color: grey;
    border-collapse: collapse; }

table.docutils th {
    border-style: none;
    border-width: 1px;
    border-color: grey;
    padding: 0 .5em 0 .5em; }

table.docutils td {
    border-style: none;
    border-width: 1px;
    border-color: grey; 
    padding: 0 .5em 0 .5em; }

table.footnote td { padding: 0; }
table.footnote { border-width: 0; }
table.footnote td { border-width: 0; }
table.footnote th { border-width: 0; }

table.noborder { border-width: 0; }

table.example pre { margin-top: 4px; margin-bottom: 0; }

/* For figures & tables */
p.caption { margin-bottom: 0; }
div.figure { text-align: center; }

/* The index */
div.index { border: 1px solid black;
            background-color: #eeeeee; }
div.index h1 { padding-left: 0.5em; margin-top: 0.5ex;
               border-bottom: 1px solid black; }
ul.index { margin-left: 0.5em; padding-left: 0; }
li.index { list-style-type: none; }
p.index-heading { font-size: 120%; font-style: italic; margin: 0; }
li.index ul { margin-left: 2em; padding-left: 0; }

/* 'Note' callouts */
div.note
{
  border-right:   #87ceeb 1px solid;
  padding-right: 4px;
  border-top: #87ceeb 1px solid;
  padding-left: 4px;
  padding-bottom: 4px;
  margin: 2px 5% 10px;
  border-left: #87ceeb 1px solid;
  padding-top: 4px;
  border-bottom: #87ceeb 1px solid;
  font-style: normal;
  font-family: verdana, arial;
  background-color: #b0c4de;
}

table.avm { border: 0px solid black; width: 0; }
table.avm tbody tr {border: 0px solid black; }
table.avm tbody tr td { padding: 2px; }
table.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }
table.avm tbody tr td.avm-eq { padding: 5px; }
table.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }
p.avm-empty { font-style: normal; }
table.avm colgroup col { border: 0px solid black; }
table.avm tbody tr td.avm-topleft 
    { border-left: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botleft 
    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-topright
    { border-right: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botright
    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-left
    { border-left: 2px solid #000080; }
table.avm tbody tr td.avm-right
    { border-right: 2px solid #000080; }
table.avm tbody tr td.avm-topbotleft
    { border: 2px solid #000080; border-right: 0px solid black; }
table.avm tbody tr td.avm-topbotright
    { border: 2px solid #000080; border-left: 0px solid black; }
table.avm tbody tr td.avm-ident
    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }
.avm-pointer
{ border: 1px solid #008000; padding: 1px; color: #008000; 
  background: #c0ffc0; font-style: normal; }

table.gloss { border: 0px solid black; width: 0; }
table.gloss tbody tr { border: 0px solid black; }
table.gloss tbody tr td { border: 0px solid black; }
table.gloss colgroup col { border: 0px solid black; }
table.gloss p { margin: 0; padding: 0; }

table.rst-example { border: 1px solid black; }
table.rst-example tbody tr td { background: #eeeeee; }
table.rst-example thead tr th { background: #c0ffff; }
td.rst-raw { width: 0; }

/* Used by nltk.org/doc/test: */
div.doctest-list { text-align: center; }
table.doctest-list { border: 1px solid black;
  margin-left: auto; margin-right: auto;
}
table.doctest-list tbody tr td { background: #eeeeee;
  border: 1px solid #cccccc; text-align: left; }
table.doctest-list thead tr th { background: #304050; color: #ffffff;
  border: 1px solid #000000;}
table.doctest-list thead tr a { color: #ffffff; }
span.doctest-passed { color: #008000; }
span.doctest-failed { color: #800000; }

</style>
</head>
<body>
<div class="document" id="language-processing-and-python">
<span id="chap-introduction"></span>
<h1 class="title">1. Language Processing and Python</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- TODO: update cspy reference to more recent book -->
<!-- TODO: add some literature references (esp to other intro linguistics textbooks) -->
<!-- TODO: adopt simpler hacker example with only single character transpositions;
move hacker example to later section (later chapter?) -->
<!-- TODO: get URL hyperlinks to be fixed width -->
<!-- TODO: websites with automatically generated language - - lobner prize... -->
<div class="section" id="natural-language-processing-systems">
<h1>1&nbsp;&nbsp;&nbsp;Natural Language Processing Systems</h1>
<div class="section" id="translation">
<h2>1.1&nbsp;&nbsp;&nbsp;Translation</h2>
<p>For decades, automatic translation between languages has been held up as the ultimate challenge for NLP.
If a system can translate arbitrary expressions and concepts between languages, it surely must have gained human-level understanding of language.
The roots of &quot;mechanical translation&quot;, now known as <a name="machine_translation_index_term" /><span class="termdef">machine translation</span> (MT), go back to the early days of the Cold War, when governments were keen to exploit computational methods for sourcing intelligence in other languages.
For the past two decades, there has been a resurgence of interest in this area, in the sub-field known as <a name="statistical_machine_translation_index_term" /><span class="termdef">statistical machine translation</span> (SMT).</p>
<p>Today, machine translation services such as &quot;Google Translate&quot; support about 100 languages, and translation is integrated into Web search engines.
However, these systems have some serious shortcomings.
Consider the following example, where the input phrase is translated into Japanese, then back to English, and so forth, until equilibrium has been reached:</p>
<div class="line-block">
<div class="line">how long before the next flight to Alice Springs</div>
<div class="line">&#12393;&#12398;&#12367;&#12425;&#12356;&#229;&#12395;&#12450;&#12522;&#12473; &#12539; &#12473;&#12503;&#12522;&#12531;&#12464;&#12473;&#12395;&#230;&#12398;&#228;</div>
<div class="line">How long before Alice Springs for the next flight</div>
<div class="line">&#230;&#12398;&#12501;&#12521;&#12452;&#12488;&#12398;&#12450;&#12522;&#12473; &#12539; &#12473;&#12503;&#12522;&#12531;&#12464;&#12473;&#12398;&#12393;&#12398;&#12367;&#12425;&#12356;&#229;&#12395;</div>
<div class="line">How much of Alice Springs on the next flight before</div>
<div class="line">&#12450;&#12522;&#12473;&#12398;&#12393;&#12398;&#12367;&#12425;&#12356;&#229;&#12395;&#230;&#12398;&#228;&#12395;&#230;&#230;&#12375;&#12414;&#12377;&#12290;</div>
<div class="line">Ago Alice how much hot springs on the next flight.</div>
<div class="line">&#229;&#230;&#12398;&#228;&#12395;&#12393;&#12398;&#12367;&#12425;&#12356;&#231;&#12356;&#12450;&#12522;&#12473;&#12473;&#12503;&#12522;&#12531;&#12464;&#12473;&#12375;&#12414;&#12377;&#12290;</div>
<div class="line">Before the next flight how hot Alice Springs the.</div>
<div class="line">&#230;&#12399;&#12393;&#12398;&#12424;&#12358;&#12395;&#12507;&#12483;&#12488; &#12450;&#12522;&#12473; &#12539; &#12473;&#12503;&#12522;&#12531;&#12464;&#12473;&#12434;&#233;&#232;&#12377;&#12427;&#229;&#12395;&#12290;</div>
<div class="line">Before the next how to fly hot Alice Springs.</div>
<div class="line">&#230;&#12398;&#229;&#12395;&#12393;&#12398;&#12424;&#12358;&#12395;&#12507;&#12483;&#12488; &#12450;&#12522;&#12473; &#12539; &#12473;&#12503;&#12522;&#12531;&#12464;&#12473;&#12434;&#233;&#12406;&#12290;</div>
<div class="line">Ago the next how to fly hot Alice Springs.</div>
<div class="line">&#21069;&#27425;&#28201;&#27849;&#12450;&#12522;&#12473;&#12398;&#25805;&#32294;&#26041;&#27861;&#12290;</div>
<div class="line">How to fly before the next hot Alice.</div>
<div class="line">&#27425;&#12398;&#12507;&#12483;&#12488; &#12450;&#12522;&#12473;&#12398;&#21069;&#12395;&#39131;&#12406;&#26041;&#27861;&#12290;</div>
<div class="line">How to fly in front of the next hot Alice.</div>
<div class="line">&#27425;&#12398;&#12507;&#12483;&#12488; &#12450;&#12522;&#12473;&#12398;&#21069;&#12395;&#39131;&#12406;&#26041;&#27861;&#12290;</div>
<div class="line">How to fly in front of the next hot Alice.</div>
</div>
<p>Observe that <span class="example">springs</span> becomes <span class="example">hot springs</span> and eventually <span class="example">springs</span> is lost entirely;
<span class="example">long before</span> becomes <span class="example">ago</span>, then <span class="example">before</span> and finally <span class="example">in front of</span>.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong> Try this yourself using <tt class="doctest"><span class="pre">http://translationparty.com/</span></tt></p>
</div>
<p>Machine translation is difficult because a given word could have several possible translations (depending on its meaning), and because word order must be changed in keeping with the grammatical structure of the target language.
These difficulties are being addressed with the help of massive quantities of parallel texts, sourced from news and government websites that publish documents in two or more languages.
Given a pair of such documents,
we can automatically find corresponding sentences using two heuristics:
(a) sentences tend to be translated in order even if they are sometimes split or merged in the process;
and (b) short sentences tend to be translated into short sentences, and long sentences tend to be translated into long sentences.
Once we have found corresponding sentences, we can identify corresponding words using the following heuristic:
find all sentences containing a particular word in the source language, then look for the most frequent word in the translation of those sentences.
Once we have a million or more sentence pairs, we can detect corresponding words and phrases, and build a model that can be used for translating new text.</p>
<p>There are two main components in an <a href="#id9"><span class="problematic" id="id10">|SMT|</span></a> system, namely a translation model and a language model.
The translation model is responsible for representing the likely translations for each word in context, and can be thought of as a mapping from source language expressions to target language expressions.
Source expressions can include idioms, e.g. French <span class="example">la goutte d'eau qui fait d&#233;border le vase</span>, literally the drop of water that made the vase overflow, becomes <span class="example">the straw the broke the camel's back</span> in English).
The language model is responsible for representing the likely words and expressions in the target language, and can be thought of as a mapping from target language expressions to a list of the most likely following words.
For example, after <span class="example">I absolutely</span> we would expect words such as <span class="example">adore</span> and <span class="example">detest</span>, but not <span class="example">like</span> or <span class="example">dislike</span>.</p>
<div class="line-block">
<div class="line">ARCHITECTURE DIAGRAM:</div>
<div class="line">Source text -&gt; translation model p(f|e) -&gt; language model p(e) -&gt; target text</div>
</div>
<p>The translation model and language model act together to produce a translation that is both faithful to the source language and coherent in the target language.
Proper names need to be handled specially, usually by some combination of lookup (e.g. German <span class="example">M&#252;nchen</span> becomes <span class="example">Munich</span> in English) or transliteration (e.g. Arabic <span class="example">&#1575;&#1604;&#1602;&#1584;&#1575;&#1601;&#1610;</span> becomes <span class="example">Gaddafi</span> or <span class="example">Qadhafi</span> in English).
Another challenge is posed by languages with complex morphology, including inflectional affixes (EXAMPLE).</p>
<p>sentence segmentation + alignment
tokenization + alignment (tokens not split by space)</p>
<ul class="simple">
<li>dialogue</li>
<li>summarisation</li>
<li>web search</li>
<li>recommendations (similarity metrics over document collections)</li>
<li>pointers to demonstrations online (links hosted at nltk.org to avoid link rot)</li>
<li>discussion to highlight the non-trivial NLP involved</li>
</ul>
</div>
<div class="section" id="question-answering">
<h2>1.2&nbsp;&nbsp;&nbsp;Question Answering</h2>
<p>Humans like asking questions. But we don't always get good answers!
One of the challengs of Question Answering as a research topic in
NLP is figuring out what knowledge can be drawn on to satisfy our curiosity.</p>
<p>An early strand of work on this topic was arguably driven by a
slightly different perception: lots of useful information is stored as
structured data models, but it's hard for the ordinary user to get at that
information. For example, querying a relational database requires that you know how
to formulate your questions in a special language such as SQL
[ref]. Wouldn't it be convenient if we could make those queries by
asking natural language questions instead! Work on natural language
interfaces to databases has a long history, dating back to at least
the 1970s, and flourished particularly in the following decade.</p>
</div>
<div class="section" id="sentiment-analysis">
<h2>1.3&nbsp;&nbsp;&nbsp;Sentiment Analysis</h2>
<p>When we talk about understanding natural language, we often focus on
'who did what to whom'. Yet in many situations, we are more interested
in attitudes and opinions. When someone writes about a movie, did they
like it or hate it? Is a product review for a water bottle on Amazon
positive or negative? Is this Tweet about the US President supportive
or critical? We might also care about the intensity of the views
expressed: <span class="example">this is a fine movie</span> is different from <span class="example">WOW!
This movie is soooooo great!!!!</span> even though both are
positive.</p>
<p><a name="sentiment_analysis_index_term" /><span class="termdef">Sentiment analysis</span> (or <a name="opinion_mining_index_term" /><span class="termdef">opinion mining</span>) is a broad term for
a range of techniques that try to identify the subjective views
expressed in texts. Many organisations care deeply about public
opinion &#8212; whether these concern commercial products, creative
works, or political parties and policies &#8212; and have consequently
turned to sentiment analysis as a way of gleaning valuable insights
from voluminous bodies of online text. This in turn has stimulated
much activity in the area, ranging from academic research to
commercial applications and industry-focussed conferences.</p>
<p>We'll look at sentiment analysis in more detail later in the book [NB
XREF]. For the time being, let's say that our task is to classify a
sentence into one of three categories: positive, negative or
neutral. Each of these can be illustrated by posts on Twitter collected during
the UK General Election in 2015.</p>
<span class="target" id="ex-twitter-sa1"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>[positive] Good stuff from Clegg. Clear, passionate &amp; honest about the difficulties of govt but also the difference &#64;LibDems have made.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>[negative] Hmm. Ed Miliband being against SNP is a bad move I think. It'll cost him n it is a dumb choice.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>[neutral] Why is Ed Milliband trending when him name is Ed Miliband?</td></tr></table></p>
</td></tr></table></p>
<p>The easiest approach to classifying examples like these is to get hold
of two lists of words, positive ones such as <span class="example">good</span>,
<span class="example">excellent</span>, <span class="example">fine</span>, <span class="example">triumph</span>, <span class="example">well</span>, <span class="example">succeed</span>
&#8230; and negative ones such as <span class="example">bad</span>, <span class="example">poor</span>, <span class="example">dismal</span>,
<span class="example">lying</span>, <span class="example">fail</span>, <span class="example">disaster</span> &#8230;. We then base our
polarity assignment on the ratio of positive tokens to negative ones
in a given string. A string with neither positive or negative tokens
(or possibly an equal number of each) will be categorised as
neutral. This simple approach is likely to yield the intuitively
correct results for <a href="#id11"><span class="problematic" id="id12">ex-twitter-sa1_</span></a>.</p>
<p>Things become more complicated when negation enter into the
picture. <a href="#id13"><span class="problematic" id="id14">ex-twitter-sa1_</span></a> is mildly positive (at least in British
English), so we need to ensure that <span class="example">not</span> flips the
polarity of <span class="example">bad</span> in appropriate contexts,</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">ch01.rst2</tt>, line 166); <em><a href="#id1">backlink</a></em></p>
Duplicate explicit target name: &quot;ex-twitter-sa1&quot;.</div>
<span class="target" id="id1"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td>Given Miliband personal ratings still 20 points behind Cameron, I'd say that not a bad margin for Labour leader  <a class="reference external" href="https://t.co/ILQP93VYLF">https://t.co/ILQP93VYLF</a></td></tr></table></p>
<p>Relatively 'shallow' techniques can deal fairly effectively
with the way in which the prior polarity of a word is modified by the
contextual effects of negation and other semantic operators.
Nevertheless it's not hard to find examples where something close to
full natural language understanding is required to determine the
correct polarity.</p>
<span class="target" id="ex-twitter-sa2"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>David Cameron doesn't seem to have done too badly until now. Otherwise #milifandom and #cleggers would be attacking him for these bad things</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>Even though I don't like UKIP I'm hating them less and less every day, they do actually have very some good policies</td></tr></table></p>
</td></tr></table></p>
<p>This has led some researchers to develop approaches where syntactic
structure is also factored into sentiment analysis,</p>
<p>A further challenge in sentiment analysis is deciding the right level
of granularity for the topic under discussion. Often, we can agree in the overall polarity of a
sentence (or even of larger texts) because there is a single dominant topic. But in a list-like construction
such as <a class="reference internal" href="#ex-twitter-sa3">(4)</a>, different sentiments are associated with different
entities, and there is no sensible way of aggregating this into a combined
polarity score for the text as a whole:</p>
<span class="target" id="ex-twitter-sa3"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td>&#64;hugorifkind Audience - good. Mili - bad. Clegg - a bit sad. Cam - unscathed</td></tr></table></p>
<p>Finally, our current approaches to language processing struggle with
sarcasm, irony and satire, which again lead to polarity reversals.</p>
<span class="target" id="ex-twitter-sa4"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>LOVE being sat on a plane for 4 hours after a 10 hour flight !! Soooo fun !</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>The wrong spelling of Ed Miliband is trending, but not the correct one. Good job, Britain.</td></tr></table></p>
</td></tr></table></p>
</div>
</div>
<div class="section" id="sub-tasks">
<h1>2&nbsp;&nbsp;&nbsp;Sub-Tasks</h1>
<ul class="simple">
<li>WSD</li>
<li>pronoun resolution</li>
<li>entailment</li>
<li>finding things in text</li>
<li>language modeling</li>
<li>collocations</li>
</ul>
<div class="section" id="sentence-segmentation">
<h2>2.1&nbsp;&nbsp;&nbsp;Sentence Segmentation</h2>
<p>Many NLP operations are performed on the sentence level, e.g. we parse sentences, not phrases or paragraphs.
A declarative sentence, like <span class="example">Today is Tuesday,</span> is the smallest linguistic unit that can be assigned a truth value.
Sentences are terminated with a period, question mark, or exclamation mark, though we cannot merely search for these characters in continuous text in order to locate sentence boundaries.
In some cases, periods occur inside a sentence, e.g. <span class="example">The work is being done by Mr. W.E.B. Du Bois, Ph. D.</span>
In some cases, a period serves dual purposes as marking an abbreviation and the sentence boundary, as in the previous sentence.
(In some cases a following quotation mark or parenthesis is part of the sentence.)</p>
<p>An initial capital is a further clue to a sentence boundary.
However, some sentences can begin with a lowercase letter, e.g. <span class="example">i is a common name for a variable.</span></p>
<p>A simple heuristic is to segment a sentence at a period, unless the previous word is from a known list of abbreviations, in which case we check whether the following word is capitalised.
This heuristic works in about 95% of cases (at least, for English).
However, some texts are extra challenging, such as the following fragment from an article in the opinion pages of the New York Times:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(6)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>When I rented my first city apartment, I discovered it came with a roommate. We did better than make do: We bonded.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>My roommate cheered and comforted me, although I only ever glimpsed her &#8212; a blur &#8212; from the corner of my eye. (The &quot;her&quot; was a guess &#8212; I thought of us as two girls on our own.)</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>&quot;She&#8217;s a runaway hamster,&quot; I told people at work. Not a pet. My friend.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">d.</td><td width="15"></td><td>My bosses stared at me. &quot;Alison. That&#8217;s. Not. A. Hamster.&quot; They warned me that my roommate would chew my eyelids while I slept.</td></tr></table></p>
</td></tr></table></p>
<p>For better sentence segmentation, it is necessary to apply automatic classification methods that learn whether a given period (or question mark or exclamation mark) marks a sentence boundary.
These methods are able to weigh up a variety of contributing factors.</p>
</div>
</div>
<div class="section" id="automatic-natural-language-understanding">
<h1>3&nbsp;&nbsp;&nbsp;Automatic Natural Language Understanding</h1>
<!-- >>> from nltk.misc import babelize_shell -->
<p>We have been exploring language bottom-up, with the help of texts and
the Python programming
language. However, we're also interested in exploiting our knowledge of language and computation
by building useful language technologies. We'll take the opportunity
now to step back from the nitty-gritty of code in order to paint a
bigger picture of natural language processing.</p>
<p>At a purely practical level, we all need help to navigate the universe of information
locked up in text on the Web. Search engines have been crucial to the
growth and popularity of the Web, but have some shortcomings.
It takes skill, knowledge, and some luck,
to extract answers to such questions as: <span class="example">What tourist sites can I
visit between Philadelphia and Pittsburgh on a limited budget?</span>
<span class="example">What do experts say about digital SLR cameras?</span> <span class="example">What
predictions about the steel market were made by credible commentators
in the past week?</span> Getting a computer to answer them automatically
involves a range of language processing tasks, including information extraction,
inference, and summarization, and would need to be carried out on a scale
and with a level of robustness that is still beyond our current capabilities.</p>
<p>On a more philosophical level, a long-standing challenge within artificial intelligence
has been to build intelligent machines, and a major part of intelligent behaviour is understanding
language. For many years this goal has been seen as too difficult.
However, as NLP technologies become more mature, and robust methods for
analyzing unrestricted text become more widespread, the prospect of
natural language understanding has re-emerged as a plausible goal.</p>
<p>In this section we describe some language understanding technologies,
to give you a sense of the interesting challenges that are waiting for you.</p>
<div class="section" id="word-sense-disambiguation">
<h2>3.1&nbsp;&nbsp;&nbsp;Word Sense Disambiguation</h2>
<p>In <a name="word_sense_disambiguation_index_term" /><span class="termdef">word sense disambiguation</span> we want to work out
which sense of a word was intended in a given context. Consider the
ambiguous words <span class="example">serve</span> and <span class="example">dish</span>:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(7)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><span class="example">serve</span>: help with food or drink; hold an office; put ball into play</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><span class="example">dish</span>: plate; course of a meal; communications device</td></tr></table></p>
</td></tr></table></p>
<p>In a sentence containing the phrase: <span class="example">he served the dish</span>, you
can detect that both <span class="example">serve</span> and <span class="example">dish</span> are being used with
their food meanings. It's unlikely that the topic of discussion
shifted from sports to crockery in the space of three words.
This would force you to invent bizarre images, like a tennis pro
taking out his or her frustrations on a china tea-set laid out beside the court.
In other words, we automatically disambiguate words using context, exploiting
the simple fact that nearby words have closely related meanings.
As another example of this contextual effect, consider the word
<span class="example">by</span>, which has several meanings, e.g.: <span class="example">the book by
Chesterton</span> (agentive &#8212; Chesterton was the author of the book);
<span class="example">the cup by the stove</span> (locative &#8212; the stove is where the
cup is); and <span class="example">submit by Friday</span> (temporal &#8212; Friday is the
time of the submitting).
Observe in <a class="reference internal" href="#ex-lost-children">(8c)</a> that the meaning of the italicized word helps us
interpret the meaning of <span class="example">by</span>.</p>
<span class="target" id="ex-lost-children"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(8)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>The lost children were found by the <span class="emphasis">searchers</span>  (agentive)</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>The lost children were found by the <span class="emphasis">mountain</span>   (locative)</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>The lost children were found by the <span class="emphasis">afternoon</span>  (temporal)</td></tr></table></p>
</td></tr></table></p>
</div>
<div class="section" id="pronoun-resolution">
<h2>3.2&nbsp;&nbsp;&nbsp;Pronoun Resolution</h2>
<p>A deeper kind of language understanding is to work out &quot;who did what to whom&quot; &#8212;
i.e., to detect the subjects and objects of verbs. You learnt to do this in
elementary school, but it's harder than you might think.
In the sentence <span class="example">the thieves stole the paintings</span>
it is easy to tell who performed the stealing action.
Consider three possible following sentences in <a class="reference internal" href="#ex-thieves">(9c)</a>, and try to determine
what was sold, caught, and found (one case is ambiguous).</p>
<span class="target" id="ex-thieves"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(9)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>The thieves stole the paintings. They were subsequently <span class="emphasis">sold</span>.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>The thieves stole the paintings. They were subsequently <span class="emphasis">caught</span>.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>The thieves stole the paintings. They were subsequently <span class="emphasis">found</span>.</td></tr></table></p>
</td></tr></table></p>
<p>Answering this question involves finding the <a name="antecedent_index_term" /><span class="termdef">antecedent</span> of the pronoun <span class="example">they</span>,
either thieves or paintings. Computational techniques for tackling this problem
include <a name="anaphora_resolution_index_term" /><span class="termdef">anaphora resolution</span> &#8212; identifying what a pronoun or noun phrase
refers to &#8212; and <a name="semantic_role_labeling_index_term" /><span class="termdef">semantic role labeling</span> &#8212; identifying how a noun phrase
relates to the verb (as agent, patient, instrument, and so on).</p>
</div>
<div class="section" id="generating-language-output">
<h2>3.3&nbsp;&nbsp;&nbsp;Generating Language Output</h2>
<p>If we can automatically solve such problems of language understanding, we will
be able to move on to tasks that involve generating language output, such as
<a name="question_answering_index_term" /><span class="termdef">question answering</span> and <a name="machine_translation_index_term_2" /><span class="termdef">machine translation</span>. In the first case,
a machine should be able to answer a user's questions relating to collection of texts:</p>
<span class="target" id="ex-qa-application"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(10)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td><em>Text:</em> ... The thieves stole the paintings. They were subsequently sold. ...</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><em>Human:</em> Who or what was sold?</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td><em>Machine:</em> The paintings.</td></tr></table></p>
</td></tr></table></p>
<p>The machine's answer demonstrates that it has correctly worked out that <span class="example">they</span>
refers to paintings and not to thieves. In the second case, the machine should
be able to translate the text into another language, accurately
conveying the meaning of the original text. In translating the example text into French,
we are forced to choose the gender of the pronoun in the second sentence:
<span class="example">ils</span> (masculine) if the thieves are found, and <span class="example">elles</span> (feminine) if
the paintings are found. Correct translation actually depends on correct understanding of
the pronoun.</p>
<span class="target" id="ex-mt-application"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(11)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>The thieves stole the paintings. They were subsequently found.</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>Les voleurs ont vol&#233; les peintures. Ils ont &#233;t&#233; trouv&#233;s plus tard. (the thieves)</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">c.</td><td width="15"></td><td>Les voleurs ont vol&#233; les peintures. Elles ont &#233;t&#233; trouv&#233;es plus tard. (the paintings)</td></tr></table></p>
</td></tr></table></p>
<p>In all of these examples, working out the sense of a word, the subject of a verb, and the
antecedent of a pronoun are steps in establishing the meaning of a sentence, things
we would expect a language understanding system to be able to do.</p>
</div>
<div class="section" id="spoken-dialog-systems">
<h2>3.4&nbsp;&nbsp;&nbsp;Spoken Dialog Systems</h2>
<p>In the history of artificial intelligence, the chief measure of intelligence
has been a linguistic one, namely the <a name="turing_test_index_term" /><span class="termdef">Turing Test</span>: can a dialogue system,
responding to a user's text input, perform so naturally that we cannot distinguish
it from a human-generated response?  In contrast, today's commercial dialogue systems
are very limited, but still perform useful functions in narrowly-defined domains,
as we see here:</p>
<div class="line-block">
<div class="line">S: How may I help you?</div>
<div class="line">U: When is Saving Private Ryan playing?</div>
<div class="line">S: For what theater?</div>
<div class="line">U: The Paramount theater.</div>
<div class="line">S: Saving Private Ryan is not playing at the Paramount theater, but</div>
<div class="line">it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.</div>
</div>
<p>You could not ask this system to provide driving instructions or
details of nearby restaurants unless the required information
had already been stored and suitable question-answer pairs
had been incorporated into the language processing system.</p>
<p>Observe that this system seems to understand the user's goals:
the user asks when a movie is showing and the system
correctly determines from this that the user wants to see
the movie. This inference seems so obvious that you probably
didn't notice it was made, yet a natural language system
needs to be endowed with this capability in order to interact
naturally. Without it, when asked <span class="example">Do you know when Saving Private
Ryan is playing?</span>, a system might unhelpfully respond with a cold <span class="example">Yes</span>.
However, the developers of commercial dialogue systems use
contextual assumptions and business logic to ensure that the different ways in which a user might
express requests or provide information are handled in a way that
makes sense for the particular application. So, if you type
<span class="example">When is ...</span>, or <span class="example">I want to know when ...</span>, or <span class="example">Can you tell me
when ...</span>, simple rules will always yield screening times. This is
enough for the system to provide a useful service.</p>
<span class="target" id="fig-sds"></span><div class="figure" id="fig-sds">
<img alt="../images/dialogue.png" src="../images/dialogue.png" style="width: 600.0px; height: 324.0px;" />
<p class="caption"><span class="caption-label">Figure 3.1</span>: Simple Pipeline Architecture for a Spoken Dialogue System:
Spoken input (top left) is analyzed, words are recognized, sentences are parsed and
interpreted in context, application-specific actions take place (top right);
a response is planned, realized as a syntactic structure, then to suitably
inflected words, and finally to spoken output; different types of
linguistic knowledge inform each stage of the process.</p>
</div>
<p>Dialogue systems give us an opportunity to mention the
commonly assumed pipeline for NLP.
<a class="reference internal" href="#fig-sds">3.1</a> shows the architecture of a simple dialogue system.
Along the top of the diagram, moving from left to right, is a
&quot;pipeline&quot; of some language understanding <a name="components_index_term" /><span class="termdef">components</span>.
These map from speech input via syntactic parsing
to some kind of meaning representation. Along the middle, moving from
right to left, is the reverse pipeline of components for converting
concepts to speech. These components make up the dynamic aspects of the system.
At the bottom of the diagram are some representative bodies of
static information: the repositories of language-related data that
the processing components draw on to do their work.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
For an example of a primitive dialogue system, try having
a conversation with an NLTK chatbot. To see the available chatbots,
run <tt class="doctest"><span class="pre">nltk.chat.chatbots()</span></tt>.
(Remember to <tt class="doctest"><span class="pre"><span class="pysrc-keyword">import</span> nltk</span></tt> first.)</p>
</div>
</div>
<div class="section" id="textual-entailment">
<h2>3.5&nbsp;&nbsp;&nbsp;Textual Entailment</h2>
<p>The challenge of language understanding has been brought into focus in recent years by a public
&quot;shared task&quot; called Recognizing Textual Entailment (RTE). The basic
scenario is simple. Suppose you want to find evidence to support
the hypothesis: <span class="example">Sandra Goudie was defeated by Max Purnell</span>, and
that you have another short text that seems to be relevant, for example,
<span class="example">Sandra Goudie was first elected to Parliament in the 2002 elections,
narrowly winning the seat of Coromandel by defeating Labour candidate
Max Purnell and pushing incumbent Green MP Jeanette Fitzsimons into
third place</span>. Does the text provide enough evidence for you to
accept the hypothesis?  In this particular case, the answer will be &quot;No.&quot;
You can draw this conclusion easily, but it is very hard to come up with
automated methods for making the right decision. The RTE
Challenges provide data that allow competitors to develop their
systems, but not enough data for &quot;brute force&quot; machine learning techniques (a topic
we will cover in <a class="reference external" href="ch06.html#chap-data-intensive">chap-data-intensive</a>). Consequently, some
linguistic analysis is crucial. In the previous example, it is important
for the system to note that <span class="example">Sandra Goudie</span> names the person being
defeated in the hypothesis, not the person doing the defeating in the
text. As another illustration of the difficulty of the task, consider
the following text-hypothesis pair:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(12)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>Hypothesis: Golinkin has written eighteen books</td></tr></table></p>
</td></tr></table></p>
<p>In order to determine whether the hypothesis is supported by the
text, the system needs the following background knowledge:
(i) if someone is an author of a book, then he/she has written that
book; (ii) if someone is an editor of a book, then he/she has not
written (all of) that book; (iii) if someone is editor or author of eighteen
books, then one cannot conclude that he/she is author of eighteen books.</p>
</div>
<div class="section" id="limitations-of-nlp">
<h2>3.6&nbsp;&nbsp;&nbsp;Limitations of NLP</h2>
<p>Despite the research-led advances in tasks like RTE, natural language
systems that have been deployed for real-world applications still cannot perform
common-sense reasoning or draw on world knowledge in a general and
robust manner. We can wait for these difficult artificial
intelligence problems to be solved, but in the meantime it is
necessary to live with some severe limitations on the reasoning and
knowledge capabilities of natural language systems. Accordingly, right
from the beginning, an important goal of NLP research has been to
make progress on the difficult task of building technologies that
&quot;understand language,&quot; using superficial yet powerful techniques instead of
unrestricted knowledge and reasoning capabilities.
Indeed, this is one of the goals of this book, and we hope to equip you with
the knowledge and skills to build useful NLP systems, and to
contribute to the long-term aspiration of building intelligent machines.</p>
</div>
</div>
<div class="section" id="overview-of-nltk">
<span id="sec-overview-of-nltk"></span><h1>4&nbsp;&nbsp;&nbsp;Overview of NLTK</h1>
<div class="section" id="getting-started">
<h2>4.1&nbsp;&nbsp;&nbsp;Getting Started</h2>
<p>If you already have Python and Pip installed, you can install NLTK by running <tt class="doctest"><span class="pre">pip install nltk</span></tt>.
If you experience any difficulties with installation, you can consult <tt class="doctest"><span class="pre">http://nltk.org/</span></tt> for more detailed instructions.</p>
<p>Once you've installed NLTK, you can install the datasets required for the book by running <tt class="doctest"><span class="pre">python -m nltk.downloader book</span></tt>.
Alternatively, you can start up the Python interpreter and install the data by
typing the following two commands at Python's <tt class="doctest"><span class="pre"><span class="pysrc-prompt">&gt;&gt;&gt;</span></span></tt> prompt, then selecting
the <tt class="doctest"><span class="pre">book</span></tt> collection as shown in <a class="reference internal" href="#fig-nltk-downloader">4.1</a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">import</span> nltk
<span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.download()</pre>
</td>
</tr></table></td></tr>
</table></div>
<span class="target" id="fig-nltk-downloader"></span><div class="figure" id="fig-nltk-downloader">
<img alt="../images/nltk-downloader.png" src="../images/nltk-downloader.png" style="width: 667.0px; height: 282.0px;" />
<p class="caption"><span class="caption-label">Figure 4.1</span>: Downloading the NLTK Book Collection: browse the available packages
using <tt class="doctest"><span class="pre">nltk.download()</span></tt>. The <strong>Collections</strong> tab on the downloader
shows how the packages are grouped into sets, and you should select the line labeled
<strong>book</strong> to obtain all
data required for the examples and exercises in this book. It consists
of about 40 compressed files requiring about 120Mb disk space.
The full collection, option <strong>all</strong> in the downloader, is about five times this size.</p>
</div>
<p>Once the data is downloaded to your machine, you can load some of it using the Python interpreter as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk.book <span class="pysrc-keyword">import</span> *
<span class="pysrc-output">*** Introductory Examples for the NLTK Book ***</span>
<span class="pysrc-output">Loading text1, ..., text9 and sent1, ..., sent9</span>
<span class="pysrc-output">Type the name of the text or sentence to view it.</span>
<span class="pysrc-output">Type: 'texts()' or 'sents()' to list the materials.</span>
<span class="pysrc-output">text1: Moby Dick by Herman Melville 1851</span>
<span class="pysrc-output">text2: Sense and Sensibility by Jane Austen 1811</span>
<span class="pysrc-output">text3: The Book of Genesis</span>
<span class="pysrc-output">text4: Inaugural Address Corpus</span>
<span class="pysrc-output">text5: Chat Corpus</span>
<span class="pysrc-output">text6: Monty Python and the Holy Grail</span>
<span class="pysrc-output">text7: Wall Street Journal</span>
<span class="pysrc-output">text8: Personals Corpus</span>
<span class="pysrc-output">text9: The Man Who Was Thursday by G . K . Chesterton 1908</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Any time we want to find out about these texts, we just have
to enter their names at the Python prompt:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text1
<span class="pysrc-output">&lt;Text: Moby Dick by Herman Melville 1851&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>text2
<span class="pysrc-output">&lt;Text: Sense and Sensibility by Jane Austen 1811&gt;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>If you have your own collection of text, stored in local files, you can load
it and &quot;tokenize&quot; it into a list of words and punctuation
(see <a href="#id15"><span class="problematic" id="id16">chap_words_</span></a> for more details about tokenization).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk <span class="pysrc-keyword">import</span> word_tokenize
<span class="pysrc-prompt">&gt;&gt;&gt; </span>raw = open(<span class="pysrc-string">'melville-moby_dick.txt'</span>).read()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>tokens = word_tokenize(raw)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>tokens[:100]
<span class="pysrc-output">['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.',</span>
<span class="pysrc-output">'(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar',</span>
<span class="pysrc-output">'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',',</span>
<span class="pysrc-output">'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', ...]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>len(tokens)
<span class="pysrc-output">254989</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>You can convert this list of tokens into an NLTK &quot;Text&quot; object in order to
perform the simple text processing operations described in the following sections,
using: <tt class="doctest"><span class="pre"><span class="pysrc-keyword">from</span> nltk <span class="pysrc-keyword">import</span> Text; text1 = Text(tokens)</span></tt>.</p>
</div>
<div class="section" id="searching-text">
<h2>4.2&nbsp;&nbsp;&nbsp;Searching Text</h2>
<p>There are many ways to examine the context of a text apart from simply reading it.
A concordance view shows us every occurrence of a given word, together with some context.
Here we look up the word <span class="example">monstrous</span> in <em>Moby Dick</em>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text1.concordance(<span class="pysrc-string">&quot;monstrous&quot;</span>)
<span class="pysrc-output">Displaying 11 of 11 matches:</span>
<span class="pysrc-output">ong the former , one was of a most monstrous size . ... This came towards us ,</span>
<span class="pysrc-output">ON OF THE PSALMS . &quot; Touching that monstrous bulk of the whale or ork we have r</span>
<span class="pysrc-output">ll over with a heathenish array of monstrous clubs and spears . Some were thick</span>
<span class="pysrc-output">d as you gazed , and wondered what monstrous cannibal and savage could ever hav</span>
<span class="pysrc-output">that has survived the flood ; most monstrous and most mountainous ! That Himmal</span>
<span class="pysrc-output">they might scout at Moby Dick as a monstrous fable , or still worse and more de</span>
<span class="pysrc-output">th of Radney .'&quot; CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l</span>
<span class="pysrc-output">ing Scenes . In connexion with the monstrous pictures of whales , I am strongly</span>
<span class="pysrc-output">ere to enter upon those still more monstrous stories of them which are to be fo</span>
<span class="pysrc-output">ght have been rummaged out of this monstrous cabinet there is no telling . But</span>
<span class="pysrc-output">of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The first time you use a concordance on a particular text, it takes a
few extra seconds to build an index so that subsequent searches are fast.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try searching for other words; to save re-typing, you might be able to
use up-arrow, Ctrl-up-arrow or Alt-p to access the previous command and modify the word being searched.
You can also try searches on some of the other texts we have included.
For example, search <em>Sense and Sensibility</em> for the word
<span class="example">affection</span>, using <tt class="doctest"><span class="pre">text2.concordance(<span class="pysrc-string">&quot;affection&quot;</span>)</span></tt>.
Search the book of Genesis to find out how long some people lived, using
<tt class="doctest"><span class="pre">text3.concordance(<span class="pysrc-string">&quot;lived&quot;</span>)</span></tt>. You could look at <tt class="doctest"><span class="pre">text4</span></tt>, the
<em>Inaugural Address Corpus</em>, to see examples of English going
back to 1789, and search for words like <span class="example">nation</span>, <span class="example">terror</span>, <span class="example">god</span>
to see how these words have been used differently over time.
We've also included <tt class="doctest"><span class="pre">text5</span></tt>, the <em>NPS Chat Corpus</em>: search this for
unconventional words like <span class="example">im</span>, <span class="example">ur</span>, <span class="example">lol</span>.
(Note that this corpus is uncensored!)</p>
</div>
<p>Once you've examined these texts, we hope you have a new
sense of the richness and diversity of language. In the next chapter
you will learn how to access a broader range of text, including text in
languages other than English.</p>
<p>A concordance permits us to see words in context. For example, we saw that
<span class="example">monstrous</span> occurred in contexts such as <span class="example">the ___ pictures</span>
and <span class="example">a ___ size</span> . What other words appear in a similar range
of contexts?</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text1.similar(<span class="pysrc-string">&quot;monstrous&quot;</span>)
<span class="pysrc-output">mean part maddens doleful gamesome subtly uncommon careful untoward</span>
<span class="pysrc-output">exasperate loving passing mouldy christian few true mystifying</span>
<span class="pysrc-output">imperial modifies contemptible</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>text2.similar(<span class="pysrc-string">&quot;monstrous&quot;</span>)
<span class="pysrc-output">very heartily so exceedingly remarkably as vast a great amazingly</span>
<span class="pysrc-output">extremely good sweet</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Observe that we get different results for different texts.
Austen uses this word quite differently from Melville; for her, <span class="example">monstrous</span> has
positive connotations, and sometimes functions as an intensifier like the word
<span class="example">very</span>.</p>
<p>The <tt class="doctest"><span class="pre">common_contexts</span></tt> method allows us to examine just the
contexts that are shared by two or more words, such as <span class="example">monstrous</span>
and <span class="example">very</span></p>
<pre class="literal-block">
&gt;&gt;&gt; text2.common_contexts([&quot;monstrous&quot;, &quot;very&quot;])
a_pretty is_pretty am_glad be_glad a_lucky
</pre>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Pick another pair of words and compare their usage in two different texts, using
the <tt class="doctest"><span class="pre">similar()</span></tt> and <tt class="doctest"><span class="pre">common_contexts()</span></tt> functions.</p>
</div>
<p>It is one thing to automatically detect that a particular word occurs in a text,
and to display some words that appear in the same context. However, we can also determine
the <em>location</em> of a word in the text: how many words from the beginning it appears.
This positional information can be displayed using a <a name="dispersion_plot_index_term" /><span class="termdef">dispersion plot</span>.
Each stripe represents an instance
of a word, and each row represents the entire text. In <a class="reference internal" href="#fig-inaugural">4.2</a> we
see some striking patterns of word usage over the last 220 years
(in an artificial text constructed by joining
the texts of the Inaugural Address Corpus end-to-end).
You can produce this plot as shown below.
You might like to try more words (e.g., <span class="example">liberty</span>, <span class="example">constitution</span>),
and different texts. Can you predict the
dispersion of a word before you view it?</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text4.dispersion_plot([<span class="pysrc-string">&quot;citizens&quot;</span>, <span class="pysrc-string">&quot;democracy&quot;</span>, <span class="pysrc-string">&quot;freedom&quot;</span>, <span class="pysrc-string">&quot;duties&quot;</span>, <span class="pysrc-string">&quot;America&quot;</span>])</pre>
</td>
</tr></table></td></tr>
</table></div>
<span class="target" id="fig-inaugural"></span><div class="figure" id="fig-inaugural">
<img alt="../images/inaugural.png" src="../images/inaugural.png" style="width: 738.0px; height: 324.0px;" />
<p class="caption"><span class="caption-label">Figure 4.2</span>: Lexical Dispersion Plot for Words in U.S. Presidential Inaugural Addresses:
This can be used to investigate changes in language use over time.</p>
</div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Important:</strong>
You need to have Python's NumPy and Matplotlib packages installed
in order to produce the graphical plots used in this book.</p>
</div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">You can also plot the frequency of word usage through time using
<tt class="doctest"><span class="pre">https://books.google.com/ngrams</span></tt></p>
</div>
<p>Let's try generating some random text in the various
styles we have just seen, e.g.:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text3.generate()
<span class="pysrc-output">In the beginning of his brother is a hairy man , whose top may reach</span>
<span class="pysrc-output">unto heaven ; and ye shall sow the land of Egypt there was no bread in</span>
<span class="pysrc-output">all that he was taken out of the month , upon the earth . So shall thy</span>
<span class="pysrc-output">wages be ? And they made their father ; and Isaac was old , and kissed</span>
<span class="pysrc-output">him : and Laban with his cattle in the midst of the hands of Esau thy</span>
<span class="pysrc-output">first born , and Phichol the chief butler unto his son Isaac , she</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">The <tt class="doctest"><span class="pre">generate()</span></tt> method is not available in NLTK 3.0 but will be
reinstated in a subsequent version.</p>
</div>
<!-- Note that the first time you run this command, it is slow because it gathers statistics
about word sequences. Each time you run it, you will get different output text.
Now try generating random text in the style of an inaugural address or an
Internet chat room. Although the text is random, it re-uses common words and
phrases from the source text and gives us a sense of its style and content.
(What is lacking in this randomly generated text?) -->
<!-- note
When ``generate`` produces its output, punctuation is split off
from the preceding word. While this is not correct formatting
for English text, we do it to make clear that words and
punctuation are independent of one another. You will learn
more about this in chap-words_. -->
</div>
<div class="section" id="frequency-distributions">
<h2>4.3&nbsp;&nbsp;&nbsp;Frequency Distributions</h2>
<p>How can we automatically identify the words of a text that are most
informative about the topic and genre of the text?  Imagine how you might
go about finding the 50 most frequent words of a book. One method
would be to keep a tally for each vocabulary item, like that shown in <a class="reference internal" href="#fig-tally">4.3</a>.
The tally would need thousands of rows, and it would be an exceedingly
laborious process &#8212; so laborious that we would rather assign the task to a machine.</p>
<span class="target" id="fig-tally"></span><div class="figure" id="fig-tally">
<img alt="../images/tally.png" src="../images/tally.png" style="width: 231.8px; height: 155.0px;" />
<p class="caption"><span class="caption-label">Figure 4.3</span>: Counting Words Appearing in a Text (a frequency distribution)</p>
</div>
<p>The table in <a class="reference internal" href="#fig-tally">4.3</a> is known as a <a name="frequency_distribution_index_term" /><span class="termdef">frequency distribution</span>,
and it tells us the frequency of each vocabulary item in the text.
(In general, it could count any kind of observable event.)
It is a &quot;distribution&quot;
because it tells us how the total number of word tokens in the text
are distributed across the vocabulary items.
Since we often need frequency distributions in language processing, NLTK
provides built-in support for them. Let's use a <tt class="doctest"><span class="pre">FreqDist</span></tt> to find the
50 most frequent words of <em>Moby Dick</em>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk <span class="pysrc-keyword">import</span> FreqDist
<span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist1 = FreqDist(text1) <a name="freq-dist-call" /><a href="#ref-freq-dist-call"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(fdist1) <a name="freq-dist-inspect" /><a href="#ref-freq-dist-inspect"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a>
<span class="pysrc-output">&lt;FreqDist with 19317 samples and 260819 outcomes&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist1.most_common(50) <a name="freq-dist-most-common" /><a href="#ref-freq-dist-most-common"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></a>
<span class="pysrc-output">[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024),</span>
<span class="pysrc-output">('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982),</span>
<span class="pysrc-output">(&quot;'&quot;, 2684), ('-', 2552), ('his', 2459), ('it', 2209), ('I', 2124),</span>
<span class="pysrc-output">('s', 1739), ('is', 1695), ('he', 1661), ('with', 1659), ('was', 1632),</span>
<span class="pysrc-output">('as', 1620), ('&quot;', 1478), ('all', 1462), ('for', 1414), ('this', 1280),</span>
<span class="pysrc-output">('!', 1269), ('at', 1231), ('by', 1137), ('but', 1113), ('not', 1103),</span>
<span class="pysrc-output">('--', 1070), ('him', 1058), ('from', 1052), ('be', 1030), ('on', 1005),</span>
<span class="pysrc-output">('so', 918), ('whale', 906), ('one', 889), ('you', 841), ('had', 767),</span>
<span class="pysrc-output">('have', 760), ('there', 715), ('But', 705), ('or', 697), ('were', 680),</span>
<span class="pysrc-output">('now', 646), ('which', 640), ('?', 637), ('me', 627), ('like', 624)]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist1[<span class="pysrc-string">'whale'</span>]
<span class="pysrc-output">906</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>When we first invoke <tt class="doctest"><span class="pre">FreqDist</span></tt>, we pass the name of the text as an
argument <a class="reference internal" href="#freq-dist-call"><span id="ref-freq-dist-call"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>. We can inspect the total number of words (&quot;outcomes&quot;)
that have been counted up <a class="reference internal" href="#freq-dist-inspect"><span id="ref-freq-dist-inspect"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a> &#8212; 260,819 in the
case of <em>Moby Dick</em>. The expression <tt class="doctest"><span class="pre">most_common(50)</span></tt> gives us a list of
the 50 most frequently occurring types in the text <a class="reference internal" href="#freq-dist-most-common"><span id="ref-freq-dist-most-common"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try the preceding frequency distribution example for yourself, for
<tt class="doctest"><span class="pre">text2</span></tt>.  Be careful to use the correct parentheses and uppercase letters.
If you get an error message <tt class="doctest"><span class="pre">NameError: name <span class="pysrc-string">'FreqDist'</span> <span class="pysrc-keyword">is</span> <span class="pysrc-keyword">not</span> defined</span></tt>,
you need to start your work with <tt class="doctest"><span class="pre"><span class="pysrc-keyword">from</span> nltk.book <span class="pysrc-keyword">import</span> *</span></tt></p>
</div>
<!-- SB: no period after the above import statement -->
<p>Do any words produced in the last example help us grasp the topic or genre of this text?
Only one word, <span class="example">whale</span>, is slightly informative!  It occurs over 900 times.
The rest of the words tell us nothing about the text; they're just English &quot;plumbing.&quot;
What proportion of the text is taken up with such words?
We can generate a cumulative frequency plot for these words,
using <tt class="doctest"><span class="pre">fdist1.plot(50, cumulative=True)</span></tt>, to produce the graph in <a class="reference internal" href="#fig-fdist-moby">4.4</a>.
These 50 words account for nearly half the book!</p>
<span class="target" id="fig-fdist-moby"></span><div class="figure" id="fig-fdist-moby">
<img alt="../images/fdist-moby.png" src="../images/fdist-moby.png" style="width: 207.20000000000002px; height: 116.2px;" />
<p class="caption"><span class="caption-label">Figure 4.4</span>: Cumulative Frequency Plot for 50 Most Frequently Words in <em>Moby Dick</em>:
these account for nearly half of the tokens.</p>
</div>
<p>If the frequent words don't help us, how about the words that occur once
only, the so-called <a name="hapaxes_index_term" /><span class="termdef">hapaxes</span>?  View them by typing <tt class="doctest"><span class="pre">fdist1.hapaxes()</span></tt>.
This list contains <span class="example">lexicographer</span>, <span class="example">cetological</span>,
<span class="example">contraband</span>, <span class="example">expostulations</span>, and about 9,000 others.
It seems that there are too many rare words, and without seeing the
context we probably can't guess what half of the hapaxes mean in any case!
Since neither frequent nor infrequent words help, we need to try
something else.</p>
</div>
<div class="section" id="fine-grained-selection-of-words">
<h2>4.4&nbsp;&nbsp;&nbsp;Fine-grained Selection of Words</h2>
<p>Next, let's look at the <em>long</em> words of a text; perhaps these will be
more characteristic and informative.  For this we adapt some notation
from set theory.  We would like to find the words from the vocabulary
of the text that are more than 15 characters long.  Let's call
this property <span class="math">P</span>, so that <span class="math">P(w)</span> is true
if and only if <span class="math">w</span> is more than 15 characters long.
Now we can express the words of interest using mathematical
set notation as shown in <a class="reference internal" href="#ex-set-comprehension-math">(13a)</a>.
This means &quot;the set of all <span class="math">w</span> such that <span class="math">w</span> is an
element of <span class="math">V</span> (the vocabulary) and <span class="math">w</span> has property <span class="math">P</span>&quot;.</p>
<span class="target" id="ex-set-comprehension"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(13)</td><td width="15"></td><td><span class="target" id="ex-set-comprehension-math"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>{<span class="math">w</span> | <span class="math">w</span> &#8712; <span class="math">V</span> &amp; <span class="math">P(w)</span>}</td></tr></table></p>
<span class="target" id="ex-set-comprehension-python"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td><tt class="doctest"><span class="pre">[w <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> V <span class="pysrc-keyword">if</span> p(w)]</span></tt></td></tr></table></p>
</td></tr></table></p>
<p>The corresponding Python expression is given in <a class="reference internal" href="#ex-set-comprehension-python">(13b)</a>.
(Note that it produces a list, not a set, which means that duplicates are possible.)
Observe how similar the two notations are.  Let's go one more step and
write executable Python code:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>V = set(text1)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>long_words = [w <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> V <span class="pysrc-keyword">if</span> len(w) &gt; 15]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sorted(long_words)
<span class="pysrc-output">['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',</span>
<span class="pysrc-output">'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',</span>
<span class="pysrc-output">'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',</span>
<span class="pysrc-output">'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',</span>
<span class="pysrc-output">'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',</span>
<span class="pysrc-output">'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>For each word <tt class="doctest"><span class="pre">w</span></tt> in the vocabulary <tt class="doctest"><span class="pre">V</span></tt>, we check whether
<tt class="doctest"><span class="pre">len(w)</span></tt> is greater than 15; all other words will
be ignored.  We will discuss this syntax more carefully later.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Try out the previous statements in the Python interpreter,
and experiment with changing the text and changing the length condition.
Does it make a difference to your results if you change the
variable names, e.g., using <tt class="doctest"><span class="pre">[word <span class="pysrc-keyword">for</span> word <span class="pysrc-keyword">in</span> vocab <span class="pysrc-keyword">if</span> ...]</span></tt>?</p>
</div>
<p>Let's return to our task of finding words that characterize a text.
Notice that the long words in <tt class="doctest"><span class="pre">text4</span></tt> reflect its national focus
&#8212; <span class="example">constitutionally</span>, <span class="example">transcontinental</span> &#8212;
whereas those in <tt class="doctest"><span class="pre">text5</span></tt> reflect its informal content:
<span class="example">boooooooooooglyyyyyy</span> and <span class="example">yuuuuuuuuuuuummmmmmmmmmmm</span>.
Have we succeeded in automatically extracting words that typify
a text?  Well, these very long words are often hapaxes (i.e., unique)
and perhaps it would be better to find <em>frequently occurring</em>
long words. This seems promising since it eliminates
frequent short words (e.g., <span class="example">the</span>) and infrequent long words
(e.g. <span class="example">antiphilosophists</span>).
Here are all words from the chat corpus
that are longer than seven characters, that occur more than seven times:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist5 = FreqDist(text5)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sorted(w <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> set(text5) <span class="pysrc-keyword">if</span> len(w) &gt; 7 <span class="pysrc-keyword">and</span> fdist5[w] &gt; 7)
<span class="pysrc-output">['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',</span>
<span class="pysrc-output">'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',</span>
<span class="pysrc-output">'innocent', 'listening', 'remember', 'seriously', 'something', 'together',</span>
<span class="pysrc-output">'tomorrow', 'watching']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Notice how we have used two conditions: <tt class="doctest"><span class="pre">len(w) &gt; 7</span></tt> ensures that the
words are longer than seven letters, and <tt class="doctest"><span class="pre">fdist5[w] &gt; 7</span></tt> ensures that
these words occur more than seven times. At last we have managed to
automatically identify the frequently-occurring content-bearing
words of the text. It is a modest but important milestone: a tiny piece of code,
processing tens of thousands of words, produces some informative output.</p>
</div>
<div class="section" id="collocations-and-bigrams">
<h2>4.5&nbsp;&nbsp;&nbsp;Collocations and Bigrams</h2>
<p>A <a name="collocation_index_term" /><span class="termdef">collocation</span> is a sequence of words that occur together
unusually often. Thus <span class="example">red wine</span> is a collocation, whereas <span class="example">the
wine</span> is not. A characteristic of collocations is that they are
resistant to substitution with words that have similar senses;
for example, <span class="example">maroon wine</span> sounds definitely odd.</p>
<p>To get a handle on collocations, we start off by extracting from a text
a list of word pairs, also known as <a name="bigrams_index_term" /><span class="termdef">bigrams</span>. This is easily
accomplished with the function <tt class="doctest"><span class="pre">bigrams()</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk <span class="pysrc-keyword">import</span> bigrams
<span class="pysrc-prompt">&gt;&gt;&gt; </span>list(bigrams([<span class="pysrc-string">'more'</span>, <span class="pysrc-string">'is'</span>, <span class="pysrc-string">'said'</span>, <span class="pysrc-string">'than'</span>, <span class="pysrc-string">'done'</span>]))
<span class="pysrc-output">[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">If you omitted <tt class="doctest"><span class="pre">list()</span></tt> above, and just typed <tt class="doctest"><span class="pre">bigrams([<span class="pysrc-string">'more'</span>, ...])</span></tt>,
you would have seen output of the form <tt class="doctest"><span class="pre">&lt;generator object bigrams at 0x10fb8b3a8&gt;</span></tt>.
This is Python's way of saying that it is ready to compute
a sequence of items, in this case, bigrams. For now, you just need
to know to tell Python to convert it into a list, using <tt class="doctest"><span class="pre">list()</span></tt>.</p>
</div>
<p>Here we see that the pair of words <span class="example">than-done</span> is a bigram, and we write
it in Python as <tt class="doctest"><span class="pre">(<span class="pysrc-string">'than'</span>, <span class="pysrc-string">'done'</span>)</span></tt>. Now, collocations are essentially
just frequent bigrams, except that we want to pay more attention to the
cases that involve rare words. In particular, we want to find
bigrams that occur more often than we would expect based on
the frequency of the individual words. The <tt class="doctest"><span class="pre">collocations()</span></tt> function
does this for us. We will see how it works later.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>text4.collocations()
<span class="pysrc-output">United States; fellow citizens; four years; years ago; Federal</span>
<span class="pysrc-output">Government; General Government; American people; Vice President; Old</span>
<span class="pysrc-output">World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;</span>
<span class="pysrc-output">God bless; every citizen; Indian tribes; public debt; one another;</span>
<span class="pysrc-output">foreign nations; political parties</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>text8.collocations()
<span class="pysrc-output">would like; medium build; social drinker; quiet nights; non smoker;</span>
<span class="pysrc-output">long term; age open; Would like; easy going; financially secure; fun</span>
<span class="pysrc-output">times; similar interests; Age open; weekends away; poss rship; well</span>
<span class="pysrc-output">presented; never married; single mum; permanent relationship; slim</span>
<span class="pysrc-output">build</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The collocations that emerge are very specific to the genre of the
texts. In order to find  <span class="example">red wine</span> as a collocation, we would
need to process a much larger body of text.</p>
</div>
<div class="section" id="counting-other-things">
<h2>4.6&nbsp;&nbsp;&nbsp;Counting Other Things</h2>
<p>Counting words is useful, but we can count other things too. For example, we can
look at the distribution of word lengths in a text, by creating a <tt class="doctest"><span class="pre">FreqDist</span></tt>
out of a long list of numbers, where each number is the length of the corresponding
word in the text:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>[len(w) <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> text1] <a name="word-lengths" /><a href="#ref-word-lengths"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-output">[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist = FreqDist(len(w) <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> text1)  <a name="freq-word-lengths" /><a href="#ref-freq-word-lengths"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a>
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(fdist)  <a name="freq-word-lengths-size" /><a href="#ref-freq-word-lengths-size"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></a>
<span class="pysrc-output">&lt;FreqDist with 19 samples and 260819 outcomes&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist
<span class="pysrc-output">FreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399,</span>
<span class="pysrc-output">  8: 9966, 9: 6428, 10: 3528, ...})</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>We start by deriving a list of the lengths of words in <tt class="doctest"><span class="pre">text1</span></tt>
<a class="reference internal" href="#word-lengths"><span id="ref-word-lengths"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>,
and the <tt class="doctest"><span class="pre">FreqDist</span></tt> then counts the number of times each of these
occurs <a class="reference internal" href="#freq-word-lengths"><span id="ref-freq-word-lengths"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>. The result <a class="reference internal" href="#freq-word-lengths-size"><span id="ref-freq-word-lengths-size"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a> is a distribution containing
a quarter of a million items, each of which is a number corresponding to a
word token in the text. But there are at most only 20 distinct
items being counted, the numbers 1 through 20, because there are only 20
different word lengths. I.e., there are words consisting of just one character,
two characters, ..., twenty characters, but none with twenty one or more
characters. One might wonder how frequent the different lengths of word are
(e.g., how many words of length four appear in the text, are there more words of length five
than length four, etc). We can do this as follows:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist.most_common()
<span class="pysrc-output">[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),</span>
<span class="pysrc-output">(8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),</span>
<span class="pysrc-output">(15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist.max()
<span class="pysrc-output">3</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist[3]
<span class="pysrc-output">50223</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fdist.freq(3)
<span class="pysrc-output">0.19255882431878046</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>From this we see that the most frequent word length is 3, and that
words of length 3 account for roughly 50,000 (or 20%) of the words making up the
book. Although we will not pursue it here, further analysis of word
length might help us understand differences between authors, genres, or
languages.</p>
<p><a class="reference internal" href="#tab-freqdist">4.1</a> summarizes the functions defined in frequency distributions.</p>
<span class="target" id="tab-freqdist"></span><table border="1" class="docutils" id="tab-freqdist">
<colgroup>
<col width="31%" />
<col width="69%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Example</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr><td><tt class="doctest"><span class="pre">fdist = FreqDist(samples)</span></tt></td>
<td>create a frequency distribution containing the given samples</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist[sample] += 1</span></tt></td>
<td>increment the count for this sample</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist[<span class="pysrc-string">'monstrous'</span>]</span></tt></td>
<td>count of the number of times a given sample occurred</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.freq(<span class="pysrc-string">'monstrous'</span>)</span></tt></td>
<td>frequency of a given sample</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.N()</span></tt></td>
<td>total number of samples</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.most_common(n)</span></tt></td>
<td>the <tt class="doctest"><span class="pre">n</span></tt> most common samples and their frequencies</td>
</tr>
<tr><td><tt class="doctest"><span class="pre"><span class="pysrc-keyword">for</span> sample <span class="pysrc-keyword">in</span> fdist:</span></tt></td>
<td>iterate over the samples</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.max()</span></tt></td>
<td>sample with the greatest count</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.tabulate()</span></tt></td>
<td>tabulate the frequency distribution</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.plot()</span></tt></td>
<td>graphical plot of the frequency distribution</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist.plot(cumulative=True)</span></tt></td>
<td>cumulative plot of the frequency distribution</td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist1 |= fdist2</span></tt></td>
<td>update <tt class="doctest"><span class="pre">fdist1</span></tt> with counts from <tt class="doctest"><span class="pre">fdist2</span></tt></td>
</tr>
<tr><td><tt class="doctest"><span class="pre">fdist1 &lt; fdist2</span></tt></td>
<td>test if samples in <tt class="doctest"><span class="pre">fdist1</span></tt> occur less frequently than in <tt class="doctest"><span class="pre">fdist2</span></tt></td>
</tr>
</tbody>
<p class="caption"><span class="caption-label">Table 4.1</span>: <p>Functions Defined for NLTK's Frequency Distributions</p>
</p>
</table>
</div>
<div class="section" id="tagging-and-parsing">
<h2>4.7&nbsp;&nbsp;&nbsp;Tagging and Parsing</h2>
<p>NLTK includes several &quot;Part-of-speech taggers&quot; (see <a href="#id17"><span class="problematic" id="id18">chap_tag_</span></a>),
which automatically associate lexical categories with words, e.g.:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk <span class="pysrc-keyword">import</span> pos_tag
<span class="pysrc-prompt">&gt;&gt;&gt; </span>pos_tag(text1[:100], tagset=<span class="pysrc-string">'universal'</span>)
<span class="pysrc-output">[('[', 'NOUN'), ('Moby', 'NOUN'), ('Dick', 'NOUN'), ('by', 'ADP'), ('Herman', 'NOUN'),</span>
<span class="pysrc-output">('Melville', 'NOUN'), ('1851', 'NUM'), (']', 'NUM'), ('ETYMOLOGY', 'NOUN'), ('.', '.'),</span>
<span class="pysrc-output">('(', '.'), ('Supplied', 'NOUN'), ('by', 'ADP'), ('a', 'DET'), ('Late', 'NOUN'),</span>
<span class="pysrc-output">('Consumptive', 'NOUN'), ('Usher', 'NOUN'), ('to', 'PRT'), ('a', 'DET'),</span>
<span class="pysrc-output">('Grammar', 'NOUN'), ('School', 'NOUN'), (')', 'NOUN'), ('The', 'NOUN'),</span>
<span class="pysrc-output">('pale', 'NOUN'), ('Usher', 'NOUN'), ('--', '.'), ('threadbare', 'NOUN'),</span>
<span class="pysrc-output">('in', 'ADP'), ('coat', 'ADJ'), (',', '.'), ('heart', 'NOUN'), (',', '.'),</span>
<span class="pysrc-output">('body', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('brain', 'NOUN'), (';', '.'), ...]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<ul class="simple">
<li>parsing</li>
</ul>
</div>
</div>
<div class="section" id="overview-of-this-book">
<h1>5&nbsp;&nbsp;&nbsp;Overview of this Book</h1>
</div>
<div class="section" id="summary">
<h1>6&nbsp;&nbsp;&nbsp;Summary</h1>
</div>
<div class="section" id="further-reading">
<h1>7&nbsp;&nbsp;&nbsp;Further Reading</h1>
<ul class="simple">
<li>Introductions to NLP</li>
</ul>
<p>You may also like to read up on
some linguistics and NLP-related concepts in Wikipedia (e.g., collocations,
the Turing Test, the type-token distinction).</p>
<p>As you delve into NLTK, you might want to subscribe to the mailing list where new
releases of the toolkit are announced. There is also an NLTK-Users mailing list,
where users help each other as they learn how to use Python and NLTK for
language analysis work. Details of these lists are available at <tt class="doctest"><span class="pre">http://nltk.org/</span></tt>.</p>
<p>For more information on the topics covered in <a href="#id19"><span class="problematic" id="id20">sec-automatic-natural-language-understanding_</span></a>,
and on NLP more generally, you might like to consult one of the following excellent
books:</p>
<ul class="simple">
<li>Indurkhya, Nitin and Fred Damerau (eds, 2010) <em>Handbook of Natural Language Processing</em>
(Second Edition) Chapman &amp; Hall/CRC. 2010. <a class="reference external" href="bibliography.html#indurkhyadamerau2010" id="id2">(Indurkhya &amp; Damerau, 2010)</a> <a class="reference external" href="bibliography.html#dale00handbook" id="id3">(Dale, Moisl, &amp; Somers, 2000)</a></li>
<li>Jurafsky, Daniel and James Martin (2008) <em>Speech and Language Processing</em> (Second Edition). Prentice Hall.
<a class="reference external" href="bibliography.html#jurafskymartin2008" id="id4">(Jurafsky &amp; Martin, 2008)</a></li>
<li>Mitkov, Ruslan (ed, 2003) <em>The Oxford Handbook of Computational Linguistics</em>. Oxford University Press.
(second edition expected in 2010). <a class="reference external" href="bibliography.html#mitkov02handbook" id="id5">(Mitkov, 2002)</a></li>
</ul>
<p>The Association for Computational Linguistics is the international organization that
represents the field of NLP. The ACL website (<tt class="doctest"><span class="pre">http://www.aclweb.org/</span></tt>) hosts many useful resources, including:
information about international and regional conferences and workshops;
the <span class="emphasis">ACL Wiki</span> with links to hundreds of useful resources;
and the <span class="emphasis">ACL Anthology</span>, which contains most of the NLP research literature
from the past 50+ years, fully indexed and freely downloadable.</p>
<p>Some excellent introductory Linguistics textbooks are:
<a href="#id21"><span class="problematic" id="id22"><span id="id6"></span>[Finegan2007]_</span></a>, <a class="reference external" href="bibliography.html#ogrady2004" id="id7">(O'Grady et al, 2004)</a>, <a class="reference external" href="bibliography.html#osu2007" id="id8">(OSU, 2007)</a>. You might like to consult
<span class="emphasis">LanguageLog</span>, a popular linguistics blog with occasional posts that
use the techniques described in this book.</p>
<!-- Natural Language Interfaces to Databases &#8211; An Introduction -->
<!-- I. Androutsopoulos, G.D. Ritchie and P. Thanisch, Natural Language -->
<!-- Engineering, 1(1):29-81, 1995. -->
<!-- Footer to be used in all chapters -->
<div class="admonition-about-this-document admonition">
<p class="first admonition-title">About this document...</p>
<p>UPDATED FOR NLTK 3.0.
This is a chapter from <em>Natural Language Processing with Python</em>,
by <a class="reference external" href="http://estive.net/">Steven Bird</a>, <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Ewan Klein</a> and <a class="reference external" href="http://ed.loper.org/">Edward Loper</a>,
Copyright &#169; 2014 the authors.
It is distributed with the <em>Natural Language Toolkit</em> [<tt class="doctest"><span class="pre">http://nltk.org/</span></tt>],
Version 3.0, under the terms of the
<em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License</em>
[<a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">This document was built on
Wed  1 Jul 2015 17:10:21 AEST</p>
</div>
</div>
<div class="system-messages section">
<h1>Docutils System Messages</h1>
<div class="system-message" id="id9">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 68); <em><a href="#id10">backlink</a></em></p>
Undefined substitution referenced: &quot;SMT&quot;.</div>
<div class="system-message" id="id11">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 150); <em><a href="#id12">backlink</a></em></p>
Duplicate target name, cannot be used as a unique reference: &quot;ex-twitter-sa1&quot;.</div>
<div class="system-message" id="id13">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 161); <em><a href="#id14">backlink</a></em></p>
Duplicate target name, cannot be used as a unique reference: &quot;ex-twitter-sa1&quot;.</div>
<div class="system-message" id="id15">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 563); <em><a href="#id16">backlink</a></em></p>
Unknown target name: &quot;chap_words&quot;.</div>
<div class="system-message" id="id17">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 999); <em><a href="#id18">backlink</a></em></p>
Unknown target name: &quot;chap_tag&quot;.</div>
<div class="system-message" id="id19">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 1041); <em><a href="#id20">backlink</a></em></p>
Unknown target name: &quot;sec-automatic-natural-language-understanding&quot;.</div>
<div class="system-message" id="id21">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">ch01.rst2</tt>, line 1061); <em><a href="#id22">backlink</a></em></p>
Unknown target name: &quot;finegan2007&quot;.</div>
</div>
</div>
</body>
</html>
