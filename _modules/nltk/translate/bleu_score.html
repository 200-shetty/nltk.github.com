<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>nltk.translate.bleu_score &mdash; NLTK 3.0 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '3.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="NLTK 3.0 documentation" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" /> 
  </head>
  <body role="document">
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../../../index.html">NLTK 3.0 documentation</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for nltk.translate.bleu_score</h1><div class="highlight"><pre>
<span class="c"># -*- coding: utf-8 -*-</span>
<span class="c"># Natural Language Toolkit: BLEU Score</span>
<span class="c">#</span>
<span class="c"># Copyright (C) 2001-2016 NLTK Project</span>
<span class="c"># Authors: Chin Yee Lee, Hengfeng Li, Ruxin Hou, Calvin Tanujaya Lim</span>
<span class="c"># Contributors: Dmitrijs Milajevs, Liling Tan</span>
<span class="c"># URL: &lt;http://nltk.org/&gt;</span>
<span class="c"># For license information, see LICENSE.TXT</span>

<span class="sd">&quot;&quot;&quot;BLEU score implementation.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">fractions</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">fractions</span><span class="o">.</span><span class="n">Fraction</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">_normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">fractions</span> <span class="kn">import</span> <span class="n">Fraction</span>
<span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">nltk.compat</span> <span class="kn">import</span> <span class="n">Fraction</span>
    

<div class="viewcode-block" id="sentence_bleu"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu">[docs]</a><span class="k">def</span> <span class="nf">sentence_bleu</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span>
                  <span class="n">smoothing_function</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate BLEU score (Bilingual Evaluation Understudy) from</span>
<span class="sd">    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.</span>
<span class="sd">    &quot;BLEU: a method for automatic evaluation of machine translation.&quot; </span>
<span class="sd">    In Proceedings of ACL. http://www.aclweb.org/anthology/P02-1040.pdf</span>

<span class="sd">    &gt;&gt;&gt; hypothesis1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;which&#39;,</span>
<span class="sd">    ...               &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;always&#39;,</span>
<span class="sd">    ...               &#39;obeys&#39;, &#39;the&#39;, &#39;commands&#39;, &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>

<span class="sd">    &gt;&gt;&gt; hypothesis2 = [&#39;It&#39;, &#39;is&#39;, &#39;to&#39;, &#39;insure&#39;, &#39;the&#39;, &#39;troops&#39;,</span>
<span class="sd">    ...               &#39;forever&#39;, &#39;hearing&#39;, &#39;the&#39;, &#39;activity&#39;, &#39;guidebook&#39;,</span>
<span class="sd">    ...               &#39;that&#39;, &#39;party&#39;, &#39;direct&#39;]</span>

<span class="sd">    &gt;&gt;&gt; reference1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;that&#39;,</span>
<span class="sd">    ...               &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;will&#39;, &#39;forever&#39;,</span>
<span class="sd">    ...               &#39;heed&#39;, &#39;Party&#39;, &#39;commands&#39;]</span>

<span class="sd">    &gt;&gt;&gt; reference2 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;guiding&#39;, &#39;principle&#39;, &#39;which&#39;,</span>
<span class="sd">    ...               &#39;guarantees&#39;, &#39;the&#39;, &#39;military&#39;, &#39;forces&#39;, &#39;always&#39;,</span>
<span class="sd">    ...               &#39;being&#39;, &#39;under&#39;, &#39;the&#39;, &#39;command&#39;, &#39;of&#39;, &#39;the&#39;,</span>
<span class="sd">    ...               &#39;Party&#39;]</span>

<span class="sd">    &gt;&gt;&gt; reference3 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;practical&#39;, &#39;guide&#39;, &#39;for&#39;, &#39;the&#39;,</span>
<span class="sd">    ...               &#39;army&#39;, &#39;always&#39;, &#39;to&#39;, &#39;heed&#39;, &#39;the&#39;, &#39;directions&#39;,</span>
<span class="sd">    ...               &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>

<span class="sd">    &gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS</span>
<span class="sd">    0.5045...</span>

<span class="sd">    &gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis2) # doctest: +ELLIPSIS</span>
<span class="sd">    0.3969...</span>

<span class="sd">    The default BLEU calculates a score for up to 4grams using uniform</span>
<span class="sd">    weights. To evaluate your translations with higher/lower order ngrams, </span>
<span class="sd">    use customized weights. E.g. when accounting for up to 6grams with uniform</span>
<span class="sd">    weights:</span>

<span class="sd">    &gt;&gt;&gt; weights = (0.1666, 0.1666, 0.1666, 0.1666, 0.1666)</span>
<span class="sd">    &gt;&gt;&gt; sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)</span>
<span class="sd">    0.45838627164939455</span>
<span class="sd">    </span>
<span class="sd">    :param references: reference sentences</span>
<span class="sd">    :type references: list(list(str))</span>
<span class="sd">    :param hypothesis: a hypothesis sentence</span>
<span class="sd">    :type hypothesis: list(str)</span>
<span class="sd">    :param weights: weights for unigrams, bigrams, trigrams and so on</span>
<span class="sd">    :type weights: list(float)</span>
<span class="sd">    :return: The sentence-level BLEU score.</span>
<span class="sd">    :rtype: float</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">corpus_bleu</span><span class="p">([</span><span class="n">references</span><span class="p">],</span> <span class="p">[</span><span class="n">hypothesis</span><span class="p">],</span> <span class="n">weights</span><span class="p">,</span> <span class="n">smoothing_function</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="corpus_bleu"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu">[docs]</a><span class="k">def</span> <span class="nf">corpus_bleu</span><span class="p">(</span><span class="n">list_of_references</span><span class="p">,</span> <span class="n">hypotheses</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">),</span>
                <span class="n">smoothing_function</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all </span>
<span class="sd">    the hypotheses and their respective references.  </span>

<span class="sd">    Instead of averaging the sentence level BLEU scores (i.e. marco-average </span>
<span class="sd">    precision), the original BLEU metric (Papineni et al. 2002) accounts for </span>
<span class="sd">    the micro-average precision (i.e. summing the numerators and denominators</span>
<span class="sd">    for each hypothesis-reference(s) pairs before the division).</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; hyp1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;which&#39;,</span>
<span class="sd">    ...         &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;always&#39;,</span>
<span class="sd">    ...         &#39;obeys&#39;, &#39;the&#39;, &#39;commands&#39;, &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ref1a = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;that&#39;,</span>
<span class="sd">    ...          &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;will&#39;, &#39;forever&#39;,</span>
<span class="sd">    ...          &#39;heed&#39;, &#39;Party&#39;, &#39;commands&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ref1b = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;guiding&#39;, &#39;principle&#39;, &#39;which&#39;,</span>
<span class="sd">    ...          &#39;guarantees&#39;, &#39;the&#39;, &#39;military&#39;, &#39;forces&#39;, &#39;always&#39;,</span>
<span class="sd">    ...          &#39;being&#39;, &#39;under&#39;, &#39;the&#39;, &#39;command&#39;, &#39;of&#39;, &#39;the&#39;, &#39;Party&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ref1c = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;practical&#39;, &#39;guide&#39;, &#39;for&#39;, &#39;the&#39;,</span>
<span class="sd">    ...          &#39;army&#39;, &#39;always&#39;, &#39;to&#39;, &#39;heed&#39;, &#39;the&#39;, &#39;directions&#39;,</span>
<span class="sd">    ...          &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; hyp2 = [&#39;he&#39;, &#39;read&#39;, &#39;the&#39;, &#39;book&#39;, &#39;because&#39;, &#39;he&#39;, &#39;was&#39;, </span>
<span class="sd">    ...         &#39;interested&#39;, &#39;in&#39;, &#39;world&#39;, &#39;history&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ref2a = [&#39;he&#39;, &#39;was&#39;, &#39;interested&#39;, &#39;in&#39;, &#39;world&#39;, &#39;history&#39;, </span>
<span class="sd">    ...          &#39;because&#39;, &#39;he&#39;, &#39;read&#39;, &#39;the&#39;, &#39;book&#39;]</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]</span>
<span class="sd">    &gt;&gt;&gt; hypotheses = [hyp1, hyp2]</span>
<span class="sd">    &gt;&gt;&gt; corpus_bleu(list_of_references, hypotheses) # doctest: +ELLIPSIS</span>
<span class="sd">    0.5920...</span>
<span class="sd">    </span>
<span class="sd">    The example below show that corpus_bleu() is different from averaging </span>
<span class="sd">    sentence_bleu() for hypotheses </span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)</span>
<span class="sd">    &gt;&gt;&gt; score2 = sentence_bleu([ref2a], hyp2)</span>
<span class="sd">    &gt;&gt;&gt; (score1 + score2) / 2 # doctest: +ELLIPSIS</span>
<span class="sd">    0.6223...</span>
<span class="sd">    </span>
<span class="sd">    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses</span>
<span class="sd">    :type references: list(list(list(str)))</span>
<span class="sd">    :param hypotheses: a list of hypothesis sentences</span>
<span class="sd">    :type hypotheses: list(list(str))</span>
<span class="sd">    :param weights: weights for unigrams, bigrams, trigrams and so on</span>
<span class="sd">    :type weights: list(float)</span>
<span class="sd">    :return: The corpus-level BLEU score.</span>
<span class="sd">    :rtype: float</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># Before proceeding to compute BLEU, perform sanity checks.</span>

    <span class="n">p_numerators</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span> <span class="c"># Key = ngram order, and value = no. of ngram matches.</span>
    <span class="n">p_denominators</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span> <span class="c"># Key = ngram order, and value = no. of ngram in ref.</span>
    <span class="n">hyp_lengths</span><span class="p">,</span> <span class="n">ref_lengths</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">list_of_references</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">hypotheses</span><span class="p">),</span> <span class="s">&quot;The number of hypotheses and their reference(s) should be the same&quot;</span>
    
    <span class="c"># Iterate through each hypothesis and their corresponding references.</span>
    <span class="k">for</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">list_of_references</span><span class="p">,</span> <span class="n">hypotheses</span><span class="p">):</span>
        <span class="c"># For each order of ngram, calculate the numerator and</span>
        <span class="c"># denominator for the corpus-level modified precision.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span> 
            <span class="n">p_i</span> <span class="o">=</span> <span class="n">modified_precision</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">p_numerators</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span>
            <span class="n">p_denominators</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p_i</span><span class="o">.</span><span class="n">denominator</span>
            
        <span class="c"># Calculate the hypothesis length and the closest reference length.</span>
        <span class="c"># Adds them to the corpus-level hypothesis and reference counts.</span>
        <span class="n">hyp_len</span> <span class="o">=</span>  <span class="nb">len</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span>
        <span class="n">hyp_lengths</span> <span class="o">+=</span> <span class="n">hyp_len</span>
        <span class="n">ref_lengths</span> <span class="o">+=</span> <span class="n">closest_ref_length</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">)</span>
    
    <span class="c"># Calculate corpus-level brevity penalty.</span>
    <span class="n">bp</span> <span class="o">=</span> <span class="n">brevity_penalty</span><span class="p">(</span><span class="n">ref_lengths</span><span class="p">,</span> <span class="n">hyp_lengths</span><span class="p">)</span>
    
    <span class="c"># Collects the various precision values for the different ngram orders.</span>
    <span class="n">p_n</span> <span class="o">=</span> <span class="p">[</span><span class="n">Fraction</span><span class="p">(</span><span class="n">p_numerators</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">p_denominators</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">_normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
           <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
    
    <span class="c"># Returns 0 if there&#39;s no matching n-grams </span>
    <span class="c"># We only need to check for p_numerators[1] == 0, since if there&#39;s</span>
    <span class="c"># no unigrams, there won&#39;t be any higher order ngrams.</span>
    <span class="k">if</span> <span class="n">p_numerators</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    
    <span class="c"># Smoothen the modified precision.</span>
    <span class="c"># Note: smooth_precision() converts values into float.</span>
    <span class="k">if</span> <span class="n">smoothing_function</span><span class="p">:</span>
        <span class="n">p_n</span> <span class="o">=</span> <span class="n">smoothing_function</span><span class="p">(</span><span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span> 
                                 <span class="n">hypothesis</span><span class="o">=</span><span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="o">=</span><span class="n">hyp_len</span><span class="p">)</span>
    
    <span class="c"># Calculates the overall modified precision for all ngrams.</span>
    <span class="c"># By sum of the product of the weights and the respective *p_n*</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">p_n</span><span class="p">)</span> 
         <span class="k">if</span> <span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">bp</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">fsum</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

</div>
<div class="viewcode-block" id="modified_precision"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.modified_precision">[docs]</a><span class="k">def</span> <span class="nf">modified_precision</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate modified ngram precision.</span>

<span class="sd">    The normal precision method may lead to some wrong translations with</span>
<span class="sd">    high-precision, e.g., the translation, in which a word of reference</span>
<span class="sd">    repeats several times, has very high precision.     </span>

<span class="sd">    This function only returns the Fraction object that contains the numerator </span>
<span class="sd">    and denominator necessary to calculate the corpus-level precision. </span>
<span class="sd">    To calculate the modified precision for a single pair of hypothesis and </span>
<span class="sd">    references, cast the Fraction object into a float. </span>
<span class="sd">    </span>
<span class="sd">    The famous &quot;the the the ... &quot; example shows that you can get BLEU precision</span>
<span class="sd">    by duplicating high frequency words.</span>
<span class="sd">    </span>
<span class="sd">        &gt;&gt;&gt; reference1 = &#39;the cat is on the mat&#39;.split()</span>
<span class="sd">        &gt;&gt;&gt; reference2 = &#39;there is a cat on the mat&#39;.split()</span>
<span class="sd">        &gt;&gt;&gt; hypothesis1 = &#39;the the the the the the the&#39;.split()</span>
<span class="sd">        &gt;&gt;&gt; references = [reference1, reference2]</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.2857...</span>
<span class="sd">    </span>
<span class="sd">    In the modified n-gram precision, a reference word will be considered </span>
<span class="sd">    exhausted after a matching hypothesis word is identified, e.g.</span>
<span class="sd">    </span>
<span class="sd">        &gt;&gt;&gt; reference1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;that&#39;,</span>
<span class="sd">        ...               &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;will&#39;, </span>
<span class="sd">        ...               &#39;forever&#39;, &#39;heed&#39;, &#39;Party&#39;, &#39;commands&#39;]</span>
<span class="sd">        &gt;&gt;&gt; reference2 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;guiding&#39;, &#39;principle&#39;, &#39;which&#39;,</span>
<span class="sd">        ...               &#39;guarantees&#39;, &#39;the&#39;, &#39;military&#39;, &#39;forces&#39;, &#39;always&#39;,</span>
<span class="sd">        ...               &#39;being&#39;, &#39;under&#39;, &#39;the&#39;, &#39;command&#39;, &#39;of&#39;, &#39;the&#39;,</span>
<span class="sd">        ...               &#39;Party&#39;]</span>
<span class="sd">        &gt;&gt;&gt; reference3 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;practical&#39;, &#39;guide&#39;, &#39;for&#39;, &#39;the&#39;,</span>
<span class="sd">        ...               &#39;army&#39;, &#39;always&#39;, &#39;to&#39;, &#39;heed&#39;, &#39;the&#39;, &#39;directions&#39;,</span>
<span class="sd">        ...               &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = &#39;of the&#39;.split()</span>
<span class="sd">        &gt;&gt;&gt; references = [reference1, reference2, reference3]</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis, n=1))</span>
<span class="sd">        1.0</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis, n=2))</span>
<span class="sd">        1.0</span>
<span class="sd">        </span>
<span class="sd">    An example of a normal machine translation hypothesis:</span>
<span class="sd">    </span>
<span class="sd">        &gt;&gt;&gt; hypothesis1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;which&#39;,</span>
<span class="sd">        ...               &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;always&#39;,</span>
<span class="sd">        ...               &#39;obeys&#39;, &#39;the&#39;, &#39;commands&#39;, &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; hypothesis2 = [&#39;It&#39;, &#39;is&#39;, &#39;to&#39;, &#39;insure&#39;, &#39;the&#39;, &#39;troops&#39;,</span>
<span class="sd">        ...               &#39;forever&#39;, &#39;hearing&#39;, &#39;the&#39;, &#39;activity&#39;, &#39;guidebook&#39;,</span>
<span class="sd">        ...               &#39;that&#39;, &#39;party&#39;, &#39;direct&#39;]</span>
<span class="sd">    </span>
<span class="sd">        &gt;&gt;&gt; reference1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;that&#39;,</span>
<span class="sd">        ...               &#39;ensures&#39;, &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;will&#39;, </span>
<span class="sd">        ...               &#39;forever&#39;, &#39;heed&#39;, &#39;Party&#39;, &#39;commands&#39;]</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; reference2 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;guiding&#39;, &#39;principle&#39;, &#39;which&#39;,</span>
<span class="sd">        ...               &#39;guarantees&#39;, &#39;the&#39;, &#39;military&#39;, &#39;forces&#39;, &#39;always&#39;,</span>
<span class="sd">        ...               &#39;being&#39;, &#39;under&#39;, &#39;the&#39;, &#39;command&#39;, &#39;of&#39;, &#39;the&#39;,</span>
<span class="sd">        ...               &#39;Party&#39;]</span>
<span class="sd">        </span>
<span class="sd">        &gt;&gt;&gt; reference3 = [&#39;It&#39;, &#39;is&#39;, &#39;the&#39;, &#39;practical&#39;, &#39;guide&#39;, &#39;for&#39;, &#39;the&#39;,</span>
<span class="sd">        ...               &#39;army&#39;, &#39;always&#39;, &#39;to&#39;, &#39;heed&#39;, &#39;the&#39;, &#39;directions&#39;,</span>
<span class="sd">        ...               &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">        &gt;&gt;&gt; references = [reference1, reference2, reference3]</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.9444...</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.5714...</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.5882352941176471</span>
<span class="sd">        &gt;&gt;&gt; float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.07692...</span>
<span class="sd">     </span>
<span class="sd">    </span>
<span class="sd">    :param references: A list of reference translations.</span>
<span class="sd">    :type references: list(list(str))</span>
<span class="sd">    :param hypothesis: A hypothesis translation.</span>
<span class="sd">    :type hypothesis: list(str)</span>
<span class="sd">    :param n: The ngram order.</span>
<span class="sd">    :type n: int</span>
<span class="sd">    :return: BLEU&#39;s modified precision for the nth order ngram.</span>
<span class="sd">    :rtype: Fraction</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># Extracts all ngrams in hypothesis.</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>

    <span class="c"># Extract a union of references&#39; counts.</span>
    <span class="c">## max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])</span>
    <span class="n">max_counts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">reference</span> <span class="ow">in</span> <span class="n">references</span><span class="p">:</span>
        <span class="n">reference_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">reference</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
            <span class="n">max_counts</span><span class="p">[</span><span class="n">ngram</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
                                    <span class="n">reference_counts</span><span class="p">[</span><span class="n">ngram</span><span class="p">])</span>
    
    <span class="c"># Assigns the intersection between hypothesis and references&#39; counts.</span>
    <span class="n">clipped_counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">ngram</span><span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">max_counts</span><span class="p">[</span><span class="n">ngram</span><span class="p">])</span> 
                      <span class="k">for</span> <span class="n">ngram</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">counts</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="n">numerator</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">clipped_counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">Fraction</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">denominator</span><span class="p">,</span> <span class="n">_normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  
    
</div>
<div class="viewcode-block" id="closest_ref_length"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.closest_ref_length">[docs]</a><span class="k">def</span> <span class="nf">closest_ref_length</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function finds the reference that is the closest length to the </span>
<span class="sd">    hypothesis. The closest reference length is referred to as *r* variable </span>
<span class="sd">    from the brevity penalty formula in Papineni et. al. (2002)</span>
<span class="sd">    </span>
<span class="sd">    :param references: A list of reference translations.</span>
<span class="sd">    :type references: list(list(str))</span>
<span class="sd">    :param hypothesis: The length of the hypothesis.</span>
<span class="sd">    :type hypothesis: int</span>
<span class="sd">    :return: The length of the reference that&#39;s closest to the hypothesis.</span>
<span class="sd">    :rtype: int    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ref_lens</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reference</span><span class="p">)</span> <span class="k">for</span> <span class="n">reference</span> <span class="ow">in</span> <span class="n">references</span><span class="p">)</span>
    <span class="n">closest_ref_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ref_lens</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">ref_len</span><span class="p">:</span> 
                          <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">ref_len</span> <span class="o">-</span> <span class="n">hyp_len</span><span class="p">),</span> <span class="n">ref_len</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">closest_ref_len</span>

</div>
<div class="viewcode-block" id="brevity_penalty"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.brevity_penalty">[docs]</a><span class="k">def</span> <span class="nf">brevity_penalty</span><span class="p">(</span><span class="n">closest_ref_len</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate brevity penalty.</span>

<span class="sd">    As the modified n-gram precision still has the problem from the short</span>
<span class="sd">    length sentence, brevity penalty is used to modify the overall BLEU</span>
<span class="sd">    score according to length.</span>

<span class="sd">    An example from the paper. There are three references with length 12, 15</span>
<span class="sd">    and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.</span>

<span class="sd">        &gt;&gt;&gt; reference1 = list(&#39;aaaaaaaaaaaa&#39;)      # i.e. [&#39;a&#39;] * 12</span>
<span class="sd">        &gt;&gt;&gt; reference2 = list(&#39;aaaaaaaaaaaaaaa&#39;)   # i.e. [&#39;a&#39;] * 15</span>
<span class="sd">        &gt;&gt;&gt; reference3 = list(&#39;aaaaaaaaaaaaaaaaa&#39;) # i.e. [&#39;a&#39;] * 17</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = list(&#39;aaaaaaaaaaaa&#39;)      # i.e. [&#39;a&#39;] * 12</span>
<span class="sd">        &gt;&gt;&gt; references = [reference1, reference2, reference3]</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)</span>
<span class="sd">        1.0</span>

<span class="sd">    In case a hypothesis translation is shorter than the references, penalty is</span>
<span class="sd">    applied.</span>

<span class="sd">        &gt;&gt;&gt; references = [[&#39;a&#39;] * 28, [&#39;a&#39;] * 28]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = [&#39;a&#39;] * 12</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)</span>
<span class="sd">        0.2635971381157267</span>

<span class="sd">    The length of the closest reference is used to compute the penalty. If the</span>
<span class="sd">    length of a hypothesis is 12, and the reference lengths are 13 and 2, the</span>
<span class="sd">    penalty is applied because the hypothesis length (12) is less then the</span>
<span class="sd">    closest reference length (13).</span>

<span class="sd">        &gt;&gt;&gt; references = [[&#39;a&#39;] * 13, [&#39;a&#39;] * 2]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = [&#39;a&#39;] * 12</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS</span>
<span class="sd">        0.9200...</span>

<span class="sd">    The brevity penalty doesn&#39;t depend on reference order. More importantly,</span>
<span class="sd">    when two reference sentences are at the same distance, the shortest</span>
<span class="sd">    reference sentence length is used.</span>

<span class="sd">        &gt;&gt;&gt; references = [[&#39;a&#39;] * 13, [&#39;a&#39;] * 11]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = [&#39;a&#39;] * 12</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; bp1 = brevity_penalty(closest_ref_len, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(reversed(references), hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; bp2 = brevity_penalty(closest_ref_len, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; bp1 == bp2 == 1</span>
<span class="sd">        True</span>

<span class="sd">    A test example from mteval-v13a.pl (starting from the line 705):</span>

<span class="sd">        &gt;&gt;&gt; references = [[&#39;a&#39;] * 11, [&#39;a&#39;] * 8]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = [&#39;a&#39;] * 7</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS</span>
<span class="sd">        0.8668...</span>

<span class="sd">        &gt;&gt;&gt; references = [[&#39;a&#39;] * 11, [&#39;a&#39;] * 8, [&#39;a&#39;] * 6, [&#39;a&#39;] * 7]</span>
<span class="sd">        &gt;&gt;&gt; hypothesis = [&#39;a&#39;] * 7</span>
<span class="sd">        &gt;&gt;&gt; hyp_len = len(hypothesis)</span>
<span class="sd">        &gt;&gt;&gt; closest_ref_len =  closest_ref_length(references, hyp_len)</span>
<span class="sd">        &gt;&gt;&gt; brevity_penalty(closest_ref_len, hyp_len)</span>
<span class="sd">        1.0</span>
<span class="sd">    </span>
<span class="sd">    :param hyp_len: The length of the hypothesis for a single sentence OR the </span>
<span class="sd">    sum of all the hypotheses&#39; lengths for a corpus</span>
<span class="sd">    :type hyp_len: int</span>
<span class="sd">    :param closest_ref_len: The length of the closest reference for a single </span>
<span class="sd">    hypothesis OR the sum of all the closest references for every hypotheses.</span>
<span class="sd">    :type closest_reference_len: int    </span>
<span class="sd">    :return: BLEU&#39;s brevity penalty.</span>
<span class="sd">    :rtype: float</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">hyp_len</span> <span class="o">&gt;</span> <span class="n">closest_ref_len</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">closest_ref_len</span> <span class="o">/</span> <span class="n">hyp_len</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="SmoothingFunction"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction">[docs]</a><span class="k">class</span> <span class="nc">SmoothingFunction</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of the smoothing techniques </span>
<span class="sd">    for segment-level BLEU scores that was presented in </span>
<span class="sd">    Boxing Chen and Collin Cherry (2014) A Systematic Comparison of </span>
<span class="sd">    Smoothing Techniques for Sentence-Level BLEU. In WMT14. </span>
<span class="sd">    http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will initialize the parameters required for the various smoothing</span>
<span class="sd">        techniques, the default values are set to the numbers used in the</span>
<span class="sd">        experiments from Chen and Cherry (2014).</span>

<span class="sd">        &gt;&gt;&gt; hypothesis1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;which&#39;, &#39;ensures&#39;, </span>
<span class="sd">        ...                 &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;always&#39;, &#39;obeys&#39;, &#39;the&#39;, </span>
<span class="sd">        ...                 &#39;commands&#39;, &#39;of&#39;, &#39;the&#39;, &#39;party&#39;]</span>
<span class="sd">        &gt;&gt;&gt; reference1 = [&#39;It&#39;, &#39;is&#39;, &#39;a&#39;, &#39;guide&#39;, &#39;to&#39;, &#39;action&#39;, &#39;that&#39;, &#39;ensures&#39;, </span>
<span class="sd">        ...               &#39;that&#39;, &#39;the&#39;, &#39;military&#39;, &#39;will&#39;, &#39;forever&#39;, &#39;heed&#39;, </span>
<span class="sd">        ...               &#39;Party&#39;, &#39;commands&#39;]</span>
<span class="sd">                </span>
<span class="sd">        &gt;&gt;&gt; chencherry = SmoothingFunction()</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4118...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method0)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4118...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method1)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4118...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method2)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4489...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method3)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4118...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method4)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4118...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method5)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4905...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method6)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.1801...</span>
<span class="sd">        &gt;&gt;&gt; print (sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method7)) # doctest: +ELLIPSIS</span>
<span class="sd">        0.4905...</span>

<span class="sd">        :param epsilon: the epsilon value use in method 1</span>
<span class="sd">        :type epsilon: float</span>
<span class="sd">        :param alpha: the alpha value use in method 6</span>
<span class="sd">        :type alpha: int</span>
<span class="sd">        :param k: the k value use in method 4</span>
<span class="sd">        :type k: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        
<div class="viewcode-block" id="SmoothingFunction.method0"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method0">[docs]</a>    <span class="k">def</span> <span class="nf">method0</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; No smoothing. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">p_n</span>
        </div>
<div class="viewcode-block" id="SmoothingFunction.method1"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method1">[docs]</a>    <span class="k">def</span> <span class="nf">method1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.</span>
<span class="sd">        &quot;&quot;&quot;</span> 
        <span class="k">return</span> <span class="p">[(</span><span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span><span class="o">/</span> <span class="n">p_i</span><span class="o">.</span><span class="n">denominator</span> 
                <span class="k">if</span> <span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">p_i</span> <span class="k">for</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="n">p_n</span><span class="p">]</span>
        </div>
<div class="viewcode-block" id="SmoothingFunction.method2"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method2">[docs]</a>    <span class="k">def</span> <span class="nf">method2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 2: Add 1 to both numerator and denominator from </span>
<span class="sd">        Chin-Yew Lin and Franz Josef Och (2004) Automatic evaluation of </span>
<span class="sd">        machine translation quality using longest common subsequence and </span>
<span class="sd">        skip-bigram statistics. In ACL04.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">Fraction</span><span class="p">(</span><span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p_i</span><span class="o">.</span><span class="n">denominator</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">_normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="n">p_n</span><span class="p">]</span>
        </div>
<div class="viewcode-block" id="SmoothingFunction.method3"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method3">[docs]</a>    <span class="k">def</span> <span class="nf">method3</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 3: NIST geometric sequence smoothing </span>
<span class="sd">        The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each </span>
<span class="sd">        precision score whose matching n-gram count is null.</span>
<span class="sd">        k is 1 for the first &#39;n&#39; value for which the n-gram match count is null/</span>
<span class="sd">        For example, if the text contains:</span>
<span class="sd">         - one 2-gram match</span>
<span class="sd">         - and (consequently) two 1-gram matches</span>
<span class="sd">        the n-gram count for each individual precision score would be:</span>
<span class="sd">         - n=1  =&gt;  prec_count = 2     (two unigrams)</span>
<span class="sd">         - n=2  =&gt;  prec_count = 1     (one bigram)</span>
<span class="sd">         - n=3  =&gt;  prec_count = 1/2   (no trigram,  taking &#39;smoothed&#39; value of 1 / ( 2^k ), with k=1)</span>
<span class="sd">         - n=4  =&gt;  prec_count = 1/4   (no fourgram, taking &#39;smoothed&#39; value of 1 / ( 2^k ), with k=2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">incvnt</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># From the mteval-v13a.pl, it&#39;s referred to as k.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_n</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">incvnt</span> <span class="o">*</span> <span class="n">p_i</span><span class="o">.</span><span class="n">denominator</span><span class="p">)</span>
                <span class="n">incvnt</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">p_n</span>
    </div>
<div class="viewcode-block" id="SmoothingFunction.method4"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method4">[docs]</a>    <span class="k">def</span> <span class="nf">method4</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 4: </span>
<span class="sd">        Shorter translations may have inflated precision values due to having </span>
<span class="sd">        smaller denominators; therefore, we give them proportionally</span>
<span class="sd">        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry </span>
<span class="sd">        suggests dividing by 1/ln(len(T)), where T is the length of the translation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">incvnt</span> <span class="o">=</span> <span class="mi">1</span> 
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_n</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">p_i</span><span class="o">.</span><span class="n">numerator</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">hyp_len</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">incvnt</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hyp_len</span><span class="p">)</span> <span class="c"># Note that this K is different from the K from NIST.</span>
                <span class="n">incvnt</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">p_n</span>

</div>
<div class="viewcode-block" id="SmoothingFunction.method5"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method5">[docs]</a>    <span class="k">def</span> <span class="nf">method5</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 5:</span>
<span class="sd">        The matched counts for similar values of n should be similar. To a </span>
<span class="sd">        calculate the n-gram matched count, it averages the n−1, n and n+1 gram </span>
<span class="sd">        matched counts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c"># Requires an precision value for an addition ngram order.</span>
        <span class="n">p_n_plus1</span> <span class="o">=</span> <span class="n">p_n</span> <span class="o">+</span> <span class="p">[</span><span class="n">modified_precision</span><span class="p">(</span><span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span>
        <span class="n">m</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_n</span><span class="p">):</span>
            <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">p_i</span> <span class="o">+</span> <span class="n">p_n_plus1</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">3</span>
            <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
        <span class="k">return</span> <span class="n">p_n</span>
        </div>
<div class="viewcode-block" id="SmoothingFunction.method6"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method6">[docs]</a>    <span class="k">def</span> <span class="nf">method6</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 6:</span>
<span class="sd">        Interpolates the maximum likelihood estimate of the precision *p_n* with </span>
<span class="sd">        a prior estimate *pi0*. The prior is estimated by assuming that the ratio </span>
<span class="sd">        between pn and pn−1 will be the same as that between pn−1 and pn−2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">p_n</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]:</span> <span class="c"># Skips the first 2 orders of ngrams.</span>
                <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pi0</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> 
                <span class="c"># No. of ngrams in translation.</span>
                <span class="n">l</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">p_n</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">pi0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_n</span>
    </div>
<div class="viewcode-block" id="SmoothingFunction.method7"><a class="viewcode-back" href="../../../api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction.method7">[docs]</a>    <span class="k">def</span> <span class="nf">method7</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Smoothing method 6:</span>
<span class="sd">        Interpolates the maximum likelihood estimate of the precision *p_n* with </span>
<span class="sd">        a prior estimate *pi0*. The prior is estimated by assuming that the ratio </span>
<span class="sd">        between pn and pn−1 will be the same as that between pn−1 and pn−2.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">p_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method4</span><span class="p">(</span><span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">)</span>
        <span class="n">p_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">method5</span><span class="p">(</span><span class="n">p_n</span><span class="p">,</span> <span class="n">references</span><span class="p">,</span> <span class="n">hypothesis</span><span class="p">,</span> <span class="n">hyp_len</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">p_n</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table Of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../news.html">NLTK News</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">Installing NLTK Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contribute to NLTK</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/nltk.html">API</a></li>
<li class="toctree-l1"><a class="reference external" href="http://www.nltk.org/howto">HOWTO</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../../../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
            </form>
            <p class="searchtip" style="font-size: 90%">
                Enter search terms or a module, class or function name.
            </p>
          </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
            <a href="../../../py-modindex.html" title="Python Module Index"
              >modules</a> |
            <a href="../../../genindex.html" title="General Index"
              >index</a>
          </div>
          <div role="note" aria-label="source link">
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, NLTK Project.
      Last updated on Apr 09, 2016.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>